\documentclass[]{article}

\usepackage{anysize}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\let\epsilon\varepsilon
\newcommand\SLASH{\char`\\}
\newcommand{\csch}{\text{csch}}
\marginsize{0.5in}{1in}{0.5in}{1in}
\setlength\parindent{0pt}

% Title Page: Modify as needed
\title{Cumulative Knowledge (Frist 2 Years of Gradschool)}
\author{Summarized by Svetlana Eden}
\date{Month Day, 2015}

\hypersetup{hidelinks}

% \usepackage[pdftex,bookmarks,pagebackref,pdfpagemode=UseOutlines,
%      colorlinks,linkcolor=\linkcol,
%      pdfauthor={Svetlana K Eden},
%      pdftitle={\titl}]{hyperref}


\begin{document}
\maketitle
\tableofcontents
\listoffigures
\listoftables
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matrix algebra
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matrix algebra
\section{Matrix algebra, Linear models}
From \emph{Linear Models in Statistics}, second edition, A.C.Rencher and G.B.Schaalje
\subsubsection{Matrix Inversion, page 23-24}
\begin{enumerate}[1)]
\item $(A^{-1})' = (A')^{-1}$
\item $(AB)^{-1} = B^{-1}A^{-1}$
\item \[ If ~~~A =
\begin{bmatrix}
    A_{11}       & A_{12}  \\
    A_{21}       & A_{22} \\
\end{bmatrix} ~~and
~~~B = A_{22} - A_{21}A_{11}^{-1}A_{12};\\
~~~ then~~A^{-1} =
\begin{bmatrix}
    A_{11}^{-1} + A_{11}^{-1}A_{12}B^{-1}A_{21}A_{11}^{-1}  &  A_{11}^{-1}A_{12}B^{-1} \\
    -B^{-1}A_{21}A_{11}^{-1}       &             B^{-1}
\end{bmatrix}
\]

\item \[ If ~~~A =
\begin{bmatrix}
    \pmb{A}_{11}       & \pmb{a}_{12}  \\
    \pmb{a}_{12}'       & a_{22} \\
\end{bmatrix} ~~and
~~~b = a_{22} - \pmb{a}_{12}'\pmb{A}_{11}^{-1}\pmb{a}_{12};\\
~~~ then~~A^{-1} = \frac{1}{b}
\begin{bmatrix}
    b\pmb{A}_{11}^{-1} + \pmb{A}_{11}^{-1}\pmb{a}_{12}\pmb{a}_{12}'\pmb{A}_{11}^{-1}  &  \pmb{A}_{11}^{-1}\pmb{a}_{12} \\
    -\pmb{a}_{12}'\pmb{A}_{11}^{-1}       &             1
\end{bmatrix}
\]

\item \[If~~ A =
\begin{bmatrix}
    A_{11}       & 0  \\
    0       & A_{22} \\
\end{bmatrix}^{-1}
~~~ then ~~A^{-1}=
\begin{bmatrix}
    A_{11}^{-1}       & 0  \\
    0       & A_{22}^{-1} \\
\end{bmatrix}
\]
 \item $(B+cc')^{-1} = B^{-1} - \frac{B^{-1}cc'B^{-1}}{1 + c B^{-1}c}$
 \item $(A + PBQ)^{-1} = A^{-1} - A^{-1} PB(B+BQA^{-1}PB)^{-1}BQA^{-1}$
\item \[If~~ A =
\begin{bmatrix}
    a       & b  \\
    c       & d \\
\end{bmatrix}^{-1}
~~~ then ~~A^{-1}=\frac{1}{ad-bc}
\begin{bmatrix}
    d       & -b  \\
    -c       & a \\
\end{bmatrix}
\]
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Positive Definite Matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Positive Definite Matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Positive Definite Matrices
\subsubsection{Positive Definite Matrices, page 27-28}
\textbf{Definition:} Matrix $A$ is \emph{positive definite} if for any $y$: $y'Ay > 0$.\\
\textbf{Definition:} Matrix $A$ is \emph{positive semidefinite} if there exists such $y \neq 0$ that: $y'Ay = 0$.
\begin{enumerate}[1)]
\item If B is $n\times p$, where $n>p$ and $rank(B)=p$($<p$) then $B'B$ is positive definite (semidefinite)
\item If $A$ is positive definite then $A^{-1}$ is positive definite.
\item If $A$ is positive definite and is partitioned in the form
\[ A =
\begin{bmatrix}
    A_{11}       & A_{12}  \\
    A_{21}       & A_{22} \\
\end{bmatrix}
\]
where $A_{11}$ and $A_{22}$ were square, then $A_{11}$ and $A_{22}$ are positive definite
\item If $A$ is positive definite (semidefinite) then diagonal elements $a_{ii}>0$ ($a_{ii}\geq 0$ )
\item If $P$ is nonsingular and $A$ is positive definite (semidefinite) then $P'AP$ is positive definite (semidefinite).
\item If $B$ is an $n\times p$ and if $rank(B)=p$ ($rank(B)<p$) then $B'B$ is positive definite (semidefinite).
\item If $A$ is positive definite $p \times p$ and $B$ is $k\times p$ with $k \leq p$ then $BAB'$ is positive definite.
\item $A$ is positive definite if and only if there exists a non-singular matrix such that $A=P'P$ (Cholesky decomposition).
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Generalized Inverse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Generalized Inverse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Generalized Inverse
\subsubsection{Generalized Inverse, page 32-}
\textbf{Definition:} A \emph{generalized inverse}(also called a \emph{conditional inverse}) of an $n\times p$ matrix $A$ is any matrix $A^-$ that satisfies:
\begin{equation}
	AA^-A = A
	\end{equation}
\begin{enumerate}[1)]
\item If B is $n\times p$, where $n>p$ and $rank(B)=p$($<p$) then $B'B$ is positive definite (semidefinite)
\item If $A$ is positive definite then $A^{-1}$ is positive definite.
\item If $A$ is positive definite and is partitioned in the form
\[ A =
\begin{bmatrix}
    A_{11}       & A_{12}  \\
    A_{21}       & A_{22} \\
\end{bmatrix}
\]
where $A_{11}$ and $A_{22}$ were square, then $A_{11}$ and $A_{22}$ are positive definite

\item Assume that $A$ is $n\times p$ and $rank(A) = r$, and $A_{11}$ is $r\times r$ and $rank(A_{11}) = r$:
 \[ If ~~~A =
\begin{bmatrix}
    A_{11}       & A_{12}  \\
    A_{21}       & A_{22} \\
\end{bmatrix}
~~~ then~~A^{-1} =
\begin{bmatrix}
    A_{11}^{-1}  &  0 \\
    0       &             0
\end{bmatrix}
\]

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Orthogonal vectors and matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Orthogonal vectors and matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Orthogonal vectors and matrices
\subsubsection{Orthogonal vectors and matrices, page 41-}
\textbf{Definition:} Two vectors $a$ and $b$ of size $n\times 1$ are said to be \emph{orthogonal} if $a'b = 0$.
\textbf{Definition:} A set of $p\times 1$ vectors $c_1,c_2,...,c_n$ that are normalized and pairwise orthogonal is said to be \emph{orthogonal}.
\textbf{Definition:} A set of $p\times 1$ vectors $c_1,c_2,...,c_n$ that are normalized and pairwise orthogonal is said to be \emph{orthogonal}.
\begin{enumerate}[1)]
	\item $C'C=I$, $CC'=I$
	\item Mulitplication by an orthogonal matrix is equivalent to axes rotation therefore, if $z=Cx$ then $z'z = (Cx)'(Cx) = x'C'Cx = x'Ix = x'x$.
\end{enumerate}
If $A$ is any $p\times p$ matrix and $C$ is orthogonal:
\begin{enumerate}[1)]
	\item $|C|=+1 ~~or~~ -1$
	\item $|C'AC| = |A|$
	\item $-1 \geq c_{ij} \geq 1$, where $c_{ij}$ is any element of $C$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Trace
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Trace
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Trace
\subsubsection{Trace, page 44-}
\begin{enumerate}[1)]
	\item If $A$ and $B$ are $n\times n$ matrices, then $tr(A \pm B) = tr(A) \pm tr(B)$
	\item If $A$ is $n\times p$ and $B$ is $p\times n$, then $tr(AB) = tr(BA)$
	\item If $A$ is $n\times p$, then $tr(A'A) = \sum_{i=1}^p a_i' a_i$, where $a_i$ is the $i^{th}$ \emph{column} of $A$ AND $tr(AA') = \sum_{i=1}^n a_i' a_i$, where $a_i$ is the $i^{th}$ \emph{row} of $A$ OR $tr(A'A) = tr(AA') = \sum_{i=1}^n\sum_{j=1}^p a_{ij}^2$
	\item If $A$ is any $n\times n$ matrix and $P$ is any $n\times n$ nonsingular matrix, then $tr(P^{-1}AP) = tr(A)$
	\item If $A$ is any $n\times n$ matrix and $C$ is any $n\times n$ orthogonal matrix, then $tr(C'AC) = tr(A)$
	\item If $A$ is any $n\times p$ of rank $r$ and $A^-$ is a generalized inverse of $A$, then $tr(A^-A) = tr(AA^-) = r$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idempotent matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idempotent matrices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idempotent matrices
\subsubsection{Idempotent matrices, page 54,55}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% matrix calculus
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% matrix calculus
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% matrix calculus
\subsubsection{Vector and matrix calculus, page 56-}
\textbf{Theorem 2.14.a, p.56}: Let $u=\pmb{a}'\pmb{x}=\pmb{x}'\pmb{a}$ where $\pmb{a}'=(a_1, a_2, ..., a_p)$ is a vector of constants. Then \\
$$
\begin{aligned}
	\frac{\partial u}{\partial\pmb{x}} = \frac{\partial \pmb{a}'\pmb{x}}{\partial\pmb{x}} = \frac{\partial \pmb{x}'\pmb{a}}{\partial\pmb{x}} = \pmb{a}
\end{aligned}
$$
\textbf{Theorem 2.14.b, p.56}: Let $u=\pmb{x}'\pmb{Ax}$ where $\pmb{A}$ is a symmetric matrix of constants. Then \\
$$
\begin{aligned}
	\frac{\partial u}{\partial\pmb{x}} = \frac{\partial \pmb{x}'\pmb{Ax}}{\partial\pmb{x}} = 2\pmb{Ax}
\end{aligned}
$$
\textbf{Theorem 2.14.c, p.57}: Let $u=tr(\pmb{XA})$ where $\pmb{X}$ is a $p\times p$ positive definite matrix and $\pmb{A}$ is a $p\times p$ matrix of constants. Then \\
$$
\begin{aligned}
	\frac{\partial u}{\partial\pmb{X}} = \frac{\partial [tr(\pmb{XA})]}{\partial\pmb{X}} = \pmb{A} + \pmb{A}' - diag\pmb{A}
\end{aligned}
$$
\textbf{Theorem 2.14.d, p.58}: Let $u=ln|\pmb{X}|$ where $\pmb{X}$ is a $p\times p$ Then \\
$$
\begin{aligned}
	\frac{\partial ln|\pmb{X}|}{\partial\pmb{X}} = 2\pmb{X}^{-1} - diag(\pmb{X}^{-1})
\end{aligned}
$$
\textbf{Theorem 2.14.e, p.58}: Let $\pmb{A}$ be nonsingular of order $n$ with derivative $\frac{\partial\pmb{A}}{\partial x}$ Then \\
$$
\begin{aligned}
	\frac{\partial \pmb{A}^{-1}}{\partial x} = -\pmb{A}^{-1} \frac{\partial \pmb{A}}{\partial x} \pmb{A}^{-1}
\end{aligned}
$$
\textbf{Theorem 2.14.f, p.58}: Let $\pmb{A}$ be $n\times n$ positive definite matrix. Then \\
$$
\begin{aligned}
	\frac{\partial ln|\pmb{A}|}{\partial x} = tr \left( \pmb{A}^{-1}\frac{\partial \pmb{A}}{\partial x}   \right)
\end{aligned}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Means and variances
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Means and variances
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Means and variances
\subsubsection{Means and variances, page 80-}
\textbf{Theorem 3.6.a,b, Corollary 1, p.80-1}: Let $\pmb{a}$ be a $p \times 1$ vector of constatns and $\pmb{y}$ be a $p \times 1$ random vector with mean vector $\pmb{\mu}$. Also, if $\pmb{X}$ is a random matrix, and $\pmb{a}$, $\pmb{b}$ are vectors of constants, and $\pmb{A}$, $\pmb{B}$ are matrices of constants, then
\begin{enumerate}[(i)]
	\item $\mu_z = E(\pmb{a}'\pmb{y}) = \pmb{a}'E(\pmb{y}) = \pmb{a}'\pmb{\mu}$
	\item $E(\pmb{A}\pmb{y}) = \pmb{A}E(\pmb{y})$
	\item $E(\pmb{a}'\pmb{Xb}) = \pmb{a}'E(\pmb{X})\pmb{b}$
	\item $E(\pmb{A}\pmb{XB}) = \pmb{A}E(\pmb{X})\pmb{B}$
	\item $E(\pmb{Ay} + \pmb{b}) = \pmb{A}E(\pmb{y}) + \pmb{b}$
\end{enumerate}

\textbf{Theorem 3.6.c, Corollary 1, p.80-1, 76}: Let $\pmb{a}$ be a $p \times 1$ vector of constatns and $\pmb{y}$ be a $p \times 1$ random vector with covariance matrix $\pmb{\Sigma}$. Also, let  $\pmb{a}$, $\pmb{b}$ be $p \times 1$ (or other dimension (play it by ear)), and $\pmb{A}$ be $k \times p$ matrix of constants, $\pmb{B}$ be an $m \times p$ matrix of constants, then
\begin{enumerate}[(i)]
	\item $\sigma^2 = var(\pmb{a}'\pmb{y}) = \pmb{a}'\pmb{\Sigma  a}$
	\item $cov(\pmb{a}'\pmb{y}, \pmb{b}'\pmb{y}) = \pmb{a}'\pmb{\Sigma  b}$
	\item $cov(\pmb{A}\pmb{y}) = \pmb{A}\pmb{\Sigma  A}'$
	\item $cov(\pmb{A}\pmb{y}+ \pmb{b}) = \pmb{A}\pmb{\Sigma  A}'$
	\item $cov(\pmb{Ay}, \pmb{Bx}) = \pmb{A}\pmb{\Sigma_{yx}  B}'$
	\item $\pmb{\Sigma} = E[(\pmb{y} - \pmb{\mu})][(\pmb{y} - \pmb{\mu})'] = E(\pmb{yy}' - \pmb{\mu\mu}')$
	\item p. 107 $E(\pmb{y}'\pmb{Ay}) = tr(\pmb{A\Sigma}) + \pmb{\mu}'\pmb{A\mu}$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Theorems related to normal distribution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Theorems related to normal distribution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Theorems related to normal distribution
\subsubsection{Theorems related to normal distribution}
\textbf{Theorem 4.4a, p.92}: Let $p\times 1$ random vector $\pmb{y}$ be $N(\pmb{\mu}, \pmb{\Sigma})$, let $\pmb{a}$ be any $p\times 1$ vector of constants, and let $\pmb{A}$ be any $k\times p$ matrix of constatns with rank $k\leq p$. Then 
\begin{enumerate}[(i)]
	\item $z = \pmb{a}'\pmb{y}$ is $N(\pmb{a}'\pmb{\mu}, \pmb{a}'\pmb{\Sigma}\pmb{a})$
	\item $z = \pmb{A}'\pmb{y}$ is $N(\pmb{A}\pmb{\mu}, \pmb{A}\pmb{\Sigma}\pmb{A}')$
\end{enumerate}
\textbf{Theorem 7.6b, p.159}: Suppose that $\pmb{y}$ is $N_n(\pmb{X}\pmb{\beta}, \sigma^2\pmb{I})$, where $\pmb{X}$ is $n\times (k+1)$ of rank $k+1<n$ and $\beta = (\beta_0, \beta_1, ..., \beta_k)'$. Then the maximum likelihood estimators $\hat{\pmb{\beta}}$ and $\hat{\sigma}^2$ given in theorem 7.6a have the following distributional properties:
\begin{enumerate}[(i)]
	\item $\hat{\pmb{\beta}}$ is  $N_{k+1}(\pmb{\beta}, \sigma^2(\pmb{X}'\pmb{X})^{-1})$
	\item $\frac{n \hat{\sigma}^2}{\sigma^2}$ is $\chi^2_{n-k-1}$, or equivalently, $\frac{(n-k-1) s^2}{\sigma^2}$ is $\chi^2_{n-k-1}$
	\item $\hat{\pmb{\beta}}$ and $\hat{\sigma}^2$ (or $s^2$) are independent.
\end{enumerate}
\textbf{Theorem 8.4a, p.199}: If $\pmb{y}$ is distributed $N_n(\pmb{X}\pmb{\beta}, \sigma^2\pmb{I})$ and $\pmb{C}$ is $q\times (k+1)$, then:
\begin{enumerate}[(i)]
	\item $\pmb{C}\hat{\pmb{\beta}}$ is  $N_q(\pmb{C\beta}, \sigma^2\pmb{C}(\pmb{X}'\pmb{X})^{-1}\pmb{C}')$.
	\item $\frac{SSH}{\sigma^2} = \frac{(\pmb{C}\hat{\pmb{\beta}})' \pmb{C}(\pmb{X}'\pmb{X})^{-1} \pmb{C}\hat{\pmb{\beta}}}{\sigma^2}$ is $\chi^2_{q, \lambda}$, where $\lambda = \frac{(\pmb{C}\pmb{\beta})' \pmb{C}(\pmb{X}'\pmb{X})^{-1} \pmb{C}\pmb{\beta}}{2\sigma^2}$.
	\item $\frac{SSE}{\sigma^2} = \frac{\pmb{y}'(\pmb{I}- \pmb{X}(\pmb{X}'\pmb{X})^{-1}\pmb{X})\pmb{y}}{\sigma^2}$ is $\chi^2_{n-k-1}$.
	\item $SSH$ and $SSE$ are independent.
\end{enumerate}

\textbf{Theorem 8.4b, p.199}: If $\pmb{y}$ be $N_n(\pmb{X}\pmb{\beta}, \sigma^2\pmb{I})$ and define the statistic:
$$
\begin{aligned}
	F = \frac{SSH/q}{SSE/(n-k-1)} = \frac{(\pmb{C}\hat{\pmb{\beta}})' \pmb{C}(\pmb{X}'\pmb{X})^{-1} \pmb{C}\hat{\pmb{\beta}}/q}{\pmb{y}'(\pmb{I}- \pmb{X}(\pmb{X}'\pmb{X})^{-1}\pmb{X})\pmb{y}/(n-k-1)}
\end{aligned}
$$
where $\pmb{C}$ is $q\times (k+1)$ of rank $q \leq k+1$ and $\hat{\pmb{\beta}} = (\pmb{X}'\pmb{X})^{-1}\pmb{X}'\pmb{y}$ the distribution of $F$ is as follows:
\begin{enumerate}[(i)]
	\item If $H_0:~\pmb{C}\pmb{\beta}=0$ is false, then $F$ is distributed as $F_{(q,~n-k-1,~\lambda)}$, where $\lambda=\frac{(\pmb{C}\pmb{\beta})'[\pmb{C}(\pmb{X}'\pmb{X})^{-1}\pmb{C}']^{-1}\pmb{C}\pmb{\beta}}{2\sigma^2}$.
	\item If $H_0:~\pmb{C}\pmb{\beta}=0$ is true, then $F$ is distributed as $F_{(q,~n-k-1)}$.
\end{enumerate}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Probability
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Probability
\section{Probability, \textbf{Book: STATISTICAL INFERENCE, Casella \& Berger, Second Edition }}
\subsection{Distributions}
\subsubsection{Normal}
\subsubsection{Geometric}
\subsubsection{Hypergeometric}
\subsubsection{Binomial}
\subsubsection{Poisson}
\subsubsection{Gamma, p. 63}
Support $0<x<\infty$, $\alpha, \beta >0$. Density and MGF:
$$
\begin{aligned}
	f(x) = \frac{1}{\Gamma (\alpha) \beta^{\alpha}}x^{\alpha - 1} e^{-x/\beta}, ~~~ MGF_X(t) = \left(\frac{1}{1-\beta t} \right)^{\alpha}
\end{aligned}
$$
Transformations:
\begin{enumerate}[1)]
	\item If $X \sim \Gamma(\alpha, \beta)$ and $\alpha=1$ then If $X \sim exp(\beta)$
	\item If $X_i \overset{independent}{\sim} \Gamma(\alpha_i, \beta)$ then   $\sum_{i=1}^n X_i \sim (p.) \Gamma(\sum_{i=1}^n \alpha, \beta)$, page 183, proof using MGF.
	\item If $X_i \overset{iid}{\sim} \Gamma(\alpha, \beta)$ and $\bar{X} = \frac{\sum_{i=1}^n X_i}{n}$ then $\bar{X} \sim \Gamma(\alpha \cdot n, \beta/n)$.
	\item If $X \sim \Gamma(\alpha, \beta)$ and $\beta=2$ then $X \sim \chi^2_{2\alpha}$
	\item If $X \sim \Gamma(\alpha, \beta)$ and $\alpha=3/2$ and $Y=\sqrt{X/\beta}$ then $Y \sim Maxwell$
	\item If $X \sim \Gamma(\alpha, \beta)$ and $Y=1/X$ then If $Y \sim Inverted~Gamma$
	\item If $X_i \sim \Gamma(\alpha_i, \beta)~for~i=1,2$ and $Y=\frac{X_1}{X_1 + X_2}$ then $Y \sim Beta(\alpha_1, \alpha_2)$
	\item If $X \sim \Gamma(\alpha, \beta)$ and then $cX \sim  \Gamma(\alpha, c\beta)$
	\item If $U_i \overset{iid}{\sim} Uniform[0, 1]$ and then $-\sum_{i=1}^n ln(U_i)  \sim  \Gamma(n, 1)$
	\item If $X \sim \Gamma(\alpha \in \pmb{Z}, \beta)$ and $Y \sim Pois\left(\frac{x}{\beta}\right)$ then $P\{X > x\} = P\{Y < \alpha\}$
	\item If $X_n \sim \Gamma(\alpha_n, \beta)$ and $\alpha_n \rightarrow \infty$ then $X_n ~is~distributed~more~like~ Normal(\alpha_n\beta, ~\alpha_n \beta^2)$
	\item see more in Wikipedia
\end{enumerate}


\subsubsection{Beta}
\subsubsection{Moment Generating function}
\subsection{Conditional Moments Identities}
$$
\begin{aligned}
	E_X(X) &= E_Y\left\{E_{X|Y}[X|Y]\right\}\\
	Var_X(X)&= E_Y\left\{Var_{X|Y}(X|Y)\right\}  +   Var_Y\left\{E_{X|Y}(X|Y)\right\}\\
	Cov_{X,Y}(X,Y)&= E_Z\left\{Cov_{X,Y|Z}(X,Y|Z)\right\}  +   Cov_Z\left\{E_{X|Z}(X|Z), E_{Y|Z}(Y|Z)\right\}\\
\end{aligned}
$$


\subsection{Inequalities}
\subsection{Wald, Likelihood, Score tests}
\subsection{Delta method}
\subsubsection{First order method}
If a sequence of random variables satisfies: $\sqrt{n}(x_n - \theta) \rightarrow~N(0, \sigma^2)$ in distribution and $g(\theta)$ is a function with existing derivative at point $\theta$ and such that $g(\theta) \neq 0$. Then:
$$
\begin{aligned}
	\sqrt{n}(g(x_n) - g(\theta)) \overset{d}{\rightarrow}~N(0, \sigma^2\cdot (g'(\theta))^2)
\end{aligned}
$$
\textbf{Proof:} From Taylor series expansion we have:
$$
\begin{aligned}
	g(x_n) &= g(\theta) + \frac{g'(\theta)(x_n - \theta)}{1!} + R(x_n, \theta)\\
	g(x_n) - g(\theta) &= g'(\theta)(x_n - \theta) + R(x_n, \theta)\\
	\sqrt{n}(g(x_n) - g(\theta)) &= g'(\theta)\sqrt{n}(x_n - \theta) + \sqrt{n}R(x_n, \theta)
\end{aligned}
$$
Because we somehow know that $\sqrt{n}R(x_n, \theta) \rightarrow 0$, it follows that $	\sqrt{n}(g(x_n) - g(\theta)) \overset{d}{\rightarrow}~N(0, \sigma^2\cdot (g'(\theta))^2)$.

\subsubsection{Second order method (Bryan's class notes)}
If a sequence of random variables satisfies: $\sqrt{n}(x_n - \theta) \rightarrow~N(0, \sigma^2)$ in distribution and $g(\theta)$ is a function with existing derivative at point $\theta$ and such that $g(\theta) = 0$. Then:
$$
\begin{aligned}
	\frac{2n(g(x_n) - g(\theta))}{\sigma^2 g''(\theta)} \overset{d}{\rightarrow}~\chi^2_1
\end{aligned}
$$
\textbf{Proof:} From Taylor series expansion we have:
$$
\begin{aligned}
	g(x_n) &= g(\theta) + \frac{g'(\theta)(x_n - \theta)}{1!} + \frac{g''(\theta)(x_n - \theta)^2}{2!}+ R^*(x_n, \theta)\\
	g(x_n) &= g(\theta)  + \frac{g''(\theta)(x_n - \theta)^2}{2!}+ R^*(x_n, \theta)\\
	n(g(x_n) - g(\theta))  &= n\frac{\sigma^2}{\sigma^2}\frac{g''(\theta)(x_n - \theta)^2}{2!}+ n\cdot R^*(x_n, \theta)\\
\end{aligned}
$$
Because again, we somehow know that $n\cdot R^*(x_n, \theta) \rightarrow 0$, and $	\frac{n(x_n - \theta)^2}{\sigma^2} \overset{d}{\rightarrow}~\chi^2_1$ the result follows.

\subsubsection{Multivariate method (Bryan's class notes)}
Suppose we have a vector of parameters $\pmb{\theta}=(\theta_1, \theta_2, ..., \theta_p)$ and its estimate $\hat{\pmb{\theta}}$.\\ And also suppose $\sqrt{n}(\hat{\pmb{\theta}} - \pmb{\theta}) \overset{d}{\rightarrow}~N(\pmb{0}, \pmb{\Sigma})$, where $\pmb{\sigma}$ is a variance covariance matrix for $\pmb{\theta}$. Then, we have:
$$
\begin{aligned}
	\sqrt{n}(g(\hat{\pmb{\theta}}) - g(\pmb{\theta})) \overset{d}{\rightarrow}~N(\pmb{0}, g'(\pmb{\theta})^T \pmb{\Sigma}~g'(\pmb{\theta}))
\end{aligned}
$$
Where $g'(\pmb{\theta}) = \left( \frac{\partial}{\partial \theta_1} g(\pmb{\theta}), \frac{\partial}{\partial \theta_2} g(\pmb{\theta}), ..., \frac{\partial}{\partial \theta_p} g(\pmb{\theta})  \right)$


\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Advanced probability. Book
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Advanced probability. Book
\section{Advanced probability. Book: \textit{Probability. Alan F.Karr}}
\subsection{Sets and measures}
\textbf{Definition 1.8, p.20}: Let $A_1, A_2, A_3, ...$ and $A$ be subsets of $\Omega$.
\begin{enumerate}[a)]
	\item The $lim~sup$ of $(A_n)$ is the set of $\omega$ such that $\omega \in A_n$ for \emph{infinitely many} values of $n$:
$$
\begin{aligned}
	\{A_n, ~i.o.\} \overset{def}{=} \underset{n}{\lim \sup} ~A_n = \cap_{k=1}^{\infty}\cup_{n=k}^{\infty} A_n
\end{aligned}
$$
	\item The $lim~inf$ of $(A_n)$ is the set of $\omega$ such that $\omega \in A_n$ for \emph{all byt infinitely many} values of $n$:
$$
\begin{aligned}
	\{A_n, ~ult.\} \overset{def}{=} \underset{n}{\lim \inf} ~A_n = \cup_{k=1}^{\infty}\cap_{n=k}^{\infty} A_n
\end{aligned}
$$
	\item The sequence $(A_n)$ \emph{converges to} $A$, which we write as $A=\lim_{n\rightarrow \infty} A_n$ or simply $A_n\rightarrow A$ if :
$$
\begin{aligned}
	\underset{n}{\lim \sup} ~A_n = \underset{n}{\lim \inf} ~A_n = A
	\end{aligned}
$$
\textbf{Theorem 1.27 \textit{(Borel-Cantelli lemma)}, p.27}: If $\sum_{n=1}^{\infty} P(A_n) < \infty$ then $P\{A_n, i.o.\} = 0$	\\
\textbf{Theorem 3.23 \textit{(Borel-Cantelli lemma)}, p.81}: If $A_1, A_2, A_3, ...$ are independent events such that $\sum_{n=1}^{\infty} P(A_n) = \infty$ then $P\{A_n, i.o.\} = 1$

\end{enumerate}

\subsection{Types of convergence and implications among forms of convergence, p. 135-}
%\usepackage{mathtools}
\textbf{Definition}: Sequence $X_n$ converges to $X$ \textbf{\textit{almost surely}} (probabilistic version of pointwise conversion), denoted by $X_n\xrightarrow{a.s.}X$, if
$$
\begin{aligned}
	P(\{ \omega: X_n(\omega) \rightarrow X(\omega) \})=1
	\end{aligned}
$$
\textbf{Definition}: Sequence $X_n$ converges to $X$ \textbf{\textit{in probability}}, denoted by $X_n\xrightarrow{P}X$, if
$$
\begin{aligned}
	\lim_{x\rightarrow \infty}P\{|X_n - X|\}=0
	\end{aligned}
$$
\textbf{Definition}: Suppose $X,X_1,X_2,...,X_i,...$ belongs to $L^2$, then $X_n$ converges to $X$ \textbf{\textit{in quadratic mean}}, denoted by $X_n\xrightarrow{q.m.}X$ or $X_n\xrightarrow{L^2}X$, if
$$
\begin{aligned}
	\lim_{x\rightarrow \infty}E[(X_n - X)^2]=0
	\end{aligned}
$$
\textbf{Definition}: Suppose $X,X_1,X_2,...,X_i,...$ $\in$ $L^1$, then $X_n$ converges to $X$ \textbf{\textit{in $L^1$}}, denoted by $X_n\xrightarrow{q.m.}X$ or $X_n\xrightarrow{L^1}X$, if
$$
\begin{aligned}
	\lim_{x\rightarrow \infty}E[|X_n - X|]=0
	\end{aligned}
$$
\textbf{Definition}: The sequence $(X_n)$ converges to $X$ \textbf{\textit{in distribution}}, denoted by $X_n\xrightarrow{q.m.}X$ or $X_n\xrightarrow{d}X$, if
$$
\begin{aligned}
	\lim_{x\rightarrow \infty}F_{X_n}{t}=F_X(t)
	\end{aligned}
$$
for all $t$ at which $F_X$ is continuous.\\
\textbf{Definition 5.15, p.142}: A sequence $(X_n)$ is \textbf{\textit{uniformly integrable}} if $X_n \in L^1$ for each $n$ and if
$$
\begin{aligned}
	\lim_{a\rightarrow \infty} \sup_n E[|X_n|; \{|X_n|>a\}] = 0
\end{aligned}
$$
$\bigstar$, \textbf{p.143} Any sequence $(X_n)$ dominated by an element from $L^1$ is uniformly integrable.\\
\textbf{Definition, p.143}: A sequence $(X_n)$ is \textbf{\textit{uniformly absolutely continuous}} if for each $\varepsilon >0$ there is $\delta >0$ such that
$$
\begin{aligned}
	\sup_n E[|X_n|; A] < \varepsilon,~~~~for~\forall A~such~that~P(A)< \delta
\end{aligned}
$$
\textbf{Proposition, p.138}: If $\sum_{n=1}^{\infty} P\{ |X_n - X|>\varepsilon \} < \infty$ for every $\varepsilon > 0$, then $X_n \overset{a.s.}{\longrightarrow} X$.

\subsubsection{Implications, theorems (p. 138,140,-)}
$$
\begin{aligned}
	L^2~ &\Longrightarrow      &L^1  &\Longrightarrow& ~ In~Probability~ &\Longrightarrow&~ In~Distribution\\
	    &        &L^1  &\overset{Uniform Integr.}{\longleftarrow}&
			 ~ In~Probability~ & \\
	    & &Almost ~Surely ~ &\Longrightarrow& ~ In~Probability~ &\Longrightarrow&~In~Distribution\\
	    & & & & ~ In~Probability~ &\overset{Constant ~Limit}{\longleftarrow}&~In~Distribution~
	\end{aligned}
$$
\subsubsection{Algebraic operations and continuous mapping theorems (p. 145,-)}
\textbf{Theorems 5.19, 5.20, 5.21, 5.22, 5.23, p.145-8} Let $X$, $X_n$, $Y$, and $Y_n$ be random variables, and $\bigodot \in \{a.s., P, q.m., L^1\}$, then it follows:
\begin{enumerate}[a)]
	\item If $X_n \overset{\bigodot}{\longrightarrow} X$ and $Y_n \overset{\bigodot}{\longrightarrow} Y$, then $X_n + Y_n \overset{\bigodot}{\longrightarrow} X + Y$
	\item If $X_n \overset{\bigodot}{\longrightarrow} X$ and $Y_n \overset{\bigodot}{\longrightarrow} Y$, then $X_nY_n \overset{\bigodot}{\longrightarrow} X Y$
	\item \textbf{Slutsky's theorem} If $X_n \overset{d}{\longrightarrow} X$ and $Y_n \overset{d}{\longrightarrow} c \in \mathbb{R}$, then $X_n + Y_n \overset{d}{\longrightarrow} X + c$
	\item \textbf{Slutsky's theorem, bis} If $X_n \overset{d}{\longrightarrow} X$ and $Y_n \overset{d}{\longrightarrow} c \in \mathbb{R}$, then $X_n Y_n \overset{d}{\longrightarrow} cX$
	\item \textbf{Continuous mappings} Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be a continuouts function. And let $\circledast \in \{a.s., P, d\}$. Then we have that if $X_n \overset{\circledast}{\longrightarrow} X$, then $g(X_n) \overset{\circledast}{\longrightarrow} g(X)$
\end{enumerate}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Characteristic functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Characteristic functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Characteristic functions
\subsection{Characteristic functions (p. 163-)}
This chapter uses notations and facts for \emph{imaginary numbers}:
$$
\begin{aligned}
	&z = x + y\cdot i = \Re(i) + \Im(i)\cdot i\\
	&\bar{z}~is~conjugate ~of~z~if~z = x + y\cdot i~~~and~~~\bar{z} = x - y\cdot i\\
	&|z| = \sqrt{x^2 + y^2} = \sqrt{z\bar{z}}~(modulus~of~z)\\
	&e^{it} = cos(t) + i\cdot sin(t) ~~~(Euler's~formula)
\end{aligned}
$$
\textbf{Definition}: The \textbf{\textit{characteristic function}}  of a random variable $X$ is the function $\varphi_X:~\mathbb{R} \rightarrow \mathbb{C}$ defined by
$$
\begin{aligned}
	\varphi_X(t) = E[e^{itX}] = \int_{-\infty}^{\infty}e^{itX} dF_X(x)
\end{aligned}
$$
$\bigstar$ $|\varphi_X(0)|= 1$.\\
$\bigstar$ Characteristic function always exists because for all $t$: $|\varphi_X(t)|\leq 1$.\\
$\bigstar$ Characteristic function is \emph{uniformly continuous}, where \textbf{\textit{Uniform Continuity}} means that for any $A$ and any $\varepsilon$, there is such $\delta>0$ that $\underset{(x,y)\in A^2,~|x-y|<}{\sup}|\varphi(x) - \varphi(y)| < \varepsilon$.\\
\textbf{Proposition 6.2 (p.164)}: The characteristic function is uniformly continuous and $\varphi_X(-t) = \overline{\varphi_X(t)},~\forall t$.\\
\textbf{Theorem 6.4 (p.165)}: If $X$ and $Y$ are independent, then $\varphi_{X+Y}(t) = \varphi_X(t)\varphi_Y(t)$.\\
\textbf{Theorem 6.5 (p.166)}: Whenever $a<b \in \mathbb{R}$ are continuity points of $F_x$,
$$
\begin{aligned}
	P\{a < X < b\} = \lim_{T \rightarrow \infty}\frac{1}{2\pi}\int_{-T}^{T} \frac{e^{-ita} - e^{itb}}{it}\varphi_X(t)dt
\end{aligned}
$$
\textbf{Theorem 6.6 (p.167)}: If $\varphi_X(t)=\varphi_Y(t),~\forall t$ then $X$ and $Y$ have the same distribution.\\
\textbf{Theorem 6.7 (p.167) (Fourier inversion theorem)}: If
$$
\begin{aligned}
	\int_{-\infty}^{\infty} |\varphi_X(t)|dt < \infty
\end{aligned}
$$
then $X$ is absolutely continuous with density
$$
\begin{aligned}
	f_X(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty}e^{itX} \varphi_X(t) dt
\end{aligned}
$$
\textbf{Theorem 6.11 (p.169)}: If $E[|X|^k] < \infty$ then the derivative $\varphi^{(k)}_X(t)$ exists  and
$$
\begin{aligned}
	E[X^k] = i^{-k}\varphi^{(k)}_X(0)
\end{aligned}
$$
\textbf{Theorem 6.12 (p.170)}: Let $k$ be and even integer, and suppose that $\varphi_X^{(k)}(0)$ exists then, $E[|X^k|] < \infty$.
\textbf{Theorem 6.15 (p.171)}: Suppose we have $X_n \overset{d}{\rightarrow} X$ \textbf{if and only if} $\varphi_{X_n}(t) \rightarrow \varphi_{X}(t)$, $\forall t \in \mathbb{R}$

\textbf{Some Theorem not from the book}: If $X$ and $Y$ areindependent real random variables with characteristic functions $\varphi$ and $\phi$ respectively, then $E e^{itXY}=E\varphi(tY)=E\phi(tX)$
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Prediction and conditional expectation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Prediction and conditional expectation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Prediction and conditional expectation
\subsection{Prediction and conditional expectation}
See page 217: Let's assume that random variable $Z$ is defined on a probability space $(\Omega, \mathcal{F}, P)$ and $Z \in L^2$ ($E[Z^2] < \infty$). We want to predict unobserved $X$ using observed $Y_1, Y_2, ..., Y_n$. The book says that we predict $X$ using some allowable functions of $Y_i$. So we need to build a \emph{linear predictor}: $Z = f(Y_1, Y_2, ..., Y_n) = a^*_1\cdot Y_1 + a^*_2\cdot Y_1 + a^*_3\cdot Y_1 + ... + a^*_1\cdot Y_n$.\\
\textbf{Definition}(p. 217): the \textbf{mean squared error of Z (MSE)} is:
$$
\begin{aligned}
	MSE(Z) = E[(Z - X)^2]
	\end{aligned}
$$
Assuming that the optimal predictor exists,
\textbf{Definition}(p. 217): the \textbf{minimum mean squared error predictor (MMSE)} of $X$ (let's denote it as $\hat{X}$), then satisfies:
$$
\begin{aligned}
	E[(\hat{X} - X)^2] \leq E[(Z - X)^2], ~~~for~ \forall~Z~\in~ V
	\end{aligned}
$$
\textbf{Definition}(my own definition (could not find it in the book)): \textbf{orthonormal base of space $V^n$} is a set of vectors $Z_i$ ($i \in \{1, 2, ..., n\}$) such that there exists such numbers $a_i, a_2, ..., a_n$:
$$
\begin{aligned}
	v = \sum_{i=1}^n a_i Z_i, ~~~\forall ~v\in V^n\\
	and ~~~\langle Z_i Z_j \rangle = 0, ~~~\forall~i\neq j ~~~~~ and ~~~~~
	||Z_i|| = 1, ~~~\forall~i
	\end{aligned}
$$
\textbf{Proposition 8.11} The MMSE linear predictor of $X\in L^2$ within V is $\hat{X} = \sum_{i=1}^n = a_i^*Y_i$, where $a^*_1, a^*_2, ..., a^*_n$ are such that
$$
\begin{aligned}
	&\sum_{j=1}^n  \langle Y_i Y_j \rangle a_j^* = \langle Y_i X \rangle, ~~~ i=1,2,...n
	&for ~ any ~basis~Y_1, Y_2, ..., Y_n\\
	&a_i^*= \langle Y_i X \rangle,~so~ \hat{X}= \sum_{j=1}^n \langle Y_i X \rangle Y_i, ~~~ i=1,2,...n
	&for ~ orthonormal ~basis~Y_1, Y_2, ..., Y_n\\
	\end{aligned}
$$
\textbf{Example} If $V = \{ aY + b: a,b \in R\}$, where $Y\in L^2$ and $\sigma^2 = Var(Y)>0$ then it can be shown that $Z_1=(Y-E[Y])/\sigma$ and $Z_2\equiv 1$ is the orthonormal basis, and MMSE, $\hat{X} = \frac{cov(X, Y)}{\sigma^2} (Y-E[Y]) + E[X]$.

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Survival analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Survival analysis
\section{Survival analysis (under construction)}
\subsection{Tests}
\subsubsection{Log-rank test}
\subsection{Hazard, survival functions and identities}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Survival analysis
\section{Maximization, Lagrange Multiplier, Karush-Kuhn-Tucker conditions (KKT)}
We skip the KKT theorem here and give an example of minimization with inequality constraints. The example is taken from {\texttt{http://myweb.clemson.edu/~pbelott/bulk/teaching/lehigh/ie426-f09/lecture20.pdf}}. We have the following minimization problem:
$$
\begin{aligned}
	\min~&X^2 + 2y^2\\
	  &x+y\geq 3\\
	  &y-x^2\geq 1
\end{aligned}
$$
The Lagrangian is:
$$
\begin{aligned}
	 \mathcal{L} = x^2 + 2y^2 - &\lambda_1(x+y - 3) + &\lambda_2(y-x^2- 1)\\
	 &\lambda_1 \geq 0,&\lambda_2 \geq 0
\end{aligned}
$$
{\scriptsize{\textbf{Note} that when we \emph{minimize} we take the main expression with $+$ sign and we \textbf{subtract} the equality constraint the way it is shown above. For inequality, we write it in a standard form (with variables on the left side, with $\geq$ or $>$ signs and a constant on the right), and also subtract it. When we \emph{maximize} or when the inequality constraits are written in a different way, the $+$ and $-$ signs might work differently.\\
}}~ \\
The KKT conditions are:
$$
\begin{aligned}
	&A)~~x+y\geq 3\\
	&B)~~y-x^2\geq 1\\
	&C)~~\partial\mathcal{L}/\partial{x}=2x - \lambda_1 + 2\lambda_2=0\\
	&D)~~\partial\mathcal{L}/\partial{y}=4y - \lambda_1 - \lambda_2=0\\
	&E)~~\lambda_1(x+y -3) = 0\\
	&F)~~\lambda_2(y-x^2 - 1)=0\\
	&G)~~\lambda_1,~\lambda_2\geq0\\
\end{aligned}
$$
We have to consider four cases:
\begin{enumerate}[1)]
	\item $\lambda_1 = \lambda_2=0$. Then $C$ and $D$ become $x=0$ and $y=0$, which violates $A$.
	\item $\lambda_1>0,~ \lambda_2=0$. From $E$ it follows that $x+y -3=0$. From $C$ and $D$ we have: $2x-\lambda_1=0$ and $4y-\lambda_1=0$, which leads to a unique solution $(x,y)=(2,1)$, which violates $y-x^2\geq 1$.
	\item $\lambda_1 =0,~ \lambda_2>0$. From $F$ we have $y - x^2 - 1=0$. From $C$ we have: $2x+2\lambda_2x=2x(1+\lambda_2)=0$. Because $\lambda_2>0$ we must have $x=0$. From $D$
	we have $4y-\lambda_2=0$. Because $x=0$ we have $y= 0 + 1 = 1$, which violates $x+y\geq 3$.
	\item $\lambda_1>0,~ \lambda_2>0$. From $E$ and $F$ we have: $y-x^2 - 1=0$ and $x+y=3$, therefore $(3-x) - x^2 - 1=0$ or $x^2 + x - 2=0$. This equation has two solutions $x \in \{-2, 1\}$, therefore $(x,y) \in \{(-2, 5), (1, 2)\}$. For $(x,y)=(-2, 5)$, equation $C$ becomes $-4-\lambda_1 - 4\lambda_2=0$, which is impossible for $\lambda_1,\lambda_2 \geq 0$. For $(x,y)=(1, 2)$ we solve $C$ and $D$ and get $\lambda_1 = 6$, $\lambda_2=2$. This is the only point that satisfies $KKT$ so this is the solution.
\end{enumerate}


\section{What to practice/ to code}
\begin{enumerate}[1)]
	\item Wald test (with C matrix), chunk test
	\item Plot coefficient effects.
	\item Spaghetti plot
	\item
	\item
\end{enumerate}

\clearpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Longitudinal Data Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Longitudinal Data Analysis
\section{Longitudinal Data Analysis}
\textbf{Estimation of $\beta$} can be done through \textbf{response feature analysis} (two-step approach), or \textbf{weighted least squares}.\\
\textbf{Estimation of $\Sigma$} can be done through \textbf{robust sandwich estimator}, \textbf{maximum likelihood (ML)}, \textbf{restricted maximum likelihood (REML)}

\section{OLS vs WLS}
When $\pmb{\Sigma}\neq \sigma^2I$ there are two sources of error (slide 42, mod.1, ch.2):
\begin{itemize}
	\item $\hat{\sigma}2$ is \textbf{biased}
	\item $\sigma^2(X^TX)^{-1}$ is \textbf{not correct} estimate for $cov(\hat{\beta})$
\end{itemize}
In order to fix this problem we can do robust sandwich estimator.
~\\
	\begin{tabular}{ l |c| c }
  ~~~ & OLS & WLS \\
  \hline
  \hline
  ~~~ &     ~~~          &     ~~~ \\
  Minimize &               &      $\sum_{i=1}^n(Y_i - X_i\beta)^TW_i(Y_i - X_i\beta)$ \\
  Score & $X^T(Y-X\beta)=0$ & $X^TW(Y-X\beta)=0$ \\

  $\hat{\beta}$ & $(X^TX)^{-1} X^TY$ &       $   \left(\sum_{i=1}^n X_i^TW_iX_i \right)^{-1} \left( \sum_{i=1}^n X_i^TW_iY_i  \right)$ \\
  Covariance & $\sigma^2(X^TX)^{-1}$ &              $ \left(\sum_{i=1}^nX_i^TW_iX_i \right)^{-1}$ \\
  Sandwich & $(X^TX)^{-1} (X^T \hat{\sigma}^2 X) (X^TX)^{-1}$ &                    $\left(\sum_{i=1}^n X_i^TW_iX_i \right)^{-1} \left(\sum_{i=1}^n X_i^T W_i \hat{\pmb{\Sigma}}_0 W_iX_i \right)  \left(\sum_{i=1}^n X_i^TW_iX_i \right)^{-1}$\\
  cov estimator & $\hat{\sigma}^2 = \frac{1}{mn-p} \sum_{i=1}^n (Y_i - X_i\hat{\beta})^T(Y_i - X_i\hat{\beta})$ &    $\hat{\Sigma}_0 = \frac{1}{n} \sum_{i=1}^n (Y_i - X_i\tilde{\beta})^T(Y_i - X_i\tilde{\beta})$\\
  \hline
  \end{tabular}\\
~\\
~\\



\section{Models for Longitudinal Data}
	\subsection{Interpretation of coefficients}
	\textbf{Population level 1:} $\beta_1$ is expected change in weight difference b/w subjects associated difference of 1$kg$ of weight at enrolled time holding other variable constant and assuming that no other factors explain this difference.
	\textbf{Population level 2:} $\beta_1$ is expected difference in weight b/w two population of subjects that differ in weight by 1$kg$ at enrolled time holding other variable constant and assuming that no other factors explain this difference.
	\textbf{Population level OR:} The odds of receiving individualized information for treatment group were 12 percent less compared to the control group (OR=.88, 95\%CI: [.78, 1.00]), assuming that these groups were identical given the rest of the covariates. In other words, we have evidence that the treatment reduced the odds of receiving the individualized information.
	\subsection{Alternating Logistic Regression}

	\subsubsection{Model Definition}
When we have binary longitudinal data, defining covariance structure is a problem because correlation for binary variables is limited by some expression of their means. \\
Alternating Logistic regression offeres a different approach. We need to solve $\sum_i D_i^T V_i^{-1} (Y_i - \mu_i) = 0$, where $V_i$ is a working covariance structure. Instead of defining it, we assume:

$$
\begin{aligned}
	log(E(Y_{ij}|Y_{ij'})) &= \Delta_{ijj'} + \alpha Y_{ij'}\\
	&or\\
	log(E(Y_{ij}|Y_{ij'})) &= \Delta_{ijj'} + (\alpha_0 + \alpha_1[|j-j'|]-1) Y_{ij'}\\
	&or\\
	log(E(Y_{ij}|Y_{ij'})) &= \Delta_{ijj'} + \alpha_0 + [\alpha_1I[|j-j'|==2) + \alpha_2I[|j-j'|==3) + \alpha_3I[|j-j'|==4) + ...] Y_{ij'}\\
\end{aligned}
$$
	\subsubsection{Method of Solving}
	\subsubsection{Hypothesis Testing}

	\subsubsection{R-code}
	If data looks like this
	\begin{tabular}{ l c r }
  id & time \\
  \hline
  11 & 0 \\
  11 & 1 \\
  11 & 2 \\
  \end{tabular}

Then the z.matrix is built  like this:
	\begin{tabular}{ l c r }
  j & j' & (j-j') \\
  \hline
  0 & 0 & 0\\
  0 & 1 & -1 \\
  0 & 2 & -2 \\
  1 & 0 & 1 \\
  1 & 1 & 0 \\
  1 & 2 & -1 \\
  2 & 0 & 2 \\
  2 & 1 & 1 \\
  2 & 2 & 0 \\
  \end{tabular}


So we can run the following code:
\begin{verbatim}
z.mat <- NULL
for (i in unique(data1$id)){
#for (i in c("1", "2")){
    tmp <- data1[data1$id==i,]
    tmp1 <- expand.grid(tmp$month,tmp$month)
    tmp2 <- tmp1[,1]-tmp1[,2]
    tmp2 <- tmp2[tmp2>0]
    len <- length(tmp2)
		zForID = cbind(rep(tmp$id[1], len), tmp2, rep(tmp$gender[1], len), rep(tmp$age[1], len),
		tmp2==2, tmp2==3, tmp2==4, tmp2==5, tmp2==6, tmp2>=7)
    z.mat <- rbind(z.mat, zForID)
}
z.mat <- data.frame(z.mat)
names(z.mat) <- c("id", "timediff", "gender", "age", "timeDiff=2", "timeDiff=3", "timeDiff=4",
"timeDiff=5", "timeDiff=6", "timeDiff>=7")
z.mat$timediff1 <- z.mat$timediff-1
z.mat$one <- 1

mod1b = ordgee(ordered(thought) ~ month*age + month*gender, data=data1,
z=z.mat[,c("one","gender", "timeDiff=2", "timeDiff=3", "timeDiff=4", "timeDiff=5", "timeDiff=6",
"timeDiff>=7")], corstr="userdefined")
\end{verbatim}

	\subsubsection{Parameter interpretation}

	\subsection{Generalized Linear Mixed Effects Models (GLMM)}
	\subsubsection{Model Definition}
	\subsubsection{Method of Solving}
	\subsubsection{Hypothesis Testing}
	\subsubsection{Parameter interpretation}
		\begin{tabular}{ | l | l | c | c | }
 \hline
		  Outcome & Coefficient & Random Intercept & Random Intercept/Slope \\
		  \hline
		    &   &   &   \\
		  Continuous & Intercept & Marginal & Marginal \\
		    & Slope & Marginal & Marginal \\
		    &   &   &   \\
		  Count & Intercept & Conditional & Conditional \\
		    & Slope & Marginal & Conditional \\
		    &   &   &   \\
		  Binary & Intercept & Conditional & Conditional \\
		    & Slope & Conditional & Conditional \\
				\hline
		\end{tabular}

	\subsubsection{Model classification}

	\begin{tabular}{|l|| c | c |}
	  \hline
	  \hline
	  ~ & Parametric Likelihood & Semiparametric\\
	  \hline
	  \hline
	  Marginal & MM-LV, MM-T, MM-TLV & GEE-I, GEE-AR, ALR  \\
	  \hline
	  Conditional (subject specific) & GLMM-RI, GLMM-RL &   \\
	  \hline
	  Conditional (transitional) & GLTM & \\
	  \hline
	\end{tabular}

	\subsubsection{Model Summary}
	\begin{tabular}{|l|| c| c | c |c|c|c|c}
	  \hline
	  \hline
	  Model & Description & Interp & Lik/Sem  & package & call & Mod.,Sl. & Section\\
	    &   &   & ML-max.lik  &   &   &   &  in this  \\
	    &   &   & RE-restr.ML  &   &   &   &   doc\\
	    &   &   & CL-cond.lik  &    &   &   &  \\
	    &   &   & QL-quasi lik  &    &   &   &  \\
	  \hline
	  \hline
	  GEE 1.0 &  $\sum D_i V_i(\alpha) (y_i - \mu_i) = 0$  & Popul& QL=semi &  &   \texttt{geepack,}   & 3   &       \\
	    &   poisson, binary  &  &   &  &   \texttt{geeglm}   & 3   &       \\
	    &       &  &   &  &   \texttt{geese}   & 3   &       \\
	  \hline
	  GEE 2.0 & GEE with Indep.  & Popul&semi  &   &      &  3  &       \\
	  \hline
	  GEE-I & GEE with Indep.  & Popul&semi  &   &    &    &       \\
	  (1.0 but it's not) & GEE with Indep.  & Popul&semi  &   &    &    &       \\
	  \hline
	  GEE-AR (1.0) & GEE with AR(1)  & Popul&semi  &   &    &     &      \\
	  \hline
	  GLMM-RI/RL & Mixed Effects,  &  & ML,RE,CL  &  &  \texttt{glmer}   &  4  &      \\
	     ~~continuous &    Rand. Int. /+Slope & Subj&   &   &   &    &      \\
	     ~~count &     & Subj&   &   &   &    &      \\
	     ~~binary &    & Subj&  &   &   &    &      \\
	  \hline
	  GLMM-RL & Mixed Effects,    &   & ML,RE,CL &  & \texttt{glmer}  & 4   &      \\
	   ~~continuous &   Rand. Int. /+Slope  &  Subj& &   &   &    &      \\
	   ~~count &    &  Subj&   &   &   &    &      \\
	   ~~binary &     &  Subj&   &   &   &    &      \\
	  \hline
	  ALR$\equiv$GEE 1.5 & Alternating Logist. Regr.  &  &semi &   &  \texttt{ordgee}  &  3,sl.64   &     \\
	   & of response dependence  &  &semi &   &  \texttt{ordgee}  &  3,sl.64   &     \\
	    & ALR is just an example  &  Popul.  &semi &   &  \texttt{ordgee}  &  3,sl.64   &     \\
	  \hline
	  GLTM & Gen.Lin.Transional  & Popul.$|Y_{j-1}$ & CL  &   &   \texttt{glm} &  5,sl.23   &    \\
	  \hline
	  MM-LV & Marginalized, Latent Var  & Popul &lik  &   &   \texttt{mm} &  5  &    \\
	  \hline
	  MM-T &  Marginalized, Transitional  & Popul &lik  &   &  \texttt{mm} &  5   &     \\
	  \hline
	  MM-TLV &  Marginalized, Trans.    & Popul &lik  &   &  \texttt{mm}  &  5  &     \\
	    &  Marginalized,   Latent Var   & Popul &lik  &   &  \texttt{mm}  &  5  &     \\
	    &  Marginalized, both   & Popul &lik  &   &  \texttt{mm}  &  5  &     \\
	  \hline
	  \hline
	  \hline
	\end{tabular}

Is GEE 2 quisi lik?
Choosing b/w model wiht a time lag, which model to use \\
Populaiton

	\subsubsection{Missing}
	For gee, glm, we can compute weights =1/(probability of being observed) and this is how we can deal with missing data.
	\subsubsection{R-code}

\subsection{Random facts}
GLM + robust standard errors = GEE + independence covariate structures (GEE-I)\\
If you fit GEE 2.0 then the second moment has to be something you are interested in. You estimate them together, but if you get second moment wrong, then the first moment might be wrong.\\
In GLMM(glmer() function) for not continuous outcome, if we get the correlaiton structure wrong then the first model would be biased.

\section{Computational Statistic}
\subsection{EM Algorithm}
This is taken from the book of Tishbarani and someone else (Matt's class).\\
~\\
{
\scriptsize{Suppose we have $N$ points $X_i$ that belong to a mixture of two distributions $\phi_{\theta_1}$ and $\phi_{\theta_2}$. And we don't know what point belong to what distribution. Ideally, we would like to maximize:
$$
\begin{aligned}
	L(\theta; \pmb{X}, \pmb{\Delta}) &= \prod_{i=1}^N [(1-\pi)\phi_{\theta_1}(x_i) + \pi\phi_{\theta_2}(x_i)]\\
	 &or~log~likelihood\\
	l(\theta; \pmb{X}, \pmb{\Delta}) &= \sum_{i=1}^N log[(1-\pi)\phi_{\theta_1}(x_i) + \pi\phi_{\theta_2}(x_i)]
\end{aligned}
$$
Where $\pmb{\Delta}$ is an indicator function of belonging to class two: $\Delta_i = I(X_i ~in~class~2)$
This likelihood is hard to maximize because it is the log of the sum. So we apply alternative approach.\\
~\\
Suppose we knew what the $\Delta_i$'s were. Then, we could have written the likelihood as:
$$
\begin{aligned}
	L(\theta; \pmb{X}, \pmb{\Delta}) &= \prod_{i=1}^N \left[\left((1-\pi)\phi_{\theta_1}(x_i)\right)^{(1-\Delta_i)} + (\pi\phi_{\theta_2}(x_i))^{\Delta_i}\right]\\
	 &or~log~likelihood\\
	l(\theta; \pmb{X}, \pmb{\Delta}) &= \sum_{i=1}^N log\left\{\left[\left((1-\pi)\phi_{\theta_1}(x_i)\right)^{(1-\Delta_i)} + (\pi\phi_{\theta_2}(x_i))^{\Delta_i}\right]\right\}\\
\end{aligned}
$$
But again, it is hard to maximize. Let's use Jensen's inequality that says that for $log(x)$, $E(log(x))\leq log(Ex)$. So instead of maximazing $E(log(x))$ we will maximize its lower bound $log(Ex)$. Or, $\sum log(x) \leq log(\sum x)$ So instead of $\sum_{i=1}^N log\left\{\left[\left((1-\pi)\phi_{\theta_1}(x_i)\right)^{(1-\Delta_i)} + (\pi\phi_{\theta_2}(x_i))^{\Delta_i}\right]\right\}$ we will maximize:
$$
\begin{aligned}
	\mathcal{l}(\theta; \pmb{X}, \pmb{\Delta}) &= \sum_{i=1}^N log\left\{\left[\left((1-\pi)\phi_{\theta_1}(x_i)\right)^{(1-\Delta_i)}\right\} + log \left\{(\pi\phi_{\theta_2}(x_i))^{\Delta_i}\right]\right\}=\\
	&=\sum_{i=1}^N \left[{(1-\Delta_i)}log\left\{\phi_{\theta_1}(x_i)\right\} + {\Delta_i}log \left\{\phi_{\theta_2}(x_i)\right\}\right] + \\
	&~~~~~~~~~~~~~~~~~~~ +\sum_{i=1}^N \left[{(1-\Delta_i)}log(1-\pi) + {\Delta_i}log (\pi)\right]\\
\end{aligned}
$$
\textbf{E-step}: We proceed iteratively by evaluating $\Delta_i$ using \emph{Bayes rule}:
$$
\begin{aligned}
	\hat{\Delta}_i = w_i = E(\Delta_i |\theta,\pmb{X}) = Pr(\Delta_i=1 |\theta,\pmb{X})
\end{aligned}
$$
\textbf{M-step}: And then by maximizing the following with respect to $\theta$ and $\pi$:
$$
\begin{aligned}
	\mathcal{l}(\theta; \pmb{X}, \pmb{\Delta})
	&=\sum_{i=1}^N \left[{(1-w_i)}log\left\{\phi_{\theta_1}(x_i)\right\} + {w_i}log \left\{\phi_{\theta_2}(x_i)\right\}\right] + \\
	&~~~~~~~~~~~~~~~~~~~ +\sum_{i=1}^N \left[{(1-w_i)}log(1-\pi) + {w_i}log (\pi)\right]\\
\end{aligned}
$$
}} %end of scriptsize font

\subsection{More thorough explanation.}
The following material is taken from some Stanford lectures (\emph{CS229 Lecture notes by Andrew Ng}).\\
~\\
The idea is that we have observed data and missing data. Let's denote the observation number as $i$. For simplicity, we consider a problem with a mixture distribution. The missing data is a group that each observation belongs to. Let's denote group as $g$ and $g^{(i)}$ as a group for $i^{th}$ observation. We  have an \emph{observed log-likelihood} $\sum_i log[p(x^{(i)};\theta)]$, we would like to maximize a \emph{complete likelihood}, $\sum_i  log\left[\sum_{g^{(i)}} p(x^{(i)},g^{(i)};\theta)\right]$:

$$
\begin{aligned}
	{l}(\theta) = \sum_i log[p(x^{(i)};\theta)] &= \sum_i  log\left[\sum_{g^{(i)}} p(x^{(i)},g^{(i)};\theta)\right] =  \sum_i  log\left[\sum_{g^{(i)}}Q_i(g^{(i)}) \frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})}\right]
\end{aligned}
$$

Note that $Q_i(g^{(i)})$ is some probability measure, that we choose to multiply and divide $p(x^{(i)},g^{(i)};\theta)$ by.

\emph{Complete likelihood} is usually too difficult to solve, so we solve a \emph{lower bound} of the \emph{complete likelihood}. We do this using the \emph{Jenson}'s inequality for log: $log(E)\geq E(log)$:\\
$$
\begin{aligned}
	{l}(\theta) &=  \sum_i  log\left[\sum_{g^{(i)}}Q_i(g^{(i)}) \frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})}\right] \geq \sum_i \sum_{g^{(i)}}Q_i(g^{(i)}) log\left[ \frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})}\right]
\end{aligned}
$$
For any choice of $Q_i$ the right side of the above expression gives a lower bound of ${l}(\theta)$, but we need the largest lower bound. It is provided by the condition that $\frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})} = c$, where $c$ is a constant and does not depend on $g^{(i)}$. This means that $Q_i(g^{(i)})$ should be proportional to $p(x^{(i)},g^{(i)}$, so we can write:
$$
\begin{aligned}
	Q_i(g^{(i)}) = \frac{p(x^{(i)},g^{(i)};\theta)}{\sum_g p(x^{(i)},g^{(i)};\theta)} = \frac{p(x^{(i)},g^{(i)};\theta)}{p(x^{i};\theta)} = p(g^{(i)}|x^{(i)};\theta)
\end{aligned}
$$
So, the \textbf{E-step} is to compute: $Q_i(g^{(i)})  = p(g^{(i)}|x^{(i)};\theta)$\\
The \textbf{M-step} is to maximize: $\sum_i \sum_{g^{(i)}}Q_i(g^{(i)}) log\left[ \frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})}\right]$

\subsubsection{Example with Poisson mixture distribution.}
Suppose 1500 gay men were surveyed and each was asked how many risky sexual encounters he had in the previous 30 days. Let $n_i$ denote the number of respondents reporting $i$ encounters, for $i=1,...,16$. The data is zero-inflated, so we come up with the following model. With probability $\alpha$ a respondent belongs to a group that always reports $0$ encounters, $z$. With probability $\beta$ a respondent belongs to a so-called typical group, $t$, that follows $Poisson(\mu)$ distribution. And with probability $1 - \alpha - \beta$ a respondent belongs to a so-called risky group, $p$, that follows $Poisson(\lambda)$ distribution.\\
The parameters are: $\theta = (\alpha, \beta, \mu, \lambda)$. First, we write the \emph{observed likelihood}:
$$
\begin{aligned}
	L &\propto \prod_{i=0}^{16}\left[\frac{ \pi(\theta)}{i!} \right]^{n_i} = \left(   \alpha + \beta  e^{-\mu} + (1-\alpha - \beta) e^{-\lambda}  \right)^{n_0} \prod _{i=1}^{16} \left[\frac{ \mu^ie^{-\mu} }{i!} \beta   +   \frac{ \lambda^i e^{-\lambda} }{i!} (1-\beta-\alpha) \right]^{n_i}\\
	  &where~\pi(\theta) = \alpha\cdot 1_{(i=0)} + \beta \mu^i exp(-\mu) + (1-\alpha - \beta) \lambda^i exp(-\lambda)
\end{aligned}
$$
The log-likelihood is:
$$
\begin{aligned}
	l &= n_0 ln\left(   \alpha + \beta  e^{-\mu} + (1-\alpha - \beta) e^{-\lambda}  \right)   \sum _{i=1}^{16} n_i ln \left[\frac{ \mu^ie^{-\mu} }{i!} \beta   +   \frac{ \lambda^i e^{-\lambda} }{i!} (1-\beta-\alpha) \right]\\
\end{aligned}
$$
Now it is time to come up with $Q_i(g^{(i)})$'s. They are (as previously said) conditional probabilities of having $i$ sexual encounters given that one of the three groups $z$, $t$, and $p$ (we omit the derivations, they are somewhat simple):
$$
\begin{aligned}
	Q_0(g^{(0)} = z) &= z_0(\theta) = \frac{\alpha}{\pi_0(\theta)}\\ 
	Q_i(g^{(i)} = z) &= 0,~~i>0\\ 
	Q_i(g^{(i)} = t) &= t_i(\theta) = \frac{\beta \mu^i e^{-\mu}}{\pi_i(\theta)}\\ 
	Q_i(g^{(i)} = p) &= p_i(\theta) = \frac{(1 - \alpha - \beta) \lambda^i e^{-\lambda}}{\pi_i(\theta)}\\ 
 \end{aligned}
$$
We continue with using notations $z_0$, $t_i$, and $p_i$ instead of $Q_i(g^{(i)})$. Let's write the \emph{lower bound} of the log-likelihood. Keeping in mind that we omit $i!$ along the way because it is a constant.

$$
\begin{aligned}
  ll &= \sum_i \sum_{g^{(i)}}Q_i(g^{(i)}) log\left[ \frac{p(x^{(i)},g^{(i)};\theta)}{Q_i(g^{(i)})}\right] =\\
	 &= n_0 z_0 ln\left(   \frac{\alpha}{z_0} \right)+  n_0 t_0 ln\left(\frac{\beta  e^{-\mu}}{t_0}\right)  +n_0 p_0 ln\left( \frac{(1-\alpha - \beta) e^{-\lambda}}{p_0}  \right)   \sum _{i=1}^{16} n_i  \left[ t_i ln \left(\frac{ \mu^ie^{-\mu}  \beta}{t_i}\right)   +  p_i ln \left(\frac{ \lambda^i e^{-\lambda}(1-\beta-\alpha) }{p_i}\right)  \right]\\
\end{aligned}
$$
Note that $ln\left(\frac{1}{t_i}\right)$ (or $z_0,~t_0,~p_0,~p_i$) is also a constant, so we have:

$$
\begin{aligned}
  ll &= n_0 z_0 ln\left( \alpha \right)+  n_0 t_0 ln\left(\beta  e^{-\mu}\right)  +n_0 p_0 ln\left( (1-\alpha - \beta) e^{-\lambda} \right)  + \\
	 &~~~~~~~~~~+ \sum _{i=1}^{16} \left[ n_i  t_i \left(  i\cdot ln(\mu) - \mu + ln(\beta)  \right)   +  n_i p_i ln \left(  i\cdot ln(\lambda) - \lambda + ln(1-\alpha -\beta)  \right)  \right]\\
\end{aligned}
$$

Derivatives:

$$
\begin{aligned}
  \frac{\partial ll}{\partial \alpha} &= \frac{n_0z_0}{\alpha} -  \frac{n_0p_0}{1-\alpha-\beta} -  \sum_{i=1}^{16}\frac{n_ip_i}{1-\alpha-\beta}=0 \\
  \frac{\partial ll}{\partial \beta}  &= \frac{n_0t_0}{\beta} -  \frac{n_0p_0}{1-\alpha-\beta} + \sum_{i=1}^{16}\frac{n_it_i}{\beta} -  \sum_{i=1}^{16}\frac{n_ip_i}{1-\alpha-\beta}=0 \\
  \frac{\partial ll}{\partial \mu}    &= -n_0t_0 +  \sum_{i=1}^{16}\frac{n_i t_i i}{\mu} - \sum_{i=1}^{16}n_i t_i i =0\\
  \frac{\partial ll}{\partial \lambda}    &= -n_0p_0 +  \sum_{i=1}^{16}\frac{n_i p_i i}{\lambda} - \sum_{i=1}^{16}n_i p_i i =0 \\
\end{aligned}
$$

For $\mu$ and $\lambda$ it is pretty straight forward:
$$
\begin{aligned}
  \hat{\mu} = \frac{\sum_{i=0}^{16} n_i t_i i }{\sum_{i=0}^{16} n_i t_i}\\
  \hat{\lambda} = \frac{\sum_{i=0}^{16} n_i p_i i }{\sum_{i=0}^{16} n_i p_i}\\
\end{aligned}
$$
For $\alpha$ and $\beta$ we have:
$$
\begin{aligned}
  \alpha &= \frac{ n_0 z_0   }{\sum_{i=0}^{16} n_i p_i   + n_0z_0}(1-\beta)\\
  \beta &= \frac{ \sum_{i=0}^{16} n_i t_i}{\sum_{i=0}^{16} n_i t_i + \sum_{i=0}^{16} n_i p_i   + n_0z_0}\ (1-\alpha)  \
\end{aligned}
$$
We keep in mind that $\sum_{i=0}^{16} n_i t_i + \sum_{i=0}^{16} n_i p_i   + n_0z_0 = N$ (total counts), and after performing some algebra a lot of stuff cancels out:
$$
\begin{aligned}
  \hat{\alpha} &= \frac{ n_0 z_0 }{N}\\
  \hat{\beta} &= \frac{ \sum_{i=0}^{16} n_i t_i}{N}\\
\end{aligned}
$$

So the \textbf{E-step} is computing $z_i(\hat{\theta})$, $t_i(\hat{\theta})$, and $p_i(\hat{\theta})$, and the \textbf{M-step} is computing $\alpha$, $\beta$, $\mu$, and $\lambda$.


\subsection{Metropolis-Hastings Algorithm}
Under construction
% The following is from the Wikipedia:\\
% \begin{enumerate}[1)]
% 	\item Choose and arbitrary point $x_0$
% \end{enumerate}
	
\section{Bayesian Stuff}
Book: \textbf{Gelman, Bayesian Data Analysis}
\subsection{Two parameters. Normal distribution's conjugate prior.}

\subsection{Two parameters. Only one is the paprameter of interests.}
Two paramters Bayesian approach looks like this:
$$
\begin{aligned}
	p(\theta_1, \theta_2|y) \propto p(y|\theta_1, \theta_2)p(\theta_1, \theta_2)
\end{aligned}
$$
We are interesting in only $\theta_1$. We can get it by either intergrating out $\theta_2$: $\int p(\theta_1, \theta_2|y) d\theta_2$ or we can factor out the joint posterior density first and then integrate $\theta_2$ out:\\
$\int p(\theta_1|\theta_2,y)p(\theta_2|y) d\theta_2$.
$$
\begin{aligned}
	&Method~1:~~~\int p(\theta_1, \theta_2|y) d\theta_2\\
	&Method~2:~~~\int p(\theta_1|\theta_2,y)p(\theta_2|y) d\theta_2
\end{aligned}
$$
The second way is usually implemented numerically. We first draw $\theta_2$ and then $\theta_1$ given $\theta_2$.

\subsubsection{Normal distribution, non-informative prior. Section 3.2, page 64}
Here, we show an example of how to implement Method 2.
Our \textbf{non-informative prior} is:
$$
\begin{aligned}
	p(\mu, \sigma^2) \propto \left( \sigma^2 \right)^{-1}
\end{aligned}
$$
Our \textbf{joint posterior distribution} is:
$$
\begin{aligned}
	p(\mu, \sigma^2|y) &\propto  \sigma^{-n-2} exp\left(\frac{1}{  -2\sigma^2 }   \sum_{i=1}^n(y_i - \mu)^2 \right) =...\\
	                 & =  \sigma^{-n-2} exp\left(\frac{1}{  -2\sigma^2 }   \left[  (n-1)s^2 + n(\bar{y} - \mu)^2 \right] \right)\\
									 &~~~~~~~~where~~s^2 = \frac{1}{n-1}\sum_{i=1}^n(y_i - \bar{y})^2
\end{aligned}
$$
We already know the \textbf{marginal posterior of $\mu$ given $\sigma^2$} from section 2.5:
$$
\begin{aligned}
	\mu| \sigma^2,y \sim N\left(\bar{y}, \sigma^2/n \right).
\end{aligned}
$$

We find the \textbf{marginal posterior of $\sigma^2$} by integrating out $\mu$ from $p(\mu, \sigma^2)$ (see above). See formula (3.4) in the book:
$$
\begin{aligned}
	p(\sigma^2|y) &\propto  \sigma^{-(n+1)/2} exp\left(\frac{ (n-1)s^2}{  2\sigma^2 }  \right) =\\
	   & \sim Inv~\chi^2(n-1, s^2)
\end{aligned}
$$
So, we factored out the joint distribution: $p(\mu, \sigma^2|y) = p(\mu|sigma^2, y)p(sigma^2| y)$.\\
\\
Analytic form of the $p(\mu|\sigma^2,y)$ also can be obtained and it is $t_{n-1}(\bar{y}, s^2/n)$. Or, after normalizing $\mu$, we get:
$$
\begin{aligned}
	\frac{\mu - \bar{y}}{s/\sqrt{n}}|y\sim  t_{n-1}
\end{aligned}
$$

Interestingly, the \textbf{marginal posterior distribution} $p(y|\mu,sigma^2)$ is
$$
\begin{aligned}
	\frac{\mu - \bar{y}}{s/\sqrt{n}}|y\sim  t_{n-1}
\end{aligned}
$$
The \textbf{posterior predictive distribution for a future observation} is :

$$
\begin{aligned}
	p(\tilde{y}|y) &= \int \int p(\tilde{y}|\mu, \sigma^2, y)p(\mu, \sigma^2 | y)d\mu d\sigma^2\\
	 &=t_{n-1}\left(\bar{y}, \left(1+\frac{1}{n}\right)s^2\right)
\end{aligned}
$$



\subsubsection{Normal distribution, conjugate prior. Section 3.3, page 67}
The conjugate prior looks like this:
$$
\begin{aligned}
	\mu|\sigma^2 &\sim N(\mu_0, \sigma^2/k_0)\\
	     \sigma^2 &\sim Inv\mbox{-}\chi^2(\nu_0, \sigma_0^2)\\
			&OR\\
			p(\mu, \sigma^2) &\propto \sigma^{-1}(\sigma^2)^{-(\nu/2 + 1)}exp\left( -\frac{1}{2\sigma^2}\left[\nu_0\sigma_0^2 + k_0(\mu_0 - \mu)^2 \right]  \right)\\
			&OR\\
	N \mbox{-} Inv \mbox{-} \chi^2&(\mu_0, \sigma_0^2/k_0, \nu_0, \sigma_0^2)
\end{aligned}
$$

The join posterior is:
$$
\begin{aligned}
			p(\mu, \sigma^2) &\propto N \mbox{-} Inv \mbox{-} \chi^2(\mu_n, \sigma_n^2/k_n; \nu_n, \sigma_n^2)\\
			&where:\\
			\mu_n &= \frac{k_0}{k_0 + n}\mu_0  +  \frac{n}{k_0 + n}\bar{y}\\
			k_n   &= k_0 + n\\
			\nu_n &= \nu_0 + n\\
			\nu_n \sigma_n^2 &= \nu_0 \sigma_0^2 + (n-1)s^2 + \frac{k_0n}{k_0 + n}(\bar{y - \mu_0})^2
\end{aligned}
$$

The \textbf{conditional posterior density of $\mu$ given $\sigma^2$}

$$
\begin{aligned}
			\mu|\sigma^2,y \sim N\left(  \mu_n, \sigma^2/k_n \right) = N\left( \frac{ \frac{k_0}{\sigma^2}\mu_0 +  \frac{n}{\sigma^2}\bar{y} }{  \frac{k_0}{\sigma^2} +  \frac{n}{\sigma^2}  },  \frac{1 }{  \frac{k_0}{\sigma^2} +  \frac{n}{\sigma^2}  }  \right)
\end{aligned}
$$

The \textbf{marginal posterior distribution, $p(\sigma^2|y)$}

$$
\begin{aligned}
			\sigma^2|y \sim  Inv\mbox{-}\chi^2 \left(\nu_n, \sigma_n^2  \right)
\end{aligned}
$$


The \textbf{analytic form of the marginal posterior distribution, $\mu$}
$$
\begin{aligned}
			\mu|y &\propto  \left( 1+ \frac{k_n(\mu - \mu_n)^2}{  \nu_n\sigma_n^2  }  \right) \\
			       &=t_{\nu_n}  \left(  \mu|\mu_n, \sigma_n^2/k_n   \right)
\end{aligned}
$$

\section{Hierarchical Model for Binomial Data, p. 107}
Let's assume that $\theta$ is a population parameter of interest. For example, it could be a prevalence of tumor in each rat experiment.\\
Let's also assume that $\theta$ has its own distribution $p(\phi)$. Therefore the joint \textbf{prior} and the joint \textbf{posterior} distributions are:
$$
\begin{aligned}
  p(\phi, \theta) &= p(\phi)\cdot p(\theta|\phi)\\
  p(\phi, \theta|y) &= \frac{p(y|\theta, \phi)\cdot p(\theta, \phi)}{p(y)} \propto p(\phi, \theta)\cdot p(y|\theta, \phi)= p(\phi, \theta)\cdot p(y|\theta) =\\
	&\propto p(\theta|\phi)\cdot p(\phi) \cdot p(y|\theta)\\
\end{aligned}
$$
Whether we are interested in prediction $\tilde{y}$ or the inference about $\theta$, we need $p(\phi|y)$ in order to sample from $p(\theta|\phi,y)$, so we could further sample from $p(y|\theta)$.\\
~\\
So the steps of for solving the hierarchical model are the following:
\begin{enumerate}[1)]
	\item Define the model: $p(\phi, \theta|y)\propto p(\theta|\phi)\cdot p(\phi) \cdot p(y|\theta)$
	\item Find $p(\theta|\phi,y)$ - I don't know how they do that???.
	\item Find $p(\phi|y)$ using one of the following ways:
	\begin{enumerate}[a)]
		\item $p(\phi|y) = \int p(\theta,\phi|y)d\theta$.
		\item $p(\phi|y) = \frac{p(\theta,\phi|y)}{p(\theta|\phi,y)}$. This way might not be valid because the normalizing constant may depend on $\phi$ and $y$.
	\end{enumerate}
\end{enumerate}
~\\
What follows is way to simulate draws from the joint posterior distribution, $p(\phi, \theta|y)$.
\begin{enumerate}[1)]
	\item Draw the vector of hyperparameters, $\phi$ from its marginal posterior distribution $p(\phi|y)$.
	\item Draw the parameter vector $\theta$ from its conditional posterior distribution $p(\theta|\phi,y)$ given $\phi$.
	\item if desired, draw $\tilde{y}$ from the posterior predictive distribution given the drawn $\theta$. Depending on the problem it might be necessary to draw a new value $\tilde{\theta}$, give $\phi$.
\end{enumerate}





\end{document}          
