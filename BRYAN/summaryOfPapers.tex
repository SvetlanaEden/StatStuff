\documentclass[]{article}

\usepackage{anysize}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}

%%% for citations. See Lucy's tutorial: https://github.com/LucyMcGowan/Tutorials/blob/master/BiblatexTutorial.md
\usepackage[backend=biber,maxnames=10,citestyle=science]{biblatex}
\addbibresource{summaryOfRef.bib}

%%% when using biber as a "backend" compile like this:
% pdflatex myFile
% biber myFile
% pdflatex myFile


\let\epsilon\varepsilon
\newcommand\SLASH{\char`\\}
\newcommand{\csch}{\text{csch}}
\marginsize{0.5in}{1in}{0.5in}{1in}
\setlength\parindent{0pt}

% Title Page: Modify as needed
\title{Summary for Bryan's research}
\author{Summarized by Svetlana Eden}
\date{Month Day, 2015}

\hypersetup{hidelinks}

% \usepackage[pdftex,bookmarks,pagebackref,pdfpagemode=UseOutlines,
%      colorlinks,linkcolor=\linkcol,
%      pdfauthor={Svetlana K Eden},
%      pdftitle={\titl}]{hyperref}


\begin{document}
\maketitle
\tableofcontents
\listoffigures
\listoftables
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matrix algebra
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Matrix algebra
\section{Test of Association Between Two Ordinal Variables While Adjusting for Covariates, Li and Shepherd, 2010 \cite{li2010test}}
We have two ordinal variables $\pmb{X}$ and $\pmb{Y}$ and we would like to test if after adjustment for $\pmb{Z}$ they are independent. We do this in two ways:
\begin{enumerate}[1)]
	\item compare \emph{observed} distribution with \emph{expected} distribution under the null.
	\item run two regressions $\pmb{X\sim Z}$ and $\pmb{Y\sim Z}$, obtain their residuals, $r_x$ and $r_y$ and test them (???)
\end{enumerate}
\subsection{Observed vs expected distributions statistic, $T_1$}
In general, the joint distribution can be written:
$$
\begin{aligned}
	P(Y,X) = \int_z P(Y,X|\pmb{Z})dP(\pmb{Z})
\end{aligned}
$$
Under the null of conditional independence:
$$
\begin{aligned}
	P_0(Y,X) = \int_z P(Y|\pmb{Z})P(X|\pmb{Z})dP(\pmb{Z})
\end{aligned}
$$
More notations:
$$
\begin{aligned}
  p_{i}^j &= P(Y_i = j|\pmb{Z}_i = \pmb{z}_i),~~j=1,...,s\\
  q_{i}^l &= P(X_i = l|\pmb{Z}_i = \pmb{z}_i),~~l=1,...,t\\
	&\gamma_i^j=P(Y_i\leq j|\pmb{Z}=\pmb{z}_i), ~~p_{i}^j = \gamma^{j}_i - \gamma^{j-1}_i\\
	 &similar~thing~for~q_i^l
\end{aligned}
$$
Now we get estimates $\hat{p}_i^j$ and $\hat{q}_i^l$ from regressions $\pmb{X~Z}$ and $\pmb{Y~Z}$ and then estimate the null distribution:
$$
\begin{aligned}
  \hat{P}_0 = \left\{\hat{\pi}_0^{jl}\right\},~~where~~\hat{\pi}_0^{jl} = \frac{1}{n}\sum_i \hat{p}_i^j \cdot \hat{q}_i^l
\end{aligned}
$$
Now we estimate the alternative distribution:
$$
\begin{aligned}
  \hat{P}_{jl} = \frac{n_{jl}}{n}
\end{aligned}
$$
Where $n_{jl}$ is number of subjects with levels j and l respectively.\\
We now summarize the observed and expected distribution separately by calculating \emph{Goodman} and \emph{Kruskal}'s gamma \cite{goodman1979measures} (Goodman, L. A., and Kruskal, W. H. (1954), "Measures of Association for Cross Classifications," Journal of the American Statistical Association).
$$
\begin{aligned}
	C&=\sum_{j_1<j_2, ~l_1<l_2}\pi_{j_1l_1}\pi_{j_2l_2}\\
	D&=\sum_{j_1<j_2, ~l_1>l_2}\pi_{j_1l_1}\pi_{j_2l_2}\\
  \Gamma &= \frac{C-D}{C+D}\\
\end{aligned}
$$
{\scriptsize{\textbf{Note:} Strictly speaking, $C$ and $D$ from above have to be multiplied by $2$ (see section \ref{sssec:GoodmanKruskal}). But because 2 cancels out, I guess, they omit the $2$}}\\
Let $\Gamma_1 = \Gamma(\hat{P})$ and  $\Gamma_0 = \Gamma(\hat{P}_0)$ be the gamma for the observed and expected distributions for $(Y,X)$. The test statistics is $T_1=\Gamma_1 - \Gamma_0$. When the multinomial models are correct (?!), under the null:
$$
\begin{aligned}
	\hat{P}_0 \rightarrow P,~~~\hat{P} \rightarrow P\\
	T_1 = \Gamma(\hat{P}) - \Gamma(\hat{P_0}) \underset{n\rightarrow \infty}{\longrightarrow} 0
\end{aligned}
$$
\subsubsection{\emph{Goodman} and \emph{Kruskal}'s gamma}
\label{sssec:GoodmanKruskal}
We have two $iid$ subjects $1$ and $2$ with two ordered responses $\underline{a}_1,\underline{b}_1$ and $\underline{a}_2,\underline{b}_2$ that have categories $1,2,...,s$, and $1,2,...,t$. If we summarised proportions of each combination in a table we would use the following notations:\\
~\\
\begin{tabular}{ l | c | c | c | c | c}
  \hline
  ~ & $B_1$ & $B_2$ & ... & $B_t$ & Total\\
  \hline
  $A_1$ & $\rho_{11}$&$\rho_{12}$&... &$\rho_{1t}$& $\rho_{1\cdot}$\\
  $A_2$ & $\rho_{21}$&$\rho_{22}$&... &$\rho_{2t}$& $\rho_{2\cdot}$\\
  ... & ...&...&... &...& ...\\
  $A_s$ & $\rho_{s1}$&$\rho_{s2}$&... &$\rho_{st}$& $\rho_{1\cdot}$\\
  \hline
  Total & $\rho_{\cdot 1}$& $\rho_{\cdot 2}$&... & $\rho_{\cdot t}$& 1\\
\end{tabular}
~\\


Let's denote:
$$
\begin{aligned}
	\Pi_c&=Pr\left\{ \underline{a}_1 <\underline{a}_2 ~and ~\underline{b}_1<\underline{b}_2;~~or~\underline{a}_1 >\underline{a}_2 ~and ~\underline{b}_1>\underline{b}_2\right\}\\
	\Pi_d&=Pr\left\{ \underline{a}_1 <\underline{a}_2 ~and ~\underline{b}_1>\underline{b}_2;~~or~\underline{a}_1 >\underline{a}_2 ~and ~\underline{b}_1<\underline{b}_2\right\}\\
	\Pi_t&=Pr\left\{\underline{a}_1 =\underline{a}_2 ~or ~\underline{b}_1=\underline{b}_2\right\}\\
\end{aligned}
$$
From the above, the conditional probability for the \emph{like} order given no ties is $\Pi_c/(1-\Pi_t)$ and the conditional probability fo the $unlike$ order given no ties is $\Pi_d/(1-\Pi_t)$. Keeping in mind that $\Pi_c +\Pi_d +\Pi_t=1$, a possible measure of association, $\gamma$ is:
$$
\begin{aligned}
	\gamma = \frac{\Pi_c - \Pi_d}{1-\Pi_t} =  \frac{\Pi_c - \Pi_d}{\Pi_c + \Pi_d} = \frac{2\Pi_c - 1 + \Pi_t}{1- \Pi_t}
\end{aligned}
$$
This measure can be interpreted as \emph{difference} between the conditional probabilites of like and unlike orders: how much more probable it is to get like than unlike orders in the two classifications when two individuals are chosen randomly from the population.\\
The latter equality is especially usefull because we can compute it using easily verifyable relationships:
$$
\begin{aligned}
	\Pi_c&=2\sum_a \sum_b \rho_{ab}\left\{ \sum_{a'>a} \sum_{b'>b} \rho_{a'b'} \right\}\\
	\Pi_t&=\sum_a \rho_{a\cdot}^2 + \sum_b \rho_{\cdot b}^2 - \sum_{a} \sum_{b} \rho_{ab}^2\\
\end{aligned}
$$

{\scriptsize{\textbf{Verification}\\
We know that $P(A\cup B) = P(A) + P(B) - P(A\cap B)$. Let's denote $A=\{\underline{a}_1 =\underline{a}_2\}$ and  $B=\{\underline{b}_1 =\underline{b}_2\}$. We think of two subjects having the same categories is like drawing from the cells of the above table with replacement because two subjects are independent. Keeping in mind that $P(\{\underline{a}_1=A_i\cap \underline{b}_1 =B_j) =\rho_{A_i,B_j}$, the probability of two subjects having the same categories is $\rho_{A_i,B_j}\cdot \rho_{A_i,B_j}$ and therefore $P(A\cap B) = \sum_{i,j}\rho_{i,j}^2$. Similarly, and imagining that we draw two subjects independently with replacement based on marginal distributions for $A$ and $B$: $P(A) = \sum_{i}\rho_{i\cdot}^2$ and $P(B) = \sum_{j}\rho_{\cdot j}^2$
\\
Now let's denote $A=\{\underline{a}_1 <\underline{a}_2 \cap\underline{b}_1 <\underline{b}_2\}$ and  $B=\{\underline{a}_1 >\underline{a}_2 \cap \underline{b}_1 >\underline{b}_2\}$. Because these two events do not overlap, we have: $P(A\cup B) = P(A) + P(B)$. Applying the same principle of drawing with replacement, we have: $P(A) = \sum_{i,j}\left(\sum_{k<i,l<j}\rho_{k,l} \right)\rho_{i,j}$ and $P(B) = \sum_{i,j}\left(\sum_{k>i,l>j}\rho_{k,l} \right)\rho_{i,j}$. It so happends that when these two probabilities are summed up, we get the double of one of them. We show it on the example of 2 $A$ categories and three $B$ categories: $P(A\cup B) = [\rho_{11}\rho_{22} + (\rho_{12} + \rho_{11})\rho_{23}]~ + ~[(\rho_{22} + \rho_{23})\rho_{11} + \rho_{23}\rho_{12}] ~=~ 2(\rho_{22} + \rho_{23})\rho_{11} + 2\rho_{12}\rho_{23}$, so $P(A\cap B) = 2\cdot \sum_{i,j}\left(\sum_{k<i,l<j}\rho_{k,l} \right)\rho_{i,j}$
\\
\textbf{Note}(Unrelated to the verification above)\\
In the same paper, there is also an index, $\lambda$, that measures association between unordered categories. Another interesting part of the paper is where they derive the same index $\lambda$ but using a loss function.
}}\\

Properties:
\begin{enumerate}[(i)]
	\item $\gamma$ is indeterminate if the population is concentrated in a single row or column of the cross-classification table.
	\item $\gamma$ is $1$ if the population is concentrated in a upper-left to lower-right diagonal of the cross classification table. $\gamma$ is $-1$ if the population is concentrated in a lower-left to upper-right diagonal of the table. It can be diagonal or step-diagonal, because the conflinct in order is impossible:\\
	~\\
	\begin{tabular}{ c | c | c }
	  $\rho_{11}$ & $\rho_{12}$ & 0\\
		\hline
	  0 & $\rho_{22}$ & $\rho_{23}$\\
		\hline
	  0 & 0 & $\rho_{33}$\\
	\end{tabular}
	~\\
	\item $\gamma$ is $0$ in the case of independence, but the converse need not hold except in the $2\times 2$ case. Example:\\
	~\\
	\begin{tabular}{ c | c | c }
	  .2 & 0 & .2\\
		\hline
	  0 & .2 & 0\\
		\hline
	  .2 & 0 & .2\\
	\end{tabular}
	~\\
	\item In $2\times 2$ case, $\gamma$ is Yule's coefficient of association $\gamma=\frac{\rho_{11}\rho_{22} - \rho_{12}\rho_{21}}{\rho_{11}\rho_{11}+\rho_{12}\rho_{21}}$
  \item Also, in some way, it is similar to Stewart's measure.
\end{enumerate}

\subsection{Residual-based test statistic, $T_2$}
For simplicity, only notations for $Y$ are introduced, the notations for $X$ are similar.\\
Assume that $Y$ is distributed multinomially with parameter $\theta^Y$. Let $Y_i=y_i$ be the observed outcome for subject $i$. The corresponding distribution of possible outcome levels $Y_{i,fit}\sim Y_i|\pmb{z}_i$ given covariate $\pmb{z}_i$ is multinomial with probability $\{p_i^j\}$. Instead of calculating the difference between $Y_{i,fit}$ and $y_i$ we compare them and give them scores of $1$, $-1$, and $0$ if $y_i>Y_{i,fit}$, $y_i<Y_{i,fit}$, and $y_i=Y_{i,fit}$ respectively.\\
Keeping in mind that $Y_{i,fit}$ is the \emph{random variable} in the following derivations:
$$
\begin{aligned}
  p_{i,high} &= P(y_i>Y_{i,fit}) = P(Y_{i,fit}<y_i) = P(Y_i \leq y_i - 1|\pmb{Z}=\pmb{z}_i) = \gamma_i^{y_i-1}\\
  p_{i,low} &= P(y_i<Y_{i,fit}) = P(Y_{i,fit}>y_i)= 1 - P(Y_i \leq y_i|\pmb{Z}=\pmb{z}_i) = 1-\gamma_i^{y_i}\\
  p_{i,tie} &= P(y_i=Y_{i,fit}) = P(Y_{i,fit}=y_i)= p_i^{y_i}
\end{aligned}
$$
The expected score $Y_{i,res} = p_{i,high} - p_{i,low}$, which is a function of data $(Y_i, \pmb{Z}_i)$ and $\theta^Y$, and we denote $Y_{i,res}$ realization as $y_{i,res} = Y_{i, res|\hat{\theta}^Y}$. Similarly, for $X$, we define: $X_{i,res} = q_{i,high} - q_{i,low}$ and $x_{i,res}$.\\
When the models for $P(X|\pmb{Z})$ and  $P(Y|\pmb{Z})$ are correct, and keeping in mind that $p_i^j = \gamma_i^j - \gamma_i^{j-1}$ and $\gamma_i^j = \sum_{k = 1}^{j}p_i^k$, we have:
$$
\begin{aligned}
  E(p_{i,high}|\pmb{Z}_i) &= E \left(\gamma_i^{Y_i-1} |\pmb{Z}_i\right) = \sum_{j=1}^s p_i^j \gamma_i^{j-1}=\sum_{j=1}^s p_i^j \left(\sum_{k = 1}^{j-1}p_i^k\right) = \sum_{j_1<j_2} p_i^{j_1} p_i^{j_2}\\
  E(p_{i,low}|\pmb{Z}_i) &= E \left(1-\gamma_i^{Y_i} |\pmb{Z}_i\right) = \sum_{j=1}^s p_i^j (1-\gamma_i^{j}) = \sum_{j=1}^s p_i^j \left(1-\sum_{k = 1}^{j}p_i^k\right) = \sum_{j=1}^s p_i^j \left(\sum_{k = j+1}^{s}p_i^k\right) = \sum_{j_1<j_2} p_i^{j_1} p_i^{j_2}\\
\end{aligned}
$$
Therefore $E(p_{i,high} - p_{i,low}|\pmb{Z}_i)=0$ and $E(Y_{i,res})=E[E(p_{i,high} - p_{i,low}|\pmb{Z}_i)]=0$. Similarly, for $E(X_{i,res})=0$ and:
$$
\begin{aligned}
	E(Y_{i,res},X_{i,res}) &= E[E(Y_{i,res},X_{i,res}|\pmb{Z}_i)] = E[E(Y_{i,res}|\pmb{Z}_i)E(X_{i,res}|\pmb{Z}_i)] = 0\\
	&therefore\\
	cov(Y_{i,res},X_{i,res}) &= E(Y_{i,res},X_{i,res}) - E(Y_{i,res})E(X_{i,res}) = 0\\
\end{aligned}
$$
Therefore, we use the sample correlation coefficient, $T_2$, between $Y_{i,res}$ and $Y_{i,res}$ across all subjects as a test statistics. Under the null, $T_2\underset{n\rightarrow \infty}{\longrightarrow}0$

\subsection{A variation of residual statistic, $T_3$}
Alternatively, we can compute a statistic based on concordance vs discordance of pairs $(Y_i, X_i)$ and $(Y_i', X_i')$:
$$
\begin{aligned}
	Concordant ~pairs: ~Y_i>X_i ~and ~Y_i'>X_i'~~or~~ Y_i<X_i ~or~ Y_i'<X_i'\\
	Discordant ~pairs: ~Y_i>X_i ~and ~Y_i'<X_i'~~or~~ Y_i<X_i ~or~ Y_i'>X_i'\\
	Tied ~pairs: otherwise\\
\end{aligned}
$$
 Under the null the probability of concordance: $C_i = p_{i,high}q_{i,high} + p_{i,low}q_{i,low}$, and the probability of discordance $D_i = p_{i,high}q_{i,low} + p_{i,low}q_{i,high}$. 
$$
\begin{aligned}
	C_i - D_i &= p_{i,high}q_{i,high} + p_{i,low}q_{i,low} -p_{i,high}q_{i,low} - p_{i,low}q_{i,high}=\\
	 &= (p_{i,high}-p_{i,low}) (q_{i,high} - q_{i,low})=\\
	 &=Y_{i,res}X_{i,res}\\
\end{aligned}
$$
We know that $E(C_i - D_i)=E(Y_{i,res}X_{i,res}) = 0$, so we suggest a third statistic:
$$
\begin{aligned}
	T_3 = E(\hat{C}_i - \hat{D}_i) &=\frac{1}{n} \sum_i y_{i,res}x_{i,res}\\
\end{aligned}
$$
\subsection{Two method of computing the p-value}
\subsubsection{Parametric Bootstrap}
We generate new datasets from the product distribution $\{\hat{p}_i^j\hat{q}_i^l\}_{j,l}$ for each subject ($i=1,...,n$). Then we carry out the entire estimating procedure for the replicate dataset to obtain the corresponding statisitc, denoted as $T^*$. We do this $N^{emp}$ times and get an empirical distribution of $T$ under the null. The two-sided p-value is then computed as either:
$$
\begin{aligned}
	\#(|T^*|\geq |T|)/N^{emp}
\end{aligned}
$$
or
$$
\begin{aligned}
	2\times min\{\#(T^*\geq T),\#(T^*\leq T)\}/N^{emp}
\end{aligned}
$$
\subsubsection{Asymptotic Distribution}
Estimate $\hat{\theta}$ can be obtained by solving the equation $\sum_{i=1}^n \Psi_i(\theta) = 0$ where $\Psi_i(\theta) = \Psi(Y_i, X_i, \pmb{Z}_i; \theta)$ is a \emph{p}-variate function that doesn't depend on $i$ or $n$ and satisfies $E_{\theta}[\Psi_i(\theta)]=0$. From \emph{M}-estimation theory (Stefanski and Boos 2002)\cite{stefanski2002calculus}, if $\Psi$ is suitable smooth, then as $n\leftarrow \infty$,
$$
\begin{aligned}
	\sqrt{n}(\hat{\pmb{\theta}} - \pmb{\theta}) &\underset{d}{\longrightarrow}N(\pmb{0}, V(\theta)),~~where\\
	  V(\theta) &= A(\theta)^{-1} B(\theta)[A(\theta)^{-1}]'\\
		A(\theta) &= E \left[-\frac{\partial}{\partial \theta} \Psi_i(\theta)\right]\\
		B(\theta) &= E \left[\Psi_i(\theta)\Psi_i'(\theta)\right]\\
\end{aligned}
$$
 {\scriptsize{\textbf{My explanation}(might not to be correct): Because $E_{\theta}[\Psi(\theta)]=\pmb{0}$}, the variance of $\Psi(\theta)$ is $B(\theta) = E \left[\Psi_i(\theta)\Psi_i'(\theta)\right]$, therefore $\sqrt{n}(\hat{\Psi}(\theta) - \Psi(\theta)) \underset{d}{\longrightarrow}N(\pmb{0}, B(\theta))$. We can express $\theta$ as $\Psi^{-1}_i(\nu)$ (an inverse of $\Psi$, do not confuse with power $-1$) and apply the delta method keeping in mind that $\frac{\partial \Psi^{-1}_i(\theta)}{\partial \nu} = -\frac{1}{\partial \Psi_i(\nu)/ \partial \theta}$. Then, according to the delta method: $\sqrt{n}(\hat{\theta} - \theta) \underset{d}{\longrightarrow}N\left(\pmb{0}, \frac{B(\theta)}{(\partial (\Psi^{-1}_i(\theta)) /\partial \theta)^2}\right)$ (here $-1$ is used in sense of power). My $i$ indices are all messed up in this explanation.
 }\\
 ~\\
Now we apply \emph{delta} method to a statistic, based on $\theta$:

$$
\begin{aligned}
	\sqrt{n}(g(\hat{\pmb{\theta}}) - g(\pmb{\theta})) &\underset{d}{\longrightarrow}N(0, \sigma^2),~~where\\
	  \sigma^2 &= \left[\frac{\partial}{\partial \theta}g(\pmb{\theta})\right]V(\theta)\left[\frac{\partial}{\partial \theta}g(\pmb{\theta})\right]'
\end{aligned}
$$
In order to estimate $\sigma^2$ we estimate:

$$
\begin{aligned}
	\hat{A}(\pmb{\theta}) &= \frac{1}{n}\sum_{i}\left[-\frac{\partial}{\partial\pmb{\theta}}\Psi_i(\hat{\pmb{\theta}})\right]\\
	\hat{B}(\pmb{\theta}) &= \frac{1}{n}\sum_{i}\Psi_i(\hat{\pmb{\theta}})\Psi_i(\hat{\pmb{\theta}})'\\
	\widehat{\frac{\partial}{\partial\pmb{\theta}}g(\pmb{\theta})} &= \frac{\partial}{\partial\pmb{\theta}}g(\hat{\pmb{\theta}})
\end{aligned}
$$
Under the null the \emph{p-value} can be computed approximately as $2\Phi\left(\frac{-|T|}{\hat{simga}/n}\right)$, where $\Phi$ is the cumulative distribution function of the standard normal distribution.

\subsubsection{Defining $\theta$, $\Psi_i(\theta)$, and $g(\theta)$}
For all three statistics, the parameter vector will have the form: $\pmb{\theta} = (\pmb{\theta}^Y, \pmb{\theta}^X, \pmb{\theta}^T)$. The corresponding estimating function $\Psi_i(\pmb{\theta})$ will have the form:
$$
\begin{aligned}
	\Psi_i(\pmb{\theta})^T = \left(\frac{\partial}{\partial \pmb{\theta}^Y}l_Y(Y_i, \pmb{Z}_i; \pmb{\theta}^Y),~\frac{\partial}{\partial \pmb{\theta}^X} l_X(X_i, \pmb{Z}_i;\pmb{\theta}^X), \psi(Y_i, X_i, \pmb{Z}_i;\pmb{\theta})\right)
\end{aligned}
$$
Where $l_X$ and $l_Y$ are the log-likelihood functions of the multinomial models that are used to model $P(Y|\pmb{Z})$ and $P(X|\pmb{Z})$, with parameters $\pmb{\theta}^Y$ and $\pmb{\theta}^X$ respectively. They are score functions and thus $E\left[ \frac{\partial}{\partial\pmb{\theta}}l_Y(Y_i, \pmb{Z}_i; \pmb{\theta}^Y) \right]=\pmb{0}$ and $E\left[ \frac{\partial}{\partial\pmb{\theta}}l_X(Y_i, \pmb{Z}_i; \pmb{\theta}^X) \right]=\pmb{0}$. The functions $\psi(Y_i, X_i, \pmb{Z}_i;\pmb{\theta})$ will be different for each statistic.\\
~\\
\textbf{For statistic $\pmb{T}_1$}: for $T_1=\Gamma(\hat{P}) - \Gamma(\hat{P}_0)$, we define $\pmb{\theta}^T = (\pi_{11},...,\pi_{1,t-1},\pi_{1,t}, ~~~\pi_{21},...,\pi_{2,t-1},\pi_{2t}, ~~~..., ~~~\pi_{s1},...,\pi_{s,t-1})$. Note that $\pi_{s,t}$ is not included because it can be expressed from the rest of $\pi_{s,t} = 1-\sum_{j,j}\pi_{ij}$.\\
The corresponding function $\psi$ is:
$$
\begin{aligned}
	\psi^T(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}^T) = (I_{\{Y_i=1,X_i=1\}}-\pi_{11}, ~~...,~~I_{\{Y_i=s,X_i=t-1\}}-\pi_{s,t-1})
\end{aligned}
$$
where $I_a$ is an indicator functon of event $a$. $E[\psi(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}^T)]=\pmb{0}$ because for each element of vector $\psi(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}^T)$, say $\psi_{11}$ we have: $E[\psi_{11}] = (1-\pi_{11})\cdot \pi_{11} + (0-\pi_{11})\cdot (1-\pi_{11})=0$. Let $g(\pmb{\theta}) = \Gamma(P) - \Gamma(P_0)$. Then $g(\pmb{\theta})=0$ under the null, and $T_1=g(\hat{\pmb{\theta}})$.\\
~\\
\textbf{For statistic $\pmb{T}_2$}: Let's denote $w_1=E(Y_{i,res})$, $w_2=E(X_{i,res})$, $w_3=E(Y_{i,res}, X_{i,res})$, $w_4=E(Y^2_{i,res})$, and $w_5=E(X^2_{i,res})$. Then $\pmb{\theta}^T = (w_1, w_2, w_3, w_4, w_5)$, and the corresponding $\psi$ is:
$$
\begin{aligned}
	\psi^T(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}) = (Y_{i,res}-w_1, ~X_{i,res}-w_2,  ~Y_{i,res}X_{i,res}-w_3, ~Y^2_{i,res}-w_4, ~X^2_{i,res}-w_5)
\end{aligned}
$$
By definition $E[\psi(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}^T)]=\pmb{0}$. Solving the equation $\sum_i\psi(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}^T)]=\pmb{0}$, we have $\hat{w}_1=\frac{1}{n}\sum_i y_{i,res}$, $\hat{w}_2=\frac{1}{n}\sum_i x_{i,res}$, $\hat{w}_3=\frac{1}{n}\sum_i y_{i,res}x_{i,res}$, $\hat{w}_4=\frac{1}{n}\sum_i y^2_{i,res}$, and $\hat{w}_5=\frac{1}{n}\sum_i x^2_{i,res}$. Let $g(\pmb{\theta}) = (w_3 - w_1w_2)/\sqrt{(w_4-w_1^2)(w_5-w_2^2)}=cor(Y_{i,res},X_{i,res})$ (?!). Because it is a correlation, $g(\pmb{\theta})=0$ under the null, and $T_2=g(\hat{\pmb{\theta}})$\\
~\\
\textbf{For statistic $\pmb{T}_3$}: $T_3 = \frac{1}{n}\sum_i (\hat{C}_i - \hat{D}_i)$, parameter $\pmb{\theta}^T$ is a scalar, $\theta^T = E(C_i - D_i)$, which is $0$ under the null. The corresponding function $\psi$ is:
$$
\begin{aligned}
	\psi^T(Y_i,X_i,\pmb{Z}_i;\pmb{\theta}) = (C_i - D_i) - \theta^T
\end{aligned}
$$
By definition $E[\psi(Y_i,X_i,\pmb{Z}_i;\theta)]=\pmb{0}$. Let $g(\pmb{\theta}) = \theta^T$. Then $g(\pmb{\theta}) = 0$ under the null, and $T_3=g(\hat{\pmb{\theta}}) $. The delta method is not needed here because $\hat{\sigma}^2$ is simply the last element of the diagonal of $\hat{V}(\pmb{\theta})$ (because $g(\cdot)$ is an identity function).


\section{A new residual for ordinal outcomes, Li and Shepherd, 2012 \cite{li2012new}}
\textbf{Desirable properties of residuals for ordial variables:}
\begin{enumerate}[(a)]
	\item results in only one value per subject;
	\item reflects the overall direction of the observed value compared with the fitted value;
	\item is monotonic with respect to the observed value for those with the same covariates;
	\item has a range of possible values that is symmetric about zero;
	\item has expectation zero;
	\item should perserve order without assigning arbitrary scores to the categories.
\end{enumerate}
~\\

\textbf{Definition:}\\
Consider a set of of $s$ ordered categories, $S=\{1,...,s\}$, with order $1<...<s$. For a category $y$ in $S$ and a distribution $F$ over $S$, we define a \textbf{\textit{residual}}:
$$
\begin{aligned}
	r(y,F)=E\{sign(y,Y\} = pr(y>Y) - pr(y<Y),
\end{aligned}
$$
where $Y$ is a random variable with distribution $F$, and $sign(a,b)$ is $-1$, $0$ and $1$ for $a<b$, $a=b$, and $a>b$, respectively. Suppose we fit an ordinal outcome variable $Y$ on covariates $Z$. For subject $i$, let $Y_i$ be the outcome and let $F_{Z_i;\theta}$ be the distribution of $Y_i$ given covariates $Z_i$ under a model with parameter $\theta$. We define:
$$
\begin{aligned}
	R_i = r(Y_i,F_{Z_i;\theta})
\end{aligned}
$$
Give data$(y_i, z_i)$ and a fitted model with parameter estimates $\hat{\theta}$, the residual for subject $i$ is $\hat{r}_i = r(Y_i,F_{Z_i;\theta})$. Notice that $\hat{r}_i$ is not a realization of $R_i$, but of the random variable $\hat{R}_i = r(Y_i,F_{Z_i;\hat{\theta}})$. If $\hat{\theta} \overset{P}{\rightarrow} \theta$, then $F_{Z_i;\hat{\theta}} \rightarrow F_{Z_i;\theta}$, and $\hat{R}_i \overset{D}{\rightarrow} R_i$. Therefore, moment properties of $R_i$ are applicable to $\hat{R}_i$ asymptotically.\\
~\\
\textbf{Properties of $r(y, F)$, $R_i$, and $\hat{r}_i$:}
\begin{enumerate}[\textit{Property} 1.]
	\item $-1\leq r(1, F) \leq ... \leq r(s,F) \leq 1$;
	\item when $s=1$, $r(1,F) = r(1,1)=0$, where $F=(1)$ is a point mass;
	\item $r(j, F) = -r(s-j+1, G)$, where $G=(q_1,...q_s) = (p_s,...,p_1)$ with $q_j=p_{s-j+1}$;
	\item as functions of $(\gamma_1, ..., \gamma_{s-1})$, $\partial r(j,F)/\partial \gamma_j = \partial r(j+1, F)/\partial \gamma_j$.
	\item \textit{Branching Property}. If two adjacent categories $t$ and $t+1$ are merged, with distribution $G=(q_1,...q_{s-1})$, where $q_j=p_j$ for $j<t$, $q_t=p_t + p_{t+1}$ and $q_j=p_{t+1}$ for $j>1$, then $r(j, G)=$
	$$
	\begin{aligned}
		r(j, G)&= r(j, F), ~~~j<t\\
		r(j, G)&= r(j+1, F), ~~~j>t\\
		r(j, G)&= \{ p_t r(t, F) + p_{t+1}(t+1, F) \}/(p_t + p_{t+1}), ~~~j=t\\
		\end{aligned}
		$$
	\item The function $r(j,F)$ satisfies Properties 2, 3(for $s=2$), 4(for $s=2$), and 5 if and only if $r(j,F) = c(\gamma_{j-1} - (1-\gamma_j))$, where $c$ is an arbitrary constant.
	\item $r(j, F)/2 = ridit_j - 1/2$, where $ridit_j = \gamma_{j-1} + p_j/2 = (\gamma_{j-1} + \gamma_j)/2$ and the mean ridit is $1/2$.
	\item $E(R) = 0$;
	\item $var(R) = (1-\sum^s_{j=1} p_j^3)/3$, or alternatively, $var(R) = \sum^s_{j=1} p_j\gamma_{j-1}\gamma_j$. This property provides alternative ways of calculating $var(R)$ and implies that $var(R)$ does not depend on the order of the probabilities.
	\item when $p_1=...=p_s = 1/s$ $var(R)$ reaches its maximum $(1-1/s^2)/3$
	\item Consider a random sample of $n$ subjects, with $n_j$ subjects in category $j$ $(j=1,...,s)$. Their empirical distribution is $\hat{F}=(\hat{p}_1, ... \hat{p}_s)$, where $\hat{p}_j = n_j/n$, with cumulative probabilities $\hat{\gamma}_j = \sum_{k\leq j} n_k/n$. With no covariates, the predictor of all subjects is a constant. Then $r_j = r(j,\hat{F}) = \hat{\gamma}_{j-1} + \hat{\gamma_j} - 1 = \left( \sum_{k<j}2n_k + n_j - n \right)/n$. If we rank these subjects, their midrank is $rank_j = \sum{k<j} n_k + (n_j + 1)/2$. The following property holds:
	$$
	\begin{aligned}
		rank_j = T(r_j) = (n/2)\cdot r_j + (n+1)/2
	\end{aligned}
	$$
The function $T(r_j)$ can be viewed as a translation from the residual scale to the rank scale. This property implies that when there are no covariates, statistics $T_2$ (see reference \cite{li2010test}) is Spearman's rank correlation coefficient between $X$ and $Y$. When covariates exist, $T(r_j)$ yields adjusted ranks of the subjects, and $T_2$ can be interpreted as an adjusted rank correlation.
	\item We now focus on models for an ordinal outcome $Y$ on covariates $Z$ with parameters $\theta$. For subject $i$ $(i=1,...,n)$ and category $j$ $(j=1,...,s)$, let $\gamma_{i,j} = pr(Y_i\leq j|Z_i;\theta)$; for convenience, we define $\gamma_{i,0} = 0$. Let $p_{i,j} = pr(Y_i=j|Z_i)=\gamma_{i,j} - \gamma_{i, j-1}$. The following two moment properties hold:
	$$
	\begin{aligned}
		E{R_i|Z_i;\theta} = 0;
	\end{aligned}
	$$
	\item $var(R_i|Z_i;\theta) = E(R_i^2|Z_i;\theta) =\sum_{j=1}^s R_{i,j}^2 p_{i,j}= \sum_{j=1}^s (\gamma_{i,j-1} + \gamma_{i,j} - 1)^2 p_{i,j}$
	
	\item Let $\hat{p}_{i,j}$ and $\hat{\gamma}_{i,j}$ be the maximum likelihood estimates of $p_{i,j}$ and $\hat{\gamma}_{i,j}$. Then $\hat{r}_i = \hat{\gamma}_{i, y_i-1} + \hat{\gamma}_{i, y_i}$ - 1. The variance of $R_i$ can be consistently estimated by inserting these estimates into Property 13, $\hat{var}(R_i) = var(R_i|z_i;\hat{\theta}) =  \sum_{j=1}^s (\hat{\gamma}_{i,j-1} + \hat{\gamma}_{i,j} - 1)^2 \hat{p}_{i,j}$. One could therefore calculate a standardized residual as $\hat{r}_i/\{\hat{var}(R_i)\}^{1/2}$, which for binary $Y$ is the Pearson residual $(I_{\{Y_i=2\}} - \hat{p}_{i,2})(\hat{p}_{i,1}\hat{p}{i,2})^{-1/2}$.
	
	\item Now we consider a proportional odds model (??? McCullagh, 1980), $logit\{pr(Y \leq j|Z)\}=\alpha_j + Z^T\beta~(j=1,...,s-1)$ with parameters $\theta = (\alpha_1, ...\alpha_{s-1}, \beta)$. Under this model, our residuals are related to score residuals (??? Therneau el al., 1990, ). Let $l_i$ be the loglikelihood for subject $i$, and $U_i=U_i(\theta) = \partial l_i/\partial \theta$ be the score function. Because $E_{\theta}(U_i) = 0$, $U_i$ is called the score residual. For proportional odds models, the following two properties hold:
	$$
	\begin{aligned}
		\hat{r}_i = - \sum_{j=1}^{s-1}(\partial l_i/\partial \alpha_j)|_{\hat{\theta}}, ~~~where ~\hat{\theta}~is~MLE
	\end{aligned}
	$$
	\item $\sum_{i=1}^n \hat{r}_i = 0$.
\textbf{Also}, under certain assumptions (see the paper), these residuals are equivalent to comparing $(\gamma_{i,j-1} + \gamma_{i,j})/2$ with $1/2$ on the probability scale.
	
\end{enumerate}
~\\

\section{PSR (probability scale residuals) for censored data}
% Terms for residuals:
% \begin{itemize}
% 	\item \textbf{OMER}-observed minus expected residuals: $E(y-Y^*) = y - \hat{y}$
% 	\item \textbf{PSR}-probability scale residuals: $E{sign(y,Y^*)} = pr(Y^* < y) - pr(Y^* > y)$
% \end{itemize}
PSR are defined as $r(t,F^*) = E\{sign(y,Y^*)\} = pr(Y^* < y) - pr(Y^* > y) = F^*(t-) - (1-F^*(y)) = F^*(t-) - 1 + F^*(t)$.\\
Since we do not always observe $t$, the PSR must be defined in terms of $y$ (time to event or censoring) and $\delta$ (if the event happened or not). If $\delta = 1$, then $t=y$ and $r(y,F^*, \delta=1) = F^*(y-) - 1 + F^*(y)$. If $\delta = 0$, then $t$ is unknown, except that it occurs some time after the censoring time $y$, so $r(t,F^*, \delta=0) = E\{r(t,F^*)|T^*>y\} = F^*(y)$.
Let's prove that $r(t,F^*, \delta=0) = F^*(y)$. Since $r(t,F^*, \delta=0) = E\{r(t,F^*)|T^*>y\} = \frac{\int_{t>y}r(t, F^*)dF^*(t)}{1-F^*(y)}$, it suffices to show that $F^*(y)\{1- F^*(y)\} = \int_{t>y} r(t, F^*) dF^*(t) = \int_{t>y}F^*(t-)dF^*(t) - \int_{t>y}\{1 - F^*(t)\}dF^*(t)$.
In short, for sensored data:
	$$
	\begin{aligned}
		r(u, F^*) &= F^*(y) + F^*(y-) - 1,~~~&\delta = 1 \\
		r(u, F^*) &= F^*(y) ,~~~&\delta = 0 \\
	\end{aligned}
	$$


\section{Summary of: An Adjustment to Improve the Bivariate Survivor Function Repaired NPMLE. Moodie, Prentice, 2005 \cite{moodie2005adjustment}}
\subsection{Clayton, 1978 \cite{clayton1978model}}
Clayton suggested a \textit{a bivariate model for ordered pairs}. Suppose we are interested in onset of disease for fathers and sons. We want to know if they are associated. If $f(s,t)$ is a density of the age at which fathers ($t$) and sons ($s$) succomb to the disease, then the proposed model is:
	$$
	\begin{aligned}
		&f(s,t)\int_s^{\infty}\int_t^{\infty} f(u, v)dvdu = \theta \int_s^{\infty}f(u, t)du\int_t^{\infty}f(s, v)dv \\
		&or\\
		& \frac{ \lambda(s_0|t=t_0)}{\lambda(s_0|t>t_0)} = \frac{ \lambda(t_0|s=s_0)}{\lambda(t_0|s>s_0)} = \theta
	\end{aligned}
	$$
The we have:
\begin{enumerate}[(i)]
	\item the joint survivor function:\\$\mathcal{F}(s, t) = \int_s^{\infty}\int_t^{\infty} f(u, v)dvdu$
	\item the hazard function for sons of fathers who survive until t, that is:\\
	$g(s;t) = \frac{\partial}{\partial s}\{log\mathcal{F}(s,t)\} = \frac{ \int_t^{\infty} f(s, v)dv} {\mathcal{F}(s, t)}$
	\item the hazard function for fathers of sons who survive until s, that is:\\
	$h(t;s) = \frac{\partial}{\partial t}\{log\mathcal{F}(s,t)\} = \frac{ \int_s^{\infty} f(u, t)du} {\mathcal{F}(s, t)}$
	\item the bivariate failure rate:\\
	$l(t;s) = \frac{f(s,t)}{\mathcal{F}(s,t)} = g(s;t) h(t;s) - \frac{ \partial^2 }{\partial s \partial t} \{-log\mathcal{F}(s,t)\}$
\end{enumerate}
After some complicated math, he derives:
	$$
	\begin{aligned}
		&\mathcal{F}(s,t) = [1 + (\theta - 1)\{a(s) + b(t)\}]^{-1/(\theta - 1)}\\
		&g(s;t) = \frac{a'(s)}{1 + (\theta - 1)\{a(s) + b(t)\}}\\
		&h(t;s) = \frac{b'(s)}{1 + (\theta - 1)\{a(s) + b(t)\}}\\
		&l(s,t) = \frac{\theta a'(s)b'(t)}{1+(\theta-1)(a(s) + b(t))^{2+1/(\theta-1)}}\\
		&f(s,t) = \frac{\theta a'(s)b'(t)}{1+(\theta-1)(a(s) + b(t))^{2}}
	\end{aligned}
	$$
Where $a(\cdot)$ and $b(\cdot)$ are nondecreasing (nuisance) functions with $a(0) = b(0)=0$.\\
\textbf{Interpretation:} when $\theta>1$, $g(s;t)$ is decreasing with respect to $t$ so that the age-specific rates for sons decrease with increased survival time of their fathers. Similarly, $h(t;s)$ decreases with respect to $s$. When $\theta=1$ and there is no asociation.\\
Parameter $\theta$ is estimated using maximum likelihood.

\subsection{Dabrowska, 1988 \cite{dabrowska1988kaplan}}
Dabrowska introduced defined a bivarate Kaplan-Meier estimator and proved its consistency (and probably something else). His notations were used in the following papers:\\
Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $F(s,t) = P(T_1>s, T_>t)$ be the corresponding joint survival function. By a bivariate cumulative hazard function, Dabrowska means a vector:
	$$
	\begin{aligned}
		\Lambda(s,t) &= (\Lambda_{10}(s,t), \Lambda_{01}(s,t), \Lambda_{11}(s,t))\\
		&where\\
		\Lambda_{11}(ds,dt) &= \frac{P(T_1 \in ds, T_2\in dt)}{P(T_1 \geq s, T_2 \geq t)} = \frac{F(ds, dt)}{F(s-, t-)}\\
		\Lambda_{10}(ds,t) &= \frac{P(T_1 \in ds, T_2 > dt)}{P(T_1 \geq ds, T_2 > t)} = \frac{-F(ds, t)}{F(s-, t)}~~~see ~the~paper~where ~it~is~F(s-, t-),~probably~a~typo\\ 
		\Lambda_{01}(s,dt) &= \frac{P(T_1 > s, T_2\in dt)}{P(T_1 > s, T_2 \geq t)} = \frac{-F(s, dt)}{F(s, t-)}\\
		&and\\
		\Lambda_{10}(0,t) &= \Lambda_{01}(s,0) = \Lambda_{11}(0,0) = 0\\
	\end{aligned}
	$$
If $F$ has a density $f(s,t)$, we have $\Lambda_{11}(ds,dt) = \lambda_{11}(s,t)ds~dt$, $\Lambda_{10}(ds,t) = \lambda_{10}(s,t)dt$, $\Lambda_{01}(s,dt) = \lambda_{01}(s,t)dt$, so

	$$
	\begin{aligned}
		\lambda_{11}(s,t) &= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[s,s+h_1], T_2\in[t,t+h_2] | T_1\geq s, T_2 \geq t) & = \frac{f(s,t)}{F(s-, t-)}\\
		\lambda_{10}(s,t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_1\in[s,s+h] | T_1\geq s, T_2 > t) & = \int_t^{\infty} \frac{f(s,v)dv}{F(s-, t)}\\
		\lambda_{01}(s,t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_2\in[t,t+h] | T_1 > s, T_2 \geq t) & = \int_s^{\infty} \frac{f(u,t)du}{F(s, t-)}\\
	\end{aligned}
	$$
Where $\lambda_{11}(s,t)$ is the instantaneous rate of \emph{double failure} at point $(s,t)$, given that the individuals were alive at times $T_1=s-$ and $T_2 = t-$. $\lambda_10(s, t)$ is the rate of a \emph{single failure} at time s give that the first individual was alive at time $T_1=s$ and the second survived beyond time $T_2 = 1$.

\subsection{Fan, 1998 \cite{fan2000dependence}}
There is a need in non-parametric measure of association between two survival times.\\
Clayton suggested a \textit{cross ratio} (Clayton, 1978, Oaks, 1989) (see Clayton's $\theta$):
	$$
	\begin{aligned}
		&\tilde{c}(s_1, s_2) = \frac{F(ds_1, ds_2)F(ds_1^-, ds_2^-)} {F(ds_1, ds_2^-)F(ds_1^-, ds_2)} \\
		 &or\\
		 & \tilde{c}(s_1, s_2) = \frac{ \lambda_1(s_1|T_2=s_2)}{\lambda_1(s_1|T_2 \geq s_2)} = \frac{ \lambda_2(s_2|T_1=s_1)}{\lambda_2(s_2|T_1 \geq s_1)}
	\end{aligned}
	$$
Which can be interpreted as ''the ratio of the breast cancer risk for daughters of age $s_2$ whose mothers developed breast cancer at age $s_1$, compared to daughters of age $s_2$ whose mothers were breast cancer free to age $s_1$. This type of hazard ratio, or relative risk, is most familiar to epidemiologists, and an average relative risk over a range of ages may be readily interpreted and meaningful in epidemiologic contexts''.\\
Kendall's (1938) coefficient of concordance could be also used for association if we there were no censored observations or restricted rance of ages:
	$$
	\begin{aligned}
		&\tau = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22}) \}
	\end{aligned}
	$$
Oakes (1989)\cite{oakes1989bivariate} proposed:
	$$
	\begin{aligned}
		&\tau(s_1, s_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} = s_1,~T_{21}\wedge T_{22} = s_2  \}
	\end{aligned}
	$$
''Following the suggestion of in Hsu and Prentice (1996) one could consider a summary dependence measure that weights the cross ratio $\tilde{c}(s_1, s_2)$ over $[0, t_1]\times[0, t_2]$ by $\frac{F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)}$''. But there are some problems with consistency of $\tilde{c}(s_1, s_2)$ when it is weighted this way.\\
Instead, the authors propose to weight $c(s_1, s_2) = \frac{1}{\tilde{c}(s_1, s_2)}$ and get:
	$$
	\begin{aligned}
		C(t_1, t_2) &= \int_0^{t_1}\int_0^{t_2} \frac{c(s_1, s_2)F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)} 
	\end{aligned}
	$$
	When $C(t_1, t_2)>1$, there is positive association. When $C(t_1, t_2)=1$, there is no association. When $C(t_1, t_2)<1$, there is negative association.  \\
	In addition to $C(t_1, t_2)$ they proposed another summary measure of dependence.
	Oakes (1989)\cite{oakes1989bivariate} also noted that $\tau(s_1, s_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} = s_1,~T_{21}\wedge T_{22} = s_2  \}$ can be written as:
	$$
	\begin{aligned}
		\tau(s_1, s_2) &= \frac{1 - c(s_1, s_2)}{1+c(s_1, s_2)}
	\end{aligned}
	$$
The authors propose to weight this $\tau(s_1, s_2)$ proportionally to the density. After some algebra, it can be shown that the weighted $\tau$ (denoted as $\mathcal{T}(t_1, t_2)$) can be written as:
	$$
	\begin{aligned}
		&\mathcal{T}(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} \leq t_1,~T_{21}\wedge T_{22} \leq t_2  \}
	\end{aligned}
	$$
	so that $\mathcal{T}(t_1, t_2) \in [-1,1]$

\subsection{Prentice, 1992 \cite{prentice1992covariance}} 
If I understand correctly, the authors model bivariate survival as two marginal survivor functions and a covariance function (instead of joint survivor function). 

\subsection{Fan, 2000 \cite{fan2000class}} 
The authors suggest a class of estimators similar to what was described in \emph{Fan, 1998} \cite{fan2000dependence}, which were weighted averages of local measures. The motivation for new estimators was that ''for certain choices of weights, the estimation of the bivariate survivor function (e.g. Dabrowska (1998)\cite{dabrowska1988kaplan} and Prentice and Cai (1992) \cite{prentice1992covariance}), can be avoided and explicit variance formulae can be obtained.''\\
The authors utilize the fact that both $C(\cdot, \cdot)$ and $\mathcal{T}(\cdot, \cdot)$ depend on $F(\cdot, \cdot)$ only through the hazard function and therefore can be estimated using Nelson-Aalan-type estimator of the hazard:

	$$
	\begin{aligned}
    C(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2)}{\int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2)}\\
    \mathcal{T}(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) - \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) + \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }\\
	\end{aligned}
	$$
	
	
	
	
	
	
\printbibliography

\end{document}          

