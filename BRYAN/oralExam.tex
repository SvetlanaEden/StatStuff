\documentclass[]{article}

%\usepackage[final]{pdfpages}
\usepackage{anysize}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{longtable}
\usepackage{nameref}

%\usepackage[usenames, dvipsnames]{color}
\usepackage{xcolor,colortbl}

\DeclareGraphicsExtensions{.png,.pdf}

% %%% for citations. See Lucy's tutorial: https://github.com/LucyMcGowan/Tutorials/blob/master/BiblatexTutorial.md
% % \usepackage[backend=biber, maxnames=10, citestyle=science]{biblatex}
% \usepackage[style=authoryear, backend= biber]{biblatex}
% \addbibresource{summaryOfRef.bib}

% %%% for citations. Jonathan's way:
\usepackage{natbib}
\bibliographystyle{unsrtnat}



%%% when using biber as a "backend" compile like this:
% pdflatex myFile
% biber myFile
% pdflatex myFile


\let\epsilon\varepsilon
\newcommand\SLASH{\char`\\}
\newcommand{\csch}{\text{csch}}
\marginsize{0.5in}{1in}{0.5in}{1in}
\setlength\parindent{0pt}

% Title Page: Modify as needed
\title{Association of bivariate survival data}
\author{Svetlana Eden}
\date{Month Day, 2015}

\hypersetup{hidelinks}

% \usepackage[pdftex,bookmarks,pagebackref,pdfpagemode=UseOutlines,
%      colorlinks,linkcolor=\linkcol,
%      pdfauthor={Svetlana K Eden},
%      pdftitle={\titl}]{hyperref}


\begin{document}
\maketitle
\tableofcontents
\listoffigures
\listoftables
\clearpage

\section{INTRODUCTION}
The interest in survival data can be traced back to at least 1846, when a Hungarian doctor of the Vienna General Hospital maternity clinic, Ignaz Semmelweis, recorded the rate of death from childbed fever. After excluding several other hypotheses, he suggested that when a child birth is helped by a doctor or a medical student, the women's rate of death from childbed fever was higher because of a disease that was caused by cadaverous particles that medical students and doctors had on their hands after dissecting cadavers. As you might know, his theory was dismissed, and he died alienated from the medical community. The rational for his hypothesis was discovered decades later, when the germ theory of the disease was developed. 


Another example was Florence Nightingale, who was born into a rich upper-class British family, and in spite of her status she pursued a career of a nurse. She was credited for significantly reducing hospital mortality of British soldiers during the Crimean War, in 1854 (with hand-washing being one of the most important practices implemented under her supervision). She also published some of her works in simple English in order to popularize medical knowledge.\\

In these two examples, survival data was collected as a binary variable: died or lived. For studies with longer follow-up, a better way of collecting survival data is to record time to the event. The problem with collecting time to event though, is that not all events can be observed. For example, suppose a five-year longitudinal study aims to test an association of blood pressure and time to cardiovascular event. Not all patients will necessarily have the event during these five years. Such patients would be labeled as \emph{censored}.  \cite{fisher1931truncated},  \cite{hald1949maximum}, and \cite{swan1969computing} were probably one of the first to suggest a more complex view of survival data by introducing a concept of \emph{censoring} and suggesting methods of dealing with it. Censoring is \emph{non-informative} if it is not associated with the potential event time. Otherwise it is called \emph{informative}. Assumption of non-informative censoring somewhat simplifies estimation of survival probability, but still requires complex theoretical tools. It was \cite{kaplan1958nonparametric} who suggested an estimate of survival probability for a univariate case and in the presence of censoring. They introduced a well known Product-Limit estimator of the survival curve. \cite{david1972regression} extended their work by introducing a regression model for censored survival outcome. His model utilized a concept of a \emph{hazard rate}, and we define it here in order to understand better the rest of the paper. \emph{Hazard rate} is defined as follows:

$$
\begin{aligned}
	\lambda(t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T\in[t,t+h] | T\geq t)=
	%&=  \frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t)  \right\}= \\&=
	             \frac{f(t)}{\mathcal{S}(t)}
\end{aligned}
$$

Where $T$ is a random variable of time to event; $F(t)$, $f(t)$, $S(t)$ are cumulative distribution function, density function, and \emph{survival function} of random variable $T$; and $\mathcal{S}(t) = 1-F(t)$. $P(T\in[t,t+h] | T\geq t)$ is a \emph{conditional probability}, the probability that the event occurs in interval $[t,t+h]$ given that the subject survived without the event at least until time $t$. In other words, hazard rate can be intuitively thought of as a probability of the event per unit of time.\\
Although the notion of hazard rate is intuitively simple, its estimation required complex theoretical tools even in a univariate case. Adding another dimension not only increases the complexity, but also makes it more difficult to apply intuitive thinking. Pursuing a purely theoretical question in the context of \emph{bivariate} survival function, \cite{basu1971bivariate} %[get this paper from the library ??? this does not add up because Cox wrote his paper later]
defined a \emph{double failure hazard rate} as:
$$
\begin{aligned}
	\lambda(t_1,t_2)&= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[t_1,t_1+h_1], T_2\in[t_2,t_2+h_2] | T_1\geq t_1, T_2 \geq t)=
	%&= \frac{\partial}{\partial t_1}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t_1,t_2)  \right\} - \frac{\partial^2}{\partial t_1\partial t_2}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\\
	 \frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}
\end{aligned}
$$
The intuition of the univariate hazard rate helps us understand the above expression because of the similarity between $\frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}$ and$\frac{f(t)}{\mathcal{S}(t)}$. We included this definition because this and other similar concepts are present in the rest of this paper.\\

Although the work of \cite{basu1971bivariate} was not targeted toward specific applications, there are many practical problems where bivariate survival analysis could find its use, for example, in studies of age of appendicitis in twins, or of time to cardiovascular disease among subjects with familial ties. In this paper, we are concerned with the question of association between paired survival data. Although the literature on this subject is vast, we review only three main approaches. First, this question was addressed by introducing a \emph{bivariate hazard ratio} in the context of specific family of distributions. The second approach was to find a non-parametric estimate of the bivariate survival function with three components: two marginal survival functions and a term that represented an association between two survival variables. The third approach was to develop semi-parametric and non-parametric tests to examine association of residuals. We especially focus on the non-parametric test of \emph{probability scale residuals} introduced and thorouphly researched by \cite{li2012new}, \cite{lui2015covariate}, and \cite{shepherd2016probability}.\\
The main goal of the paper was to show the complexity of theoretical tools required to address the question of bivariate survival in the presence of censoring and to highlight the generalizability of \emph{probability scale residuals} approach and simplicity of its use. In conclusion, we identify the direction of our future research with a goal, to apply \emph{probability scale residuals} for a wide range of different statistical scenarios in the context of bivariate survival with non-informative censoring.

\section{LITERATURE REVIEW}

\subsection{Modeling a Bivariate Hazard Ratio}

Suppose we study an association of time to cardiovascular disease, $T$ and parental history of cardiovascular disease. One way to approach this problem would be to use Cox proportional hazard regression with parental history of cardiovascular disease as the main covariate, $x$, recorded as \emph{1-yes} and \emph{0-No}:

$$
\begin{aligned}
	\lambda(t|x) &= \lambda_0(t) \cdot e^{\beta\cdot x}
\end{aligned}
$$

Note that for this model, the \emph{hazard ratio} or having a cardiovascular event given parental history is simply:

$$
\begin{aligned}
 \theta = \frac{\lambda(t|x=1)}{\lambda(t|x=0)} = e^{\beta}
 \end{aligned}
$$
 
The problem with this approach is that a parent may not live long enough to experience time to cardiovascular disease. This means that not only our outcome, time to cardiovascular disease, but also our covariate, parental history may be censored. This is what motivated \cite{clayton1978model} to think of a different approach for epidemiological studies of chronic disease with familial tendency. Following the intuition of the hazard ratio in the context of Cox regression, he defined a measure similar to hazard ratio, that had the following properties:
\begin{enumerate}
	\item Easy to compute
	\item Expressible as a constant ratio of age-specific rates
  \item Symmetrical in two variables
\end{enumerate}
Let's denote time to event for fathers to be $t$ and time to event for sons to be $s$. Also, let's denote $\lambda_s(s_0|t=t_0) = \frac{f(s_0|t=t_0)}{S(s_0|t=t_0)}$ and $\lambda_s(s_0|t>t_0) = \frac{f(s_0|t>t_0)}{S(s_0|t>t_0)}$. Then Clayton's \emph{cross ratio}, $\theta$, is defined in the following way:
$$
\begin{aligned}
	\theta = \frac{\lambda_s(s_0|t=t_0)}{\lambda_s(s_0|t>t_0)} = \frac{\lambda_t(t_0|s=s_0)}{\lambda_t(t_0|s>s_0)}
\end{aligned}
$$
We clearly see a similarity between the hazard ratio of Cox model and the \emph{cross ratio} defined by Clayton. The requirement of symmetry was motivated by the fact than neither son nor father were causes of the event, rather, there was an unobservable variable that was associated with the event for both of them.\\

This measure can be interpreted for sons (as much as for fathers) in the following way: how much more likely for a son to have an event at time $s_0$ given that his father had an event at time $t_0$ compared to a son whose father survived until time $t_0$ without the event. Independence between fathers and sons would mean $\theta = 1$, positive association $\theta > 1$ and negative association $\theta < 1$.
 %Clayton also explained how his model could be generalized to include covariates, and derived the likelihood to estimate $\theta$ when two marginal distributions are completely unknown using approach similar to one used by Cox.\\
Clayton's assumptions lead to a specific form of survival function (see \cite{oakes1989bivariate}):
$$
\begin{aligned}
S(t_1,t_2) = \left[\left\{ \frac{1}{S_1(t_1)} \right\}^{\theta - 1} + \left\{ \frac{1}{S_2(t_2)} \right\}^{\theta - 1}   - 1 \right]^{-\frac{1}{\theta-1}}
\end{aligned}
$$
where $S_1(t_1)$ and $S_2(t_2)$ are marginal survival functions. Although this bivariate function has a very specific form, Clayton's non-parametric likelihood approach allowed to estimate $\theta$ without knowing specific forms of $S_1(t_1)$ and $S_2(t_2)$.\\
~\\
\cite{oakes1982model} studied Clayton's model extensively and showed that: $\frac{\theta - 1}{\theta + 1} = \tau$, where $\tau$ is a coefficient of concordance introduced by  \cite{kendall1938new}: for two independent pairs of variables with the same bivariate distribution $(U_1, Z_1)$ and $(U_2, Z_2)$, Kendal's $\tau$ is the probability of concordance minus the probability of discordance:
$\tau = P[(U_1 - U_2)(Z_1 - Z_2)>0] - P[(U_1 - U_2)(Z_1 - Z_2)<0] = E[sign((U_1 - U_2)(Z_1 - Z_2))]$. This fact suggested that $\theta$ can be estimated through $\tau$ as well.\\

In order to generalize these results, \cite{oakes1989bivariate} introduced a bivariate survival model based on \emph{frailties}, unobservable random variables. If we denote an unobservable random variable as $W$, the probability of survival can be written as: $Pr(T>t|W=w)$. In the bivariate case, we assume that both survival times depend on the same unobservable random variable, $W$:
$$
\begin{aligned}
	Pr(T_1>t_1|W=w)=\left\{ B_1(t_1) \right\}^w\\
	Pr(T_2>t_2|W=w)=\left\{ B_2(t_2) \right\}^w\\
\end{aligned}
$$
This induced dependence between the variables $T_1$ and $T_2$:
$$
\begin{aligned}
	S(t_1,t_2) = \int \left\{ B_1(t_1) B_2(t_2)\right\}^w dF(w)
\end{aligned}
$$

He showed that bivariate distributions introduced by frailty models were part of a larger class, the class of \emph{archimedian} distributions studied by \cite{genest1986copules}:
$$
\begin{aligned}
	S(t) = p\left[ q\left\{ S_1(t_1) \right\}  + q\left\{ S_2(t_2) \right\}  \right]
\end{aligned}
$$

Where $p(u)$ is a non-negative decreasing function with non-negative second derivative, with $p(0)=1$, and $q(u)$ is an inverse function of $p(v)$. It can be shown (see \nameref{appendixA}) that for \emph{croos ratio} the following equality holds:
	$$
	\begin{aligned}
		\theta(t_1,t_2) = \frac{\lambda_{t_1}(t_1|T_2=t_2)}{\lambda_{t_1}(t_1|T>t_2)} = \frac{ \frac{\partial^2 S(t_1,t_2)}{\partial t_1 \partial t_2} S(t_1, t_2)}    {\frac{\partial S(t_1, t_2)}{\partial t_2} \frac{\partial S(t_1, t_2)}{\partial t_1}}
	\end{aligned}
	$$
Using this equality, Oakes showed that for \emph{archimedian} class of models, $\theta$ depends on $t$ only through $S(t)$, which allowed him to show the following:
$$
\begin{aligned}
\theta(v) = -v\cdot q''(v)/q'(v)
\end{aligned}
$$
where $v=S(t_1, t_2)$. He also pointed out that because $\theta$ and \emph{Kendall}'s $\tau$ were related through the following equality: $\tau(v) = \frac{\theta(v)-1}{\theta(v)+1}$, both $\theta(v)$ and $\tau(v)$ can be considered as measures of local dependence because they depend on $v=S(t_1, t_2)$. By showing that Clayton's model is a subset of the \emph{archimedian} class, Oakes extended the set of models for which $\theta(v)$ and $\tau(v)$ can be used and studied.\\
~\\
As a side note, the \emph{archimedian} class of survival function used by Oakes is a subclass of functions called \emph{copulas}. According to \cite{nelsen2007introduction}, a two-dimensional \emph{copula} is a function with support on $[0, 1] \times [0, 1]$ and values in $[0, 1]$. We omit the formal defintion of a \emph{copula} and only mention its important quality summarized in \emph{Sklar's theorem}: if $H(x,y)$ is a joint distribution function with margins $F(x)$ and $G(y)$, there exists a copula $C$ such that for all $x,y$: $H(x,y) = C(F(x), G(y))$, where, if $F$ and $G$ are continuous then $C$ is unique. Nelson also defines a \emph{survival copula} that holds the same property: $\bar{H}(x,y) = C_S(\bar{F}(x), \bar{G}(y))$, where $\bar{H}(x,y)$, $\bar{F}(x)$, and $\bar{G}(y)$ are joint and marginal survival functions respectively. The implication of this property is that for any type of survival copula, the marginal distributions and their association parameter can be estimated separately. 
%Without going into the details, we mention a few papers that used copula approach, . 
In addition to this important property, the theory of copulas for continuous data allows to express two popular association measures, \emph{Kendal's} $\tau$ and \emph{Spearman} correlation, in terms of copulas. Without going into details, we only mention few papers where authors used the theory of copulas for inference in bivariate survival: \cite{shih1996tests}, \cite{shih1995inferences}, and \cite{zhang2008inference}.\\
~\\
Now we go back to the \emph{cross ratio} $\theta$, defined by Clayton and later developed by Oakes. In survival studies, the age of participants is often restricted by inclusion criteria. For example, the age of mothers could be restricted to 50-80, and of daughters to 30-60. This created a need for an association measure that would take this restriction into account and would be well suited for censored time to event. This argument lead \cite{fan2000dependence} to consider a weighted average of Clayton's \textit{cross ratio} $\theta = \frac{ \lambda_2(t_2|T_1=t_1)}{\lambda_2(t_2|T_1 \geq t_1)}$ over $[0,t_1] \times [0, t_2]$. As a reminder, $\theta$ can be interpreted as a risk for daughters of age $t_2$ whose mothers developed a disease at age $t_1$, compared to daughters of age $t_2$ whose mothers were disease free at age $t_1$. The authors chose this measure because of its interpretability, and suggested that in order to increase comparability across studies, the weighting should be independent of censoring mechanism. The following weights were proposed: $\frac{F(dt_1,dt_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)}$. For one dimension, this weight could be interpreted as a probability of an event at point $t$ given that the subject survived until $t$.\\

%They allowed the time to be discrete, continuous, or mixed, but $F(., .)$ was required to possess a density relative to the corresponding counting Lebesque measure.\\
Instead of weighting $\theta(t_1, t_2)$, the authors chose to weight $c(t_1, t_2) = \frac{1}{\theta(t_1, t_2)}$ because weighting $c(t_1, t_2)$ results in a consistent estimator:
	$$
	\begin{aligned}
		C(t_1, t_2) &= \int_0^{t_1}\int_0^{t_2} \frac{c(s_1, s_2)F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)} = ...= \int_0^{t_1}\int_0^{t_2} \frac{F(s_1^-, s_2^-)\Lambda_{10}(ds_1, s^-_2)\Lambda_{01}(s^-_1, ds_2)}{1 - F(t_1, 0) - F(0, t_2)+ F(t_1, t_2)} \\
	\end{aligned}
	$$
	The above measure can be estimated non-parametrically using $\hat{F}$ of \cite{dabrowska1988kaplan} or \cite{prentice1992covariance}, and $\hat{\Lambda}$ can be estimated using a Nelson-Aalen type estimator.
	%??? Nelson-Aalen
Note that $C(t_1, t_2)$ may be interpreted as an average relative risk over $[0,t_1] \times [0, t_2]$. When $C(t_1, t_2)=1$, there is no association. When $C(t_1, t_2)<1$, there is positive association. It can also be interpreted as a weighted average hazard ratio with weights proportional to the failure time density at $(t_1,t_2)$.\\
Because \cite{oakes1989bivariate} noted that \emph{Kendal's} $\tau(t_1, t_2)$ can be written as:
	$$
	\begin{aligned}
		\tau(t_1, t_2) &= \frac{1 - c(t_1, t_2)}{1+c(t_1, t_2)}
	\end{aligned}
	$$
	The authors also proposed to weight conditional version of \emph{Kendal's} $\tau=\tau(t_1, t_2)$:
	$$
	\begin{aligned}
		&\tau(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} = t_1,~T_{21}\wedge T_{22} = t_2  \}
		&~~~~where~\wedge~is~minimum
	\end{aligned}
	$$
The above measure can be viewed as an association between paired subjects who reached certain age. After weighting $\tau(t_1, t_2)$ proportionally to the density, they obtained a weighted conditional \emph{Kendal's} $\tau(t_1, t_2)$:
	$$
	\begin{aligned}
		&\mathcal{T}(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} \leq t_1,~T_{21}\wedge T_{22} \leq t_2  \}
	\end{aligned}
	$$
This measure can be especially useful in studies with long follow-up. For example, the authors estimated that the association of time to appendectomy between monozygotic female twins measured using $C(t_1, t_2)$ and $\mathcal{T}(t_1, t_2)$ was stronger at younger ages and weaker at older ages.\\

Measures $C(t_1, t_2)$ and $\mathcal{T}(t_1, t_2)$ were further considered by
\cite{fan2000class} who suggested a class of estimators similar to ones described by \cite{fan2000dependence}. They considered special cases of $\theta(t_1, t_2)$ and weights, where instead of using the bivariate survival function estimators of \cite{dabrowska1988kaplan} and \cite{prentice1992covariance} the hazard function estimators of Nelson-Aalen type could be used. And instead of bootstrap variance estimator, an explicit variance formulae can be obtained.\\
% The authors utilize the fact that in some specific cases, both $C(\cdot, \cdot)$ and $\mathcal{T}(\cdot, \cdot)$ depend on $F(\cdot, \cdot)$ only through the hazard function and therefore can be estimated using Nelson-Aalan-type estimator of the hazard:
% 	$$
% 	\begin{aligned}
%     C(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2)}{\int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2)}\\
%     \mathcal{T}(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) - \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) + \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }\\
% 	\end{aligned}
%  $$

This completes our overview of papers that studied association of bivariate survival through modeling of a \emph{cross ratio}. We proceed to a different but perhaps more complex way of evaluating association of bivariate survival data: by estimating a \emph{bivariate survival surface}.

\subsection{Estimating Bivariate Survival Surface}

In univariate survival analysis, Kaplan-Meier curve is a popular visual summary and a reliable analytic tool. For bivariate survival, several attempts were made to find a similar estimator. The main difficulty however according to \cite{kalbfleisch2011statistical} is that in the bivariate case, there is a problem of identifiability of bivariate survival surface. For example, a nonparametric maximum likelihood for a survival function, $S(t_1, t_2) = P(T_1>t_1, T_2>t_2)$ in the presence censoring has a uniqueness problem and also, cumulative hazard function $\Lambda(t_1, t_2) = \int_0^{t_1}\int_0^{t_2}\Lambda(du_1,du_2)$ is not uniquely defined by $\Lambda(du_1,du_2)$.  \cite{dabrowska1988kaplan} found a way around this problem. By introducing marginal cumulative hazards in addition to $\Lambda(du_1,du_2)$, she was able to derive a consistent non-parametric estimator of the bivariate survival surface. Let's introduce the following notations. We let the density of time to event be $f(s,t)$ and let $S(s,t) = P(T_1>s, T_2>t)$ be the corresponding joint survival function. Bivariate cumulative hazard functions are defined by Dabrowska in the following way:
% 	$$
% 	\begin{aligned}
% 		\lambda_{11}(t_1,t_2) &= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[t_1,t_1+h_1], T_2\in[t_2,t_2+h_2] | T_1\geq t_1, T_2 \geq t_2) & = \frac{f(t_1,t_2)}{S(t_1-,t_2-)}\\
% 		\lambda_{10}(t_1,t_2) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_1\in[t_1,t_1+h] | T_1\geq t_1, T_2 >t_2) & = \int_{t_2}^{\infty} \frac{f(t_1,v)dv}{S(t_1-,t_2)}\\
% 		\lambda_{01}(t_1,t_2) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_2\in[t_2,t_2+h] | T_1 > t_1, T_2 \geq t_2) & = \int_{t_1}^{\infty} \frac{f(u,t_2)du}{S(t_1,t_2-)}\\
% 	\end{aligned}
% 	$$
% Where $\lambda_{11}(s,t)$ is the instantaneous rate of \emph{double failure} at point $(s,t)$, given that the individuals were alive at times $T_1=s-$ and $T_2 = t-$; $\lambda_{10}(s, t)$ is the rate of a \emph{single failure} at time $s$ given that the first individual was alive at time $T_1=s$ and the second survived beyond time $T_2 = t_1$.\\
	$$
	\begin{aligned}
		%\Lambda(t_1,t_2) &= (\Lambda_{10}(t_1,t_2), \Lambda_{01}(t_1,t_2), \Lambda_{11}(t_1,t_2))\\
		%&where\\
		\Lambda_{11}(dt_1,dt_2) &= \frac{P(T_1 \in dt_1, T_2\in dt_2)}{P(T_1 \geq t_1, T_2 \geq t_2)} = \frac{S(dt_1, dt_2)}{S(t_1^-, t_2^-)}\\
		\Lambda_{10}(dt_1,t_2) &= \frac{P(T_1 \in dt_1, T_2 > t_2)}{P(T_1 \geq dt_1, T_2 > t_2)} = \frac{-S(dt_1, t_2)}{S(t_1^-, t_2)}\\ %~~~t_1ee ~t_2he~paper~where ~it_2~it_1~S(t_1-, t_2-),~probably~a~t_2ypo\\
		\Lambda_{01}(t_1,dt_2) &= \frac{P(T_1 > t_1, T_2\in dt_2)}{P(T_1 > t_1, T_2 \geq t_2)} = \frac{-S(t_1, dt_2)}{S(t_1, t_2^-)}\\
		% &and\\
		% \Lambda_{10}(0,t_2) &= \Lambda_{01}(t_1,0) = \Lambda_{11}(0,0) = 0\\
	\end{aligned}
	$$
% If , we have $\Lambda_{11}(ds,dt) = \lambda_{11}(s,t)ds~dt$, $\Lambda_{10}(ds,t) = \lambda_{10}(s,t)dt$, $\Lambda_{01}(s,dt) = \lambda_{01}(s,t)dt$, so


Under condition of independence of event times and censoring times, the author derived a bivariate survival function estimator that could be computed as a product of two univariate survival function estimators and a third term:
	$$
	\begin{aligned}
		{S}(t_1,t_2) &= {S}(t_1,0){S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}\\
	\end{aligned}
	$$
Where $S(t_1,0)$ and $S(0,t_2)$ are marginal survival functions and $L$ is the following:
	$$
	\begin{aligned}
		L(du, dv) &= \frac{{\Lambda}_{10}(d u,v^-){\Lambda}_{01}(u^-,d v) - {\Lambda}_{11}(du, dv)}{\left(1-{\Lambda}_{10}(d u,v^-)\right)\left(1-{\Lambda}_{01}(u^-,\Delta v)\right)}
	\end{aligned}
	$$
From the estimator's definition, ${S}(t_1,t_2) = {S}(t_1,0){S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}$ it is clear that term $\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}$ can be written as:
	$$
	\begin{aligned}
		\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\} &=
		\frac{S(0,0)  S(t_1,t_2)}{ S(t_1,0)S(0,t_2) }
	\end{aligned}
	$$
The above expression looks very much like:
	$$
	\begin{aligned}
		\theta &= \frac{ \frac{\partial^2 S(t_1,t_2)}{\partial t_1 \partial t_2} S(t_1, t_2)}    {\frac{\partial S(t_1, t_2)}{\partial t_2} \frac{\partial S(t_1, t_2)}{\partial t_1}},
	\end{aligned}
	$$
which is one of the ways to define the \emph{cross ratio} of \cite{clayton1978model}. This helps us interpret Dabrowska's association term the same way as Clayton's \emph{cross ratio}, but instead of being constant, it varies over $[0, t_1]\times [0, t_2]$.\\

Dabrowska's estimator of the bivariate survival surface is computed in the following way:
	$$
	\begin{aligned}
		\hat{S}(t_1,t_2) &= \hat{S}(t_1,0)\hat{S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
	\end{aligned}
	$$
Where $\hat{S}(t_1,0)$ and $\hat{S}(0,t_2)$ are usual Kaplan-Meier estimates, for example, $\hat{S}(t_1,0) = \prod_{u\leq t_1}[1-\hat{\Lambda}_{10}(\Delta u, 0)]$, and the third term is estimating in the following way:
	$$
	\begin{aligned}
    \hat{L}(\Delta u, \Delta v) &= \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}(\Delta u, \Delta v) = \#(T_1=u, T_2=v, \delta_1=\delta_2=1)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{10}(\Delta u, v^-) = \#(T_1=u, \delta_1=1, T_2\geq v)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{01}(u^-,\Delta  v) = \#(T_2=v, \delta_2=1, T_1\geq u)/\#(T_1\geq u, T_2\geq v)
	\end{aligned}
	$$

This estimator is easy to compute, but as we see later, it assigns negative mass to certain points (see \nameref{appendixB}). The authors provided an example with negative mass assignment and also proved that in the absence of censoring, this problem does not arise.\\

\cite{pruitt1991negative} investigated conditions under which Dabrowska's estimator assigns negative mass and showed that although the value of negative mass decreased, the number of such points did not disappear with growing sample size, resulting in non-disappearing negative mass. This problem was related to the problem of identifiability of the bivariate survival functions in the presence of censored data. The proof was rather complicated, but we can intuitively see why this can be the case. Let's rewrite Dabrowska's estimator:

	$$
	\begin{aligned}
		\hat{S}(s,t) &= \hat{S}(s,0)\hat{S}(0,t)\cdot \prod_{{0<u\leq s~0<v\leq t}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
    1 - \hat{L}(\Delta u, \Delta v) &= 1 - \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}(\Delta u, \Delta v) = \#(T_1=u, T_2=v, \delta_1=\delta_2=1)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{10}(\Delta u, v^-) = \#(T_1=u, \delta_1=1, T_2\geq v)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{01}(u^-,\Delta  v) = \#(T_2=v, \delta_2=1, T_1\geq u)/\#(T_1\geq u, T_2\geq v)
	\end{aligned}
	$$
It is possible that $\hat{S}(s,t\neq 0) > \hat{S}(s,0)$ when expression $1 - \hat{L}(\Delta u, \Delta v)$ is greater than one, which in turn may happen when  $\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) \cdot \hat{\Lambda}_{10}(\Delta t_1, t_2^-)< \hat{\Lambda}_{11}(\Delta t_1, \Delta t_2)$. In the presence of censoring, $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, $\hat{\Lambda}_{11}$ are somewhat \emph{unsynchronized}, in the sense that the number at risk for single failures is the same as the number at risk for double failures in spite of the fact that marginally the number at risk can be larger. Also, because of the censoring, each term $1 - \hat{L}(\Delta u, \Delta v)$ is slightly contributing to the overall larger value that eventually can be bigger than one, which results in the fact that the bivariate survival function is not monotone.\\
Simple simulations showed (not presented here) that term $1 - \hat{L}(\Delta u, \Delta v)$ is greater than $2$ when $\hat{\Lambda}_{01}$ is very large and $\hat{\Lambda}_{10}$ is very small (or the other way around), and when $\hat{\Lambda}_{11}$ is very large. Even for $\hat{\Lambda}_{01} = \hat{\Lambda}_{10} = \hat{\Lambda}_{11} = 0.5$, term $1 - \hat{L}(\Delta u, \Delta v) = 2$.  Dabrowska's example presented in \nameref{appendixB} can also illustrates that when all observations are not censored, the values of $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, and $\hat{\Lambda}_{11}$ balance out and the negative mass disappears.\\

\cite{prentice1992covariance} introduced an estimator similar to Dabrowska's. It had similar performance, and the same negative mass problem. In fact, \cite{van1997nonparametric} sited the work of \cite{gill1990survey} who proved that the estimators of Dabrowska and Prentice and Cai were equivalent and performed similarly well under the assumption of complete independence of events and censoring. Van Der Laan further discussed difficulties of evaluating bivariate survival using non-parametric likelihood (NPMLE). He cited the work of \cite{tsai1986nonparametric}, who give an example showing that NPMLE for continuous bivariate survival case is not consistent. Without formally proving this, Van Der Laan gave an intuitive explanation of why this is true using the mechanics of expectation-maximization (EM) algorithm applied to discrete case. In his explanation, Van Der Laan talks about \emph{mass redistribution}, and in order to understand the explanation let's consider a univariate case, Kaplan-Meier estimator. Suppose we have three events, $A$, $B$, and $C$. According to Kaplan-Meier estimator, each of them has a mass $p_i = \frac{1}{3},~i = \{A,B,C\}$ (see left panel of Figure \ref{fig:km}). Now suppose that the earliest event ($i=1$) is censored. Then the Kaplan-Meier curve looks a little different (see right panel of Figure \ref{fig:km}). We say that the mass of the censored observation, $p_A=\frac{1}{3}$ was redistributed to the uncensored observations in equal amounts: $p_B=p_C=\frac{1}{3}+\frac{p_1}{2} = \frac{1}{2}$. We are able to redistribute the mass because we know that observations $B$ and $C$ happened after observation $A$ was censored.\\

\begin{figure}[!h]
\caption{Example of mass redistribution in univariate case. The left panel shows Kaplan-Meier curve for three uncensored observations, $A$, $B$, and $C$. The right panel shows Kaplan-Meier curve for one censored observation $A$ and two uncensored observations $B$ and $C$. When observation $A$ is censored its mass gets redistributed equally between $B$ and $C$: $p_B=p_C=\frac{1}{3}+\frac{p_1}{2} = \frac{1}{2}$. The light gray curve on the right panel traces the Kaplan-Meier curve for three uncensored observations, $A$, $B$, and $C$ for comparison.}
\includegraphics{Simulations/kaplanMeierExplanation.pdf}
\label{fig:km}
\end{figure}

Now let's consider a bivariate case with three points $A$, $B$, and $C$, such that they all have the same coordinates $T_2$, $A=c(T_{1,A}, T_2)$, $B=c(T_{1,B}, T_2)$, $C=c(T_{1,C}, T_2)$, and point $A$ is censored in coordinate $T_1$ (see left panel of Figure \ref{fig:bc}). We can still redistribute its mass because observations $B$ and $C$ are contained in the \emph{half-line} of observation $A$. But what if we have a different case depicted in the right panel of Figure \ref{fig:bc}? Observations $B$ and $C$ are no longer contained in the \emph{half-line} of observation $A$, and the question arises how the mass of observation $A$ should be redistributed. 

\begin{figure}[!h]
\caption{Illustration of mass redistribution problem in the bivariate case. When the uncensored points are contained in the \emph{half-line} of a censored point (see left panel), the mass redistribution is similar to the bivariate case and is straight forward. When no uncensored points are contained in the \emph{half-line} of a censored point (see right panel), there is a problem what uncensored observations should get the mass and how much mass.}
\includegraphics{Simulations/bivariateCase.pdf}
\label{fig:bc}
\end{figure}

Van Der Laan explained the same problem, but in terms of the EM algorithm that attempts to solve NPMLE. The algorithm works by assigning equal mass to uncensored observations. The mass of doubly censored observations is redistributed to the uncensored observations that are located in their quadrant. The mass for singly censored observations is redistributed onto to the uncensored observations that are contained in their \emph{half-line}. But if the time to event is continuous, the probability that any uncensored observation is contained in the \emph{half-line} of any singly censored observation is $0$. This means that the EM algorithm would not know how to redistribute the mass.\\

Van Der Laan suggested a repaired NPMLE. He introduced partition strips that were wide enough that in addition to singly censored observations they contained the uncensored observations. According to his approach, the mass of singly censored observations was redistributed to the uncensored observations that happened after censoring and that were in the same strip. The left panel of figure \ref{fig:bcMoodie} illustrates this approach. Point $A$ is censored in coordinate $T_1$, and its mass is assigned to point $B$ because $B$ is in the same strip and has a greater coordinate $T_1$ compared to the censorng time of $A$.  The author compared performance of several estimators and came to the conclusion that with no or small dependence, the Dabrowska's and Prentice and Cai's estimators performed better. With increased dependence however, the author's estimator performed better.\\

\begin{figure}[!h]
\caption{Mass redistribution in repaired NPMLE suggested by \cite{van1996efficient} (left panel) and repaired NPMLE with adjustment suggested by \cite{moodie2005adjustment} (right panel). The horizontal dotted lines denote the strips that define what censored points have their mass assigned to what uncensored points. The left panel shows that point $A$ is censored in coordinate $T_1$, and its mass is assigned to point $B$ because $B$ is in the same strip as point $A$. The right panel shows a different approach: instead of $B$ getting the mass of point $A$, an additional point is created and is assigned a mass according to the algorithm of \cite{moodie2005adjustment}.}
\includegraphics{Simulations/bivariateCaseStrips.pdf}
\label{fig:bcMoodie}
\end{figure}

\cite{moodie2005adjustment} pointed out that the method of Van Der Laan could be inefficient when the strips were large because the uncensored points could borrow the mass from singly censored on one coordinate observations that have very different event time for the other coordinate. For example, Figure \ref{fig:bcMoodie} (left panel) shows mass redistribution for repaired NPMLE: points $A$ and $C$ are \emph{closer} in coordinate $T_2$, yet it is point $B$ that is assigned the mass because it is in the same strip as point $A$. The authors suggested an improvement. They first applied Van Der Laan's method (including dividing the plane into strips containing uncensored and censored observations), and then redistributed the mass from uncensored observations onto points that were located on the half lines of the singly censored observations from the same strip. This is illustrated on the right panel of Figure \ref{fig:bcMoodie}. Point $A$ is censored in coordinate $T_1$, and instead of $B$ getting the mass of point $A$, an additional point is created and is assigned a mass according to the algorithm of \cite{moodie2005adjustment}.

They reported that this method improved efficiency and reduced computational complexity when computing the variance.

% \begin{figure}[!h]
% \includegraphics{MoodiePrentice2005.png}
% \caption{Moodie and Prentice suggestion.}
% \label{fig:bubbles}
% \end{figure}
% \clearpage

\subsection{Linear Rank Tests}
One might argue that there is no need to estimate a bivariate survival surface if our main question is the existence of association. We now summarize several papers whose authors developed tests that do just that. We chose these papers because their tests use cumulative distribution function of time to survival, which is the basis of the defintion of Probability Scale Residuals (PSR) defined by \cite{li2012new}. In order for us to show the advantages of testing association using PSR compared to the previous literature, we need to summarize PSR first.

\subsubsection{Probability Scale Residuals for Censored Data}

\cite{li2012new} proposed a new method of evaluating residuals for ordinal data. Suppose, $T$ is a random variable of time to event, and $F(t)$ is its cumulative distribution function. The Probability Scale Residuals are defined as:\\
$$
\begin{aligned}
	r(t,F) &= E\{sign(t,T)\} = pr(T < t) - pr(T > t) = F(t-) - (1-F(t)) =\\
	 &=F(t-) - 1 + F(t)
	\end{aligned}
$$

Where $E\{sign(t,T)\}$ is $-1$, $0$, and $1$ for $t<T$, $t=T$, and $t>T$ respectively.
The authors also extended this definition to two cases: censored time to event and current status data. In order to understand how the PSR were extended for censored data, let's assume that $T$ is time to event, but we only observe $Y = min(T, C)$, where $C$ is time to censoring. When we do not observe $T$, the PSR are defined in terms of $Y$ (time to event or censoring) and $\Delta$, where $\Delta = 1$ when $min(T, C)=T$ (we observe the event) and $\Delta =0$ when $min(T, C)=C$ (the event is not observed because of censoring). \textbf{By definition}, if $\Delta = 1$, then $Y=T$ and $r(y,F, \delta=1) = F(y-) - 1 + F(y)$. If $\Delta = 0$, then $T$ is unknown, except that it occurs some time after the censoring time $C$, so \textbf{by definition} $r(t,F, \delta=0) = E\{r(t,F)|T>y\}$. The authors prove that:

	$$
	\begin{aligned}
		r(y, F) &= F(y) + F(y-) - 1,~~~&\delta = 1 \\
		r(y, F) &= E\{r(T,F)|T>y\} = F(y) ,~~~&\delta = 0 \\
		&or\\
    r(y, F, \delta) &= F(y) - \delta(1 - F(y-)),~~~&where~\delta \in \{0,1\}
	\end{aligned}
	$$

In a similar manner, PSR are defined for current status data. Current status data is a pair $(C, \Delta)$, where variable $C$ is the time when we observe the subject, and variable $\Delta$ is whether the subject has already experienced the event or not. PSR for for current status data are defined as following:
	$$
	\begin{aligned}
		r(c, F) &= E\{r(T,F)|T\leq c\} = F(c) - 1,~~~&\delta = 1 \\
		r(c, F) &= E\{r(T,F)|T>c\} = F(c) ,~~~&\delta = 0 \\
		&or\\
    r(c, F, \delta) &= F(c) - \delta,~~~&where~\delta \in \{0,1\}\\
	\end{aligned}
	$$

The authors proved that in both cases the PSR have expectation of zero when $F$ is properly defined and $T \perp C$. They also showed how PSR were related to martingale, Cox-Snell, and deviance residuals and demonstrated that PSR could be used to examine the functional adequacy of the model, with positive PSR indicating that the event time was longer than expected. In addition to examining the functional adequacy of the model and other important properties, PSR can be used to measure association of two random variables of time to event. One of the intuitive ways to measure association between two time-to-event outcomes would be to compute their correlation. As it turns out, correlation of PSR is equivalent to the censored version of \emph{Spearman} correlation.

\subsubsection{Spearman Correlation and Correlation of PSR for Continuous Censored Outcome.}
In this following section we use the following notations. $S$, $C$, $\delta=I(S\leq C)$, $X = min(S, C)$ are time to event, time to censoring, event indicator, and time to event or censoring for the first outcome; and $T$, $D$, $\epsilon=I(T\leq D)$, $Y = min(T, D)$ are time to event, time to censoring, event indicator, and time to event or censoring for the for the second outcome.\\
Spearman correlation for \emph{continuous} outcome can be defined as $cor(F_S(S), F_T(T))$. In case of censored outcomes however, we don't always observe $S$ and $T$. This means that we have to modify the definition of Spearman correlation in the following way: When $\delta = 1$ or $S\leq C$, we have that $X = min(S, C)=S$ so $F_S(s)=F_S(x)$ and we use $F_S(x)$ in order to compute correlation. When $\delta = 0$ or $S > C$, we have that $X = min(S, C)=C$ and instead of $F_S(s)$, we use $E_S[F_S(s)|S>x]$ in order to compute correlation. Let's first compute $E_S[F_S(s)|S>x]$:
% keeping in mind that in continuous case $F_S(s-) = F_S(s)$
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)}\int_{(x, \infty)} F_S(s)dF_S(s) = \frac{1}{1-F_S(x)} \left.\frac{ [F_S(s)]^2}{2}\right|_{(x, \infty)} = \frac{1-[F_S(x-)]^2}{2(1-F_S(x))} \\
	\end{aligned}
	$$
Since this is a continuous case, we can assume that $F_S(x)=F_S(x-)$ therefore we have
	$$
	\begin{aligned}
		E_S[F_S(x)|S>X] &=\frac{1-[F_S(x)]^2}{2(1-F_S(x))}=\frac{\cancel{(1-F_S(x))}(1+F_S(x))}{2\cancel{(1-F_S(x))}}  = \frac{1+F_S(x)}{2} \\
	\end{aligned}
	$$
So, for the continuous censored data, Spearman correlation can be defined as (p. 252, \cite{dabrowska1986rank}):
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta F_S(x) + (1-\delta) \frac{1+F_S(x)}{2},~\epsilon F_T(y) + (1-\epsilon) \frac{1+F_T(y)}{2}  \right)\\
		&=~ cor\left(  \frac{1+\delta}{2}F_S(x) +  \frac{1-\delta}{2},~\frac{1+\epsilon}{2}F_T(y) +  \frac{1-\epsilon}{2}  \right)\\
		&=~ cor\left(  (1+\delta)F_S(x) +  1-\delta,~(1+\epsilon)F_T(y) +  1-\epsilon  \right)\\
	\end{aligned}
	$$
The correlation of PSR in the continuous case ($F_S(s-) = F_S(s)$) is:
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  (1+\delta)F_S(x) -\delta,~(1+\epsilon)F_T(y) -\epsilon  \right)\\
	\end{aligned}
	$$

It is easy to check that the expressions for \emph{Spearman correlation} and \emph{PSR correlation} are equivalent.
	
\subsubsection{Spearman Correlation and Correlation of PSR for Discrete Censored Outcome.}
For the discrete case, Spearman correlation can be defined as:
	$$
	\begin{aligned}
		cor\left( \frac{F_S(s) + F_S(s-)}{2},~\frac{F_T(t) + F_T(t-)}{2} \right)\\
	\end{aligned}
	$$
Similarly to continuous case, if we do not observe time to event, instead of $F_S(x)$ and $F_S(x-)$ we take the expectation:  $E_S[F_S(x)|S>x]$ and $E_S[F_S(x-)|S>x]$ respectively. Let's first find $E_S[F_S(x)]$. For brevity, we denote $Pr\left\{S=s\right\}$ as $P_s$:
	$$
	\begin{aligned}
		E_S[F_S(x)] &= \sum_{i=0}^{\infty}F_S(i)P_i = \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		&= (P_0 P_0) + (P_0 P_1 + P_1 P_1) + (P_0 P_2 + P_1 P_2 + P_2 P_2) + ...= \\
		&= P_0^2 + P_1^2 + P_2^2 + ... + P_0 P_1 + P_0 P_2 + P_2 P_1 + ... =\frac{1}{2}(P_0 + P_1 + P_2 + ...)^2 + \frac{1}{2}(P_0^2 + P_1^2 + P_2^2 + ...) =\\
		&= \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2
	\end{aligned}
	$$
Now it is easier to derive $E_S[F_S(x)|S>x]$:
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 + \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$

In a similar manner, we derive $E_S[F_S(x-)|S>x]$, assuming that $F_S(0-)=0$:
	$$
	\begin{aligned}
		E_S[F_S(x-)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i-1)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i-1}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} - \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 - \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$
Now, let's recall that in the discrete case each Spearman correlation component is $\frac{F_S(s) + F_S(s-)}{2}$, so let's compute it for the case when $\delta=0$:
	$$
	\begin{aligned}
	  \frac{E_S[F_S(x)|S>x] + E_S[F_S(x-)|S>x]}{2} &= \frac{1}{2}\left[\frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}   +   \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\right]=\\
		&= \frac{1 - F_S^2(x)}{2(1-F_S(x))}= \frac{1 + F_S(x)}{2}\\
	\end{aligned}
	$$
As a result, we have:
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta\frac{F_S(x) + F_S(x-)}{2} + (1-\delta) \frac{1 + F_S(x)}{2}, ~\epsilon\frac{F_T(y) + F_T(y-)}{2} + (1-\epsilon) \frac{1 + F_T(y)}{2} \right)=\\
		&=~ cor\left(  \delta( F_S(x) + F_S(x-)) + (1-\delta) (1 + F_S(x)), ~\epsilon (F_T(y) + F_T(y-)) + (1-\epsilon) (1 + F_T(y)) \right)=\\
		&=~ cor\left( \cancel{\delta F_S(x)} + \delta F_S(x-) +  1 + F_S(x) -\delta - \cancel{\delta F_S(x)}, ~\cancel{\epsilon F_T(y)} + \epsilon F_T(y-) + 1 + F_T(y)-\epsilon -\cancel{\epsilon F_T(y)}   \right)=\\
		&=~ cor\left( F_S(x) + \delta F_S(x-) +  1 -\delta , ~ F_T(y) + \epsilon F_T(y-) + 1 -\epsilon   \right)\\
	\end{aligned}
	$$
It can be easily shown that the above expression is equivalent to correlation of \emph{PSR} in the discrete case:
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  F_S(x) + \delta F_S(x-) -\delta,~F_T(y) + \epsilon F_T(y-) -\epsilon  \right)\\
	\end{aligned}
	$$

We have proved the equivalence of \emph{Spearman} correlation and the correlation of the PSR based on the following definition of \emph{Spearman} correlation: $corr(F_S(s), F_T(t))$. It is also possible to show the equivalence of \emph{Spearman} correlation and the correlation of PSR using another definition: $E(sign(S-S_0)sign(T-T_0))$, where $(S,T)$ is a paired event time, and $S_0 \perp T_0$.\\
~\\
We now summarized papers that in the search of a linear rank test for association of bivariate survival data, arrived to a censored version of \emph{Spearman} correlation.


\subsection{Censored Version of Spearman Correlation in Earlier Research}

\cite{prentice1978linear}  considered a model of linear regression with time to event as an outcome, while allowing the time to be censored.
	$$
	\begin{aligned}
		y = \alpha + \beta z + \sigma e
	\end{aligned}
	$$
The author's goal was to develop a linear rank statistic in order to check the following hypothesis: $\beta = 0$. Although this model described univariate survival, we are interested in it because of the author's approach to censored observations.
%In order to do this he had to find an approach to rank survival outcome in the presence of censored data. He chose a simplified approach to the ranking of censored values [Kalbfleisch \& Prentice, 1973].
According to this approach, uncensored observations were ranked among themselves, while censored observations were ranked according to their location relatively to the uncensored observations. Prentice derived a linear rank statistic based on this simplified approach to ranking censored observations.\\
%Even after using ranks and approximations, Prentice's approach required assuming $F_i$ - distribution of residuals (check this please).

Using the results of Prentice's work, \cite{cuzick1982rank} considered a general model of association:
	$$
	\begin{aligned}
		Y_1 = aZ + e_1 ~~~~~~~~ Y_2 = bZ + e_2,
	\end{aligned}
	$$
where $Y_1$, and $Y_2$ were event times that could be censored, and random variable $Z$ and $e_1$ and $e_2$ were indenpendent, and finally $b=a\lambda,~~0<|\lambda|<\infty$. The model suggested that two random variables $(Y_1, Y_2)$ were related to a third unobserved covariate.\\
Cuzick assumed the same generalized ranking of censored data as \cite{prentice1978linear}: the uncensored observations were ranked among themselves and the censored observations were given the same rank as its closest uncensored observation on the left. Although his approach was rather general, it did require knowing $f_i(x)$ the density of $e_i$, and as an illustration, he assumed a logistic density of $e_1$ and $e_2$:  $f(x) = 2\pi_{-1} e^{-x}/(1+e^{-x})^2$. It turned out that for this particular case the rank of each observation with no censoring was $F(x)$, and in the presence of censoring, the rank was $\frac{1+F(x)}{2}$, which resulted in the association test statistic equivalent to \emph{Spearman} correlation.\\

Another author derived similar statistic but for a different hypothesis and under different assumptions. \cite{dabrowska1986rank} developed a semi-parametric statistic for testing an association of two survival outcomes, possibly censored, with the following null hypothesis: $H_0:~F=F_1 F_2$. In order to test this hypothesis, the author further developed the results of \cite{prentice1978linear} and \cite{cuzick1982rank} , and suggested a linear rank statistic and its simplification in the following form:
	$$
	\begin{aligned}
		 S_n = \sum_{n=1}^N \mathcal{J}_1( u_1, \delta_{1n}) \cdot \mathcal{J}_2( u_2, \delta_{2n}),
	\end{aligned}
	$$
where $\hat{F}_i$ were estimators similar to the usual \emph{Kaplan-Meier} estimators of the marginal \emph{CDF}'s. She showed that under certain assumptions and with a particular choice of $\mathcal{J}_i(u_i,d_i)$, $\mathcal{J}_i(u_i,d_i) =d_i-(1+d_i)u_i$, the test statistic corresponds to the censored data version of the Spearman test. Substituting this choice of $\mathcal{J}_i(u_i=F_i,~d_i=\delta_i)$ into the above expression, we get the following statistic:
	$$
	\begin{aligned}
		 S_n & = \sum_{n=1}^N (\delta_{1n} - (1+\delta_{1n})F_1)\cdot (\delta_{2n} - (1+\delta_{2n})F_2)=\\ 
		 &=\sum_{n=1}^N (F_1 - \delta_{1n}(1-F_1))\cdot (F_2 - \delta_{2n}(1-F_2))\\
	\end{aligned}
	$$
Because $PSR(x, \delta) = F(x)-\delta(1-F(x))$, the above expression is the covariance of probability scale residuals, and can be scaled to obtain \emph{Spearman} correlation.\\

\cite{ding2004testing} suggested the following statistic for testing independence for bivariate \emph{current status data}:
	$$
	\begin{aligned}
		 E[cov(\delta_1, \delta_2)|C_1,C_2] = E\left\{ [\delta_1 - F_1(C_1)][\delta_2 - F_2(C_2)]  \right\}\\
	\end{aligned}
	$$
Which is exactly covariance of PSR for \emph{current status data}.\\
~\\
Although censored version of \emph{Spearman} correlation was suggested before PSR were introduced, the framework for \emph{Spearman} correlation statistics was based on specific distribution assumptions were not generalized for discrete data. The using the tests statistic computed as correlation of PSR does not have require distribution assumptions and can be applied to discrete time to event.

\section{DISCUSSION AND FUTURE RESEARCH}

In this paper we summarized several different approaches to address association of bivariate survival data. Our focus was on the three main approaches. We first summarized literature on evaluating a \emph{cross ratio} in the context of a parametric model. This approach was later developed to include larger family of models, and to generalize the estimate of this association measure to a non-parametric approach. The second approach was to evaluate a bivariate survival surface as a product of two marginal survival functions and a function representing the association component between two variables of time to event. The third approach was developing association tests for bivariate variables of time to event.\\

Although all approaches were based on a complex mathematical and statistical theory, their main disadvantages were parametric assumptions, computational complexity, and lack of generalizability. For example, \cite{cuzick1982rank}, \cite{dabrowska1988kaplan}, and \cite{ding2004testing} derived statistics identical to correlation of PSR for continuous case, but their conclusions were rooted in a semi-parametric context, with very specific assumptions about the distribution of time to event, asymptotic properties of the statistics, and ranking of censored events. Also, their results were not generalized to discrete outcomes.\\

The use of correlation of PSR, on the other hand, does not require parametric assumptions and can be generalized to discrete censored data and to discrete currents status data. The test of association can be performed using the following statistic:
$$
\begin{aligned}
	T = \frac{\sum_{i=1}^{n} (r_{si} - \bar{r}_s)(r_{ti} - \bar{r}_t)}{\sqrt{\sum_{i=1}^{n} ((r_{si} - \bar{r}_s)^2\sum_{i=1}^{n} (r_{ti} - \bar{r}_t)^2}}
\end{aligned}
$$
where $r_{si}$ and $r_{ti}$ are PSR and $\bar{r}_s = \frac{1}{n}\sum_{i=1}^{n} r_{si}$ and $\bar{r}_t = \frac{1}{n}\sum_{i=1}^{n} r_{ti}$. For ordinal uncensored data, \cite{li2010test} suggested two ways of performing the inference: 1) a bootstrap procedure and 2) a large sample approximation. In our future work we will focus on proving the validity of applying bootstrap procedure to finding the confidence intervals for $T$, and on finding the asymptotic distribution for $T$ for discrete censored data and \emph{current status} data.\\

We also plan to focus on conditional and partial correlation of PSR for censored and current status data in order to test adjusted association of time to event and adjusted association of \emph{current status}. Partial correlation can be obtain by fitting a parametric or semi-parametric survival model using covariates of interest, and then computing correlation of the resulting PSRs. Conditional correlation can also be computed by first fitting a parametric or semi-parametric survival model, and then evaluating conditional moments that approximate conditional correlation of PSR.

Another goal of our future work is to compare the performance of test based on covariance of PSR with the performance of test described in \cite{shih1996tests}. The authors suggested two test statistics based on the covariance process of the martingale residuals: a supremum of covariance and a weighted covariance. \cite{shepherd2016probability} have already established the connection between PSR and martingale residuals and have compared the performance of these two residuals when checking functional adequacy of model fit. In the future we are planning to compare the performance of PRS correlation with the performance of the statistics suggested by \cite{shih1996tests} using optimal and suboptimal weights and with varying fraction of censored data.

% \subsection{Zhang, 2008: Inference on the association measure for bivariate survival data with hybrind censoring and applications to an HIV study, ref ?}
% The author uses archimedian copulas and Kendall's $\tau$ to assess association of bivariate survival data with hybrid censoring.
%
% \section{Summary of: An Adjustment to Improve the Bivariate Survival Function Repaired NPMLE. Moodie, Prentice, 2005 \cite{moodie2005adjustment}}
%
% \subsection{Alvo and Cabilio, 1995 citation ?}
% UNDER CONSTRUCTION.\\
% The authors use a concept of distance applied to \emph{Spearman} and \emph{Kendal} correlation measure in order to build a test that can deal with missing data.


%
% \section{Oral Exam Stuff}
% \begin{itemize}
% 	\item Probability scale residuals, PSR (Li\&Shepard)
% 	\item Challanges and limitations (discrete data covariates)
% 	\item Qi - covariates (look at prentice and Cuzick)
% 	\item show that this is Spearman
% 	\item apply to dataset
% 	\item simmulations
% 	\item explore performance of partial and conditional Spearman.
% 	\item Understand Qi's conditional, partial, and conditional-partial correlation
% 	\item Develop Spearman correlation for different $s$ and $t$: instead of having one number $\rho$ develop a new measure $\rho(s, t)$.
% 	\item \textbf{READ: Hanley, J. A., and Parnes, M. N. (1983), Nonparametric Estimation of a Multivariate Distribution in the Presence of Censoring, Biometrics, 39, 129-139.}
% \end{itemize}
~\\
% \subsubsection{Shih, Louis, 1996, \cite{shih1996tests}}
% \textbf{Motivation}: Leukemia patients that undergo bone marrow transplantation are at risk of \emph{acute graft versus host disease (AGVHD)} and \emph{cytomegalovirus (CMV)}, and the question is if these events are correlated:
%
% The authors consider martingale residuals, $M_{ij}$, which is the difference between observed and expected deaths. They base their new statistic on the result of Clayton [\cite{clayton1978model}] and Cuzick [\cite{shih1996tests}] who developed a model for bivariate time to event with frailties, and suggested the following test statistic that measured the sample covariance of the martingale residuals:
% $$
% \begin{aligned}
%   T_n=\sum_i \left\{ \delta_{i1} - \hat{\Lambda}(X_{i1}) \right\} \left\{ \delta_{i2} - \hat{\Lambda}(X_{i2}) \right\} =\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2).
% \end{aligned}
% $$
%
% Shih and Louis suggested to improve its power by introducing the following two statistics:
%
% \begin{enumerate}
% 	\item take supremum: $U_n= \sup_{0\leq t \leq t_0} \left|\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)\right|$
% 	\item weigh it: $V_n= \sum_i \int_0^{t_0}  \int_0^{t_0} W_n(u_1,u_2) d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)$ (which is analogous to the weighted logrank test)
% \end{enumerate}
% They choose \emph{optimal} weights by somehow using the \emph{cross ratio} defined by Oakes $\theta = \frac{ \frac{\partial^2 S(s,t)}{\partial s \partial t} S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}$:
%
% Although statistic $U_n$ is a supremum over $t_1=t_2$, the simulations showed promising results.
% For $V_n$, if the weights are not specified correctly, the test based on $V_n$ looses power.


% Lucy's way:
%\printbibliography
% Jonathan's way:
%\bibliographystyle{plainnat}

\bibliography{summaryOfRef}
%\clearpage

\addcontentsline{toc}{section}{Appendix A}
\section*{Appendix A} \label{appendixA}
Here we show that the \emph{cross ratio} of \cite{clayton1978model} can be written in the following ways:
	$$
	\begin{aligned}
	  \theta = \frac{ \lambda_s(s|T=t)}{\lambda_s(s|T>t)} = \frac{ \lambda_t(t|S=s)}{\lambda_t(t|S>s)} = \frac{ \frac{\partial^2 S(s, t)}{\partial s\partial t} \cdot S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}
	\end{aligned}
	$$

Let $f(s,t)$ and $S(s,t)$ be joint density and survival function respectively of age at which fathers ($T$) and sons ($S$) succumb to disease, then we can write:
	$$
	\begin{aligned}
		\frac{\partial^2 S(s, t)}{\partial s\partial t} &= f(s,t) = f(s|t)\cdot f(t)\\
		\frac{\partial S(s,t)}{\partial t} &= \int_s^{\infty}f(u, t)du = \int_s^{\infty}f(u|t)f_t(t)du = S(s|t)\cdot f_t(t)\\
		\frac{\partial S(s,t)}{\partial s} &=...= S(t|s)\cdot f(s) = Pr(T>t|s)\cdot f(s) = Pr(T>t, s) = Pr(s,T>t) = Pr(s|T>t)S_t(t) = \\
		&=f(s|T>t)\cdot S_t(t)\\
		S(s,t) &= Pr(S>s|T>t)\cdot Pr(T>t) = S(s|T>t)\cdot S_t(t)
	\end{aligned}
	$$
By substituting the right-hand sides of the above equalities into the expression for $\theta$, we get:
	$$
	\begin{aligned}
		\theta &= \frac{\frac{\partial^2 S(s, t)}{\partial s\partial t}/\frac{\partial S(s, t)}{\partial t}}  {\frac{\partial S(s, t)}{\partial s}/ S(s, t) } = \frac{  \frac{f(s|t)\cancel{f_t(t)}}{S(s|t)\cancel{f_t(t)}} } { \frac{f(s|T>t)\cdot \cancel{S_t(t)}}{S(s|T>t)\cdot \cancel{S_t(t)}}  }
		  = \frac{f(s|t)/S(s|t)}     {f(s|T>t) / S(s|T>t) }\\
	\end{aligned}
	$$
By defintion, $\lambda_s(s|t) = \frac{f(s|t)}{S(s|t)}$ and $\lambda_s(s|T>t) = \frac{f(s|T>t)}{S(s|T>t)}$, therefore:
	$$
	\begin{aligned}
		\theta &=\frac{f(s|t)/S(s|t)}     {f(s|T>t) / S(s|T>t) }
		= \frac{\lambda_s(s|t) } {\lambda_s(s|T>t) }\\
	\end{aligned}
	$$
Because of the symmetry, we can write:
	$$
	\begin{aligned}
	  \theta = \frac{ \frac{\partial^2 S(s, t)}{\partial s\partial t} \cdot S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}} = \frac{ \lambda_s(s|T=t)}{\lambda_s(s|T>t)} = \frac{ \lambda_t(t|S=s)}{\lambda_t(t|S>s)}~~\blacksquare
	\end{aligned}
	$$
\clearpage

\addcontentsline{toc}{section}{Appendix B}
\section*{Appendix B} \label{appendixB}

In the same paper where \cite{dabrowska1988kaplan} derived the estimator of bivariate survival surface, she gave an example data for which her estimator assigned a negative mass. Figure \ref{fig:bubbles} shows this data graphically. The red points represent uncensored observations. The orange point is a singly censored observation (it is censored on the $T_2$ axis). The white point is an observation censored in both components, $T_1$ and $T_2$. The values for each element of the Dabrowska's estimator can be found in Table \ref{dabrExTable}. The computations following this table show that the bivariate survival function estimator in point $(0.51,~0.02)$ (see Figure \ref{fig:bubbles}) was greater than its marginal estimator in point $(0.51,~0)$.
\begin{figure}[!h]
\caption{Example that demonstrates assignment of negative point mass. The red points represent uncensored observations. The orange point is a singly censored observation (it is censored on the $T_2$ axis). The white point is an observation censored in both components. The number at risk is plotted near each point.}
\includegraphics{figure1.pdf}
\label{fig:bubbles}
\end{figure}

We use Table \ref{dabrExTable} and the formula below to order to show that $\hat{S}(t_1=0.51,~t_2=0) < \hat{S}(t_1=0.51,~t_2=0.02)$:

	$$
	\begin{aligned}
		\hat{S}(t_1,t_2) &= \hat{S}(t_1,0)\hat{S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
		\hat{L}(\Delta u, \Delta v) &= \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u, \Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}
	\end{aligned}
	$$
Where:
	$$
	\begin{aligned}
	&\hat{\Lambda}(\Delta u, \Delta v) = \#(T_1=u, T_2=v, \delta_1=\delta_2=1)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{10}(\Delta u, v^-) = \#(T_1=u, \delta_1=1, T_2\geq v)/\#(T_1\geq u, T_2\geq v)\\
	&\hat{\Lambda}_{01}(u^-,\Delta  v) = \#(T_2=v, \delta_2=1, T_1\geq u)/\#(T_1\geq u, T_2\geq v)
	\end{aligned}
	$$


\input{dabrowskaExampTable}
From Table \ref{dabrExTable} we have:
	$$
	\begin{aligned}
		&\hat{S}(0.51,~0)  = 0.375\\
		&\hat{S}(0,~0.02)  = 0.75\\
		&\hat{S}(0.51,~0.02) = \hat{S}(0.51,~0)\hat{S}(0,~0.02) \times 
		\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}=\\
		&~~~~~~~~~~~~~= 0.375 \cdot 0.75 \cdot (1-0.1111)\cdot (1-0)\cdot (1-(-1)) = \\
		&~~~~~~~~~~~~~= 0.5
	\end{aligned}
	$$

We observe that $\hat{S}(0.51,~0) = 0.375 < 0.5 = \hat{S}(0.51,~0.02)$.\\

Note that when there is no censoring, there is no negative mass in point $(0.51,~0.02)$: .
	$$
	\begin{aligned}
		&\hat{S}(0.51,~0.02) = 0.375 \cdot 0.75 \cdot (1-0.1111)\cdot (1-0.25)\cdot (1-(-1)) = \\
    &~~~~~~~~ = 0.375 = \\
    &~~~~~~~~ =\hat{S}(0.51,~0)
	\end{aligned}
	$$



% \begin{verbatim}
% ### simulate events at times S and T and censoring times C, D, and produce X,Y, delta,epsilon
% ############################# figure out X-:
% xMinus = function(X){
%   sortedX = X
%   sortedX = c(0, sortedX[1:(length(X)-1)])
%   sortedX[order(order(X))]
% }
%
% set.seed(20160919)
% subjNum = 100
% simNum = 2000
% data = matrix(NA, nrow=subjNum, ncol=8)
% one = rep(NA, simNum)
% two = rep(NA, simNum)
% two1 = rep(NA, simNum)
% colnames(data) = c("S", "C", "X", "delta", "T", "D", "Y", "epsilon")
%
% for (si in 1:simNum){
%   ### first simulate zero correlation using exponential distribution
%   ### unequal rate
%   rates = c(3, 5, 4, 6)
%   names(rates) = c("S", "T", "C", "D")
%   for (ri in names(rates)){
%     data[,ri] = rexp(n=subjNum, rate = rates[ri])
%   }
%
%   data[,"delta"] = as.numeric(data[,"S"] <= data[,"C"])
%   data[,"X"] = data[,"C"]
%   data[,"X"][data[,"delta"] == 1] = data[,"S"][data[,"delta"]==1]
%
%   data[,"epsilon"] = as.numeric(data[,"T"] <= data[,"D"])
%   data[,"Y"] = data[,"D"]
%   data[,"Y"][data[,"epsilon"] == 1] = data[,"T"][data[,"epsilon"]==1]
%
%   # Fs <- ecdf(data[,"S"])
%   # PSRsx = Fs(data[,"X"]) - data[,"delta"]*(1 - Fs(xMinus(data[,"X"])))
%
%   Fx <- ecdf(data[,"X"])
%   PSRxx = Fx(data[,"X"]) - data[,"delta"]*(1 - Fx(xMinus(data[,"X"])))
%   #PSRxx = Fx(data[,"X"]) - data[,"delta"]*(1 - Fx((data[,"X"])))
%   Fy <- ecdf(data[,"Y"])
%   PSRyy = Fy(data[,"Y"]) - data[,"epsilon"]*(1 - Fy(xMinus(data[,"Y"])))
%   #PSRyy = Fy(data[,"Y"]) - data[,"epsilon"]*(1 - Fy((data[,"Y"])))
%
%   one[si] = cor(PSRxx, PSRyy)
%   # spearman correlation assuming continuous outcome
%   #two[si] = cor((Fx(data[,"X"]) + (1-data[,"delta"]))*(1+data[,"delta"])/2, (Fy(data[,"Y"]) + (1-data[,"epsilon"]))*(1+data[,"epsilon"])/2)
%   # spearman correlation twicked for discrete censored outcome
%   #two[si] = cor(Fx(data[,"X"]) + data[,"delta"]*Fx(xMinus(data[,"X"])) + 1-data[,"delta"]^2, Fy(data[,"Y"]) + data[,"epsilon"]*Fy(xMinus(data[,"Y"]))+1-data[,"epsilon"]^2)
%
%   ### The correct way of defining spearman as (F(x)+F(x-))/2 for discrete case:
%   censoredPartX = 1+Fx(data[,"X"])
%   censoredPartY = 1+Fy(data[,"Y"])
%   censoredPartX[!is.finite(censoredPartX) | is.na(censoredPartX)] = 0
%   censoredPartY[!is.finite(censoredPartY) | is.na(censoredPartY)] = 0
%   two[si] = cor((data[,"delta"]/2)*(Fx(data[,"X"]) + Fx(xMinus(data[,"X"])))   +   ((1-data[,"delta"])/2) * censoredPartX,   (data[,"epsilon"]/2)*(Fy(data[,"Y"]) + Fy(xMinus(data[,"Y"])))   +   ((1-data[,"epsilon"])/2) * censoredPartY )
% }
%
% plot(one, two, xlim=range(c(one, two)), ylim=range(c(one, two)), col="#55555533", pch=19, cex=.7)
% abline(0, 1, col="#55555533")
% cor(one, two)
%
% # plot(PSRsx, PSRxx)
% # plot(PSRxx)
%
% # formally show PRS = what you have
% # paper of Dabrowska and Prentice (and the book)
% # try to get another definition of Spearman correlation
%
% ### Example from Dabrowska, 1988, Kaplan-Meier estimate of the plane
% ### Example from Dabrowska, 1988, Kaplan-Meier estimate of the plane
% ### Example from Dabrowska, 1988, Kaplan-Meier estimate of the plane
% ### Example from Dabrowska, 1988, Kaplan-Meier estimate of the plane
% library(survival)
% kmdata = data.frame(Y1 = c(.51, .68, .11, .24), d1 = c(1, 1, 1, 0), Y2 = c(.02, .68, .62, .24), d2 = c(1, 1, 0, 0))
% KM1 <- survfit( Surv(kmdata$Y1, kmdata$d1)~ 1)$surv
% KM2 <- survfit( Surv(kmdata$Y2, kmdata$d2)~ 1)$surv
% KM1
% KM2
%
% X = c(rep(.11, 4), rep(.24, 4), rep(.51, 4), rep(.68, 4))
% Y = c(rep(c(.02, .24, .62, .68), 4))
% lam11 = c(rep(0, 8),   .5,   rep(0, 6),   1)
% lam10 = c(c(.25, 1/3, 1/2, 1),    rep(0, 4),   0.5,   rep(1,7))
% lam01 = c(.25, c(0, 0, 1),   1/3,   c(0, 0, 1),   .5,   c(0, 0, 1),   1,   c(0, 0, 1) )
% numAtRisk = c(4,3,2,1,   3,2,1,1,   2,1,1,1,   1,1,1,1)
% L = (lam10*lam01 - lam11)/((1 - lam10)*(1 - lam01))
% bivarSurvEstData = data.frame(X=X, d1=NaN, Y=Y, d2=NaN, lam11=lam11, lam10=lam10, lam01=lam01, KM1=rep(KM1, each=4), KM2 = rep(KM2, 4), L=L, numAtRisk = numAtRisk)
% bivarSurvEstData[bivarSurvEstData$X == 0.51 & bivarSurvEstData$Y == 0.02, c("d1", "d2")] = c(1, 1)
% bivarSurvEstData[bivarSurvEstData$X == 0.68 & bivarSurvEstData$Y == 0.68, c("d1", "d2")] = c(1, 1)
% bivarSurvEstData[bivarSurvEstData$X == 0.11 & bivarSurvEstData$Y == 0.62, c("d1", "d2")] = c(1, 0)
% bivarSurvEstData[bivarSurvEstData$X == 0.24 & bivarSurvEstData$Y == 0.24, c("d1", "d2")] = c(0, 0)
%
% ############## estimator at point (X,Y) = (.51, .02):
% L_11_02 = bivarSurvEstData$L[bivarSurvEstData$X == .11 & bivarSurvEstData$Y == .02]
% L_24_02 = bivarSurvEstData$L[bivarSurvEstData$X == .24 & bivarSurvEstData$Y == .02]
% L_51_02 = bivarSurvEstData$L[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
% KM1_51_02 = bivarSurvEstData$KM1[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
% KM2_51_02 = bivarSurvEstData$KM2[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
%
% BivarSurvEstimate_51_02 = KM1_51_02 * KM2_51_02 * (1-L_11_02) * (1-L_24_02) * (1-L_51_02)
% BivarSurvEstimate_51_02
%
% BivarSurvEstimate_51_02 > KM1_51_02
%
% ############# plot of Dabrowska example:
% ############# plot of Dabrowska example:
% ############# plot of Dabrowska example:
% ylim = range(c(X, Y))
% cex = 0.7
% par(mar=c(4, 4, 2, 2))
% plot(ylim, ylim, type="n", axes=FALSE, xlab="T1", ylab="T2")
% segments(x0=c(0, unique(X)), y0=0, y1 = range(Y)[2], lty=3, col="gray")
% segments(x0=0, y0=c(0, unique(Y)), x1 = range(X)[2], lty=3, col="gray")
% axis(side=1, at = unique(X), col = "gray", cex.axis=cex)
% axis(side=2, at = unique(Y), col = "gray", cex.axis=cex)
% koef = .06/sqrt(2)
% segments(x0=c(.11, .24), y0=c(.62, .24), x1=c(.11, .24+koef), y1=c(.68, .24+koef))
% points(unique(X), c(.62, .24, .02, .68), cex=1.2, pch=21, bg=c("orange", "orange", "red", "red"), col="black")
% offset = 0.015
% text(bivarSurvEstData$X-offset, bivarSurvEstData$Y-offset, bivarSurvEstData$numAtRisk, cex=cex)
%
%
%
% setwd("/Users/svetlanaeden/stuff/StatStuff/BRYAN/Simulations")
% source("latextablezoo.R")
%
% tableData = bivarSurvEstData
% for (ni in c("lam10", "lam01", "L")){
%   tableData[[ni]] = round(tableData[[ni]], 4)
% }
% for (ni in c("d1", "d2", "L")){
%   tableData[[ni]][is.na(tableData[[ni]])] = ""
% }
% tableData[["L"]][tableData["L"]==Inf] = "$\\infty$"
% # pattern = rep(0, nrow(bivarSurvEstData))
% # pattern[bivarSurvEstData$Y==0.02] = 1
% pattern = rep("lightwhite", nrow(bivarSurvEstData))
% pattern[tableData$Y==0.02 & tableData$X != .68] = "lightgray"
% zebraTable(file="../dabrowskaExampTable.tex", caption = "Elements of Dabrowska's Bivariate Surface Estimator. The highlighted rows are the points used in the computation of the negative mass at: $(t_1,t_2) = (0.51, 0.02)$", fontSize="small", dataframe=tableData, zebraPattern="none", vars = c("numAtRisk", "X", "d1", "Y", "d2", "lam11", "lam10", "lam01", "KM1", "KM2", "L"), colNames = c("Number at risk", "$t_1$", "$\\delta_1$", "$t_2$", "$\\delta_2$", "$\\hat{\\Lambda}_{11}(\\Delta t_1, \\Delta t_2)$", "$\\hat{\\Lambda}_{10}(\\Delta t_1, t^-_2)$", "$\\hat{\\Lambda}_{01}(t^-_1, \\Delta t_2)$", "$\\hat{S}(t_1, 0)$", "$\\hat{S}(0, t_2)$", "$\\hat{L}$"), pattern = pattern, fixedColVars = c("numAtRisk"), fixedColWdths = c(33), marker = "dabrExTable", toLatexChar=FALSE)
% #, by=names(dataframe)[1], orderGroups=FALSE, rightVars=c(), centerVars=c(), fixedColVars=c(), fixedColWdths=c(), toLatexChar=TRUE, pallet=NULL, pattern=NULL, marker=NULL, append=FALSE)
%
% ####################### Problem with bivariate survaval surface estimation
% ####################### Problem with bivariate survaval surface estimation
% ####################### Problem with bivariate survaval surface estimation
%
% ####################### kaplan-meier plots:
% ####################### kaplan-meier plots:
% pdf("kaplanMeierExplanation.pdf", width=7, height=2.5)
% par(mfrow=c(1, 2), mar=c(3,4,2,2))
%
% x = c(.6, .8, 1.2, 1.5)
% y = c(1, 2/3, 1/3, 0)
% plot(c(0, max(x)), range(y), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% axis(1, c (0, x), labels=c("0", "A", "B", "C", ""), cex.axis = .7)
% axis(2, y, labels=c("1", "2/3", "1/3", "0"), cex.axis = .7)
% segments(x0=c(0, x), y0=c(y, 0), x1=x[c(1:4, 4)], y1=y[c(1:4, 4)])
% segments(x0=x[1:3], y0=y[1:3], x1=x[1:3], y1=y[2:4], lty=3)
% text(x[1:3], y[2:4], labels=rep("1/3", 3), cex=.5, pos=2)
% #mtext("Time", 1, cex=.7, line=2)
% points(x[1:3], y[1:3], pch=21, bg="white", col="black", cex=.7)
% points(x[1:3], y[2:4], pch=21, bg="black", col="black", cex=.7)
%
% x1 = c(.8, 1.2, 1.5)
% y1 = c(1, 1/2, 0)
% plot(c(0, max(x)), range(y), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% # segments(x0=c(0, x), y0=c(y, 0), x1=x[c(1:4, 4)], y1=y[c(1:4, 4)], col="gray", lwd=3)
% # segments(x0=x[1:3], y0=y[1:3], x1=x[1:3], y1=y[2:4], lty=3, col="gray", lwd=3)
% # points(x[1:3], y[2:4], pch=21, bg="red", col="black", cex=.7)
% axis(1, c (0, x), labels=c("0", "A", "B", "C", ""), cex.axis = .7)
% axis(2, y1, labels=c("1", "1/2", "0"), cex.axis = .7)
% lines(x=c(0, x), y=c(y, 0), type="s", col=gray(.9), lwd=4)
% segments(x0=c(0, x1), y0=c(y1, 0), x1=x1[c(1:3, 3)], y1=y1[c(1:3, 3)])
% segments(x0=x1[1:2], y0=y1[1:2], x1=x1[1:2], y1=y1[2:3], lty=3)
% mtext("(censored)", side=1, at= x[1], cex=.7, line=1.6)
% text(x1[1:2], y1[2:3], labels=rep("1/3 + 1/6 = 1/2", 2), cex=.5, pos=2)
% points(x1[1:2], y1[1:2], pch=21, bg="white", col="black", cex=.7)
% points(x1[1:2], y1[2:3], pch=21, bg="black", col="black", cex=.7)
% dev.off()
%
% ####################### bivariate case:
% ####################### bivariate case:
% pdf("bivariateCase.pdf", width=7, height=3)
% par(mfrow=c(1, 2), mar=c(3,4,2,2))
%
% x = c(.6, .8, 1.2, 1.5)
% y = rep(.5, 4)
% plot(c(0, max(x)), c(.3, .7), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% box(col="gray")
% segments(x0=x[1], y0=y2[1], x1=x[1] + 10, y1=y2[1], col="gray")
% segments(x0=x[1], y0=unique(y), x1=x[1] + .1, y1=unique(y))
% points(x[1:3], y[2:4], pch=21, bg=c("yellow", "red", "red"), col="black")
% mtext(c("0", "T1"), 1, line=.2, cex=.7, at=c(-.15, x[length(x)]))
% mtext("T2", 2, line=.5, cex=.7, at=.7, las=2)
% text(x, y, c("A", "B", "C", ""), pos=3, cex=.7)
% text(x, y, labels=c("", rep("1/3 + 1/6", 2), ""), cex=.5, pos=1)
% #points(x[1:3], y[1:3], pch=21, bg="white", col="black", cex=.7)
%
% y2 = c(0.5, .35, .6, .5)
% plot(c(0, max(x)), c(.3, .7), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% box(col="gray")
% segments(x0=x[1], y0=y2[1], x1=x[1] + 10, y1=y2[1], col="gray")
% segments(x0=x[1], y0=y2[1], x1=x[1] + .1, y1=y2[1])
% points(x[1:3], y2[1:3], pch=21, bg=c("yellow", "red", "red"), col="black")
% mtext(c("0", "T1"), 1, line=.2, cex=.7, at=c(-.15, x[length(x)]))
% mtext("T2", 2, line=.5, cex=.7, at=.7, las=2)
% text(x, y2, c("A", "B", "C", ""), pos=3, cex=.7)
% text(x, y2, labels=c("", rep("1/3 + ?", 2), ""), cex=.5, pos=4)
% dev.off()
%
% ####################### bivariate case with strips:
% ####################### bivariate case with strips:
% pdf("bivariateCaseStrips.pdf", width=7, height=3)
% par(mfrow=c(1, 2), mar=c(3,4,2,2))
% x = c(.6, .8, 1.2, 1.5)
% y2 = c(0.5, .35, .6, .5)
% stripY = c(.33, .55, .77)
%
% plot(c(0, max(x)), c(.2, .8), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% box(col="gray")
% abline(h=stripY, col="gray", lty=3)
% segments(x0=x[1], y0=y2[1], x1=x[1] + 10, y1=y2[1], col="gray")
% segments(x0=x[1], y0=y2[1], x1=x[1] + .1, y1=y2[1])
% points(x[1:3], y2[1:3], pch=21, bg=c("yellow", "red", "red"), col="black")
% mtext(c("0", "T1"), 1, line=.2, cex=.7, at=c(-.15, x[length(x)]))
% mtext("T2", 2, line=.5, cex=.7, at=.8, las=2)
% text(x, y2, c("A", "B", "C", ""), pos=3, cex=.7)
% text(x, y2, labels=c("", "1/3 + 1/3", "1/3", ""), cex=.5, pos=4)
%
% plot(c(0, max(x)), c(.2, .8), type="n", xlab="", ylab="", cex=.5, axes = FALSE)
% box(col="gray")
% abline(h=stripY, col="gray", lty=3)
% segments(x0=x[1], y0=y2[1], x1=x[1] + 10, y1=y2[1], col="gray")
% segments(x0=x[1], y0=y2[1], x1=x[1] + .1, y1=y2[1])
% points(x[2], y2[1], pch=21, bg=c("red"), col="black")
% points(x[1:3], y2[1:3], pch=21, bg=c("yellow", "red", "red"), col="black")
% mtext(c("0", "T1"), 1, line=.2, cex=.7, at=c(-.15, x[length(x)]))
% mtext("T2", 2, line=.5, cex=.7, at=.8, las=2)
% text(x, y2, c("A", "B", "C", ""), pos=3, cex=.7)
% text(x, y2, labels=c("", rep("1/3", 2), ""), cex=.5, pos=4)
% text(x[c(2,2)], y2[c(1,1)], labels=c("A1", "1/3"), cex=c(.7, .5), pos=c(3,4))
% dev.off()
%   \end{verbatim}


\end{document}          

