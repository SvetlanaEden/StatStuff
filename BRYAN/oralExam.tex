\documentclass[]{article}

%\usepackage[final]{pdfpages}
\usepackage{anysize}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{longtable}

%\usepackage[usenames, dvipsnames]{color}
\usepackage{xcolor,colortbl}

\DeclareGraphicsExtensions{.png,.pdf}

% %%% for citations. See Lucy's tutorial: https://github.com/LucyMcGowan/Tutorials/blob/master/BiblatexTutorial.md
% % \usepackage[backend=biber, maxnames=10, citestyle=science]{biblatex}
% \usepackage[style=authoryear, backend= biber]{biblatex}
% \addbibresource{summaryOfRef.bib}

% %%% for citations. Jonathan's way:
\usepackage{natbib}
\bibliographystyle{unsrtnat}



%%% when using biber as a "backend" compile like this:
% pdflatex myFile
% biber myFile
% pdflatex myFile


\let\epsilon\varepsilon
\newcommand\SLASH{\char`\\}
\newcommand{\csch}{\text{csch}}
\marginsize{0.5in}{1in}{0.5in}{1in}
\setlength\parindent{0pt}

% Title Page: Modify as needed
\title{Association of bivariate survival data}
\author{Svetlana Eden}
\date{Month Day, 2015}

\hypersetup{hidelinks}

% \usepackage[pdftex,bookmarks,pagebackref,pdfpagemode=UseOutlines,
%      colorlinks,linkcolor=\linkcol,
%      pdfauthor={Svetlana K Eden},
%      pdftitle={\titl}]{hyperref}


\begin{document}
\maketitle
\tableofcontents
\listoffigures
\listoftables
\clearpage

\section{INTRODUCTION}
The interest in survival data can be traced back to at least 1846, when a Hungarian doctor of the Vienna General Hospital maternity clinic, Ignaz Semmelweis, recorded the rate of death from childbed fever. After excluding several other hypotheses, he suggested that when a child birth is helped by a doctor or a medical student, the women's rate of death from childbed fever was higher because of a disease that was caused by cadaverous particles that medical students and doctors had on their hands after dissecting cadavers. As you might know, his theory was dismissed, and he died alienated from the medical community. The rational for his hypothesis was discovered decades later, when the germ theory of the disease was developed. 


Another example was Florence Nightingale, who was born into a rich upper-class British family, and in spite of her status she pursued a career of a nurse. She was credited for significantly reducing hospital mortality of British soldiers during the Crimean War, in 1854 (with hand-washing being one of the most important practices implemented under her supervision). She also published some of her works in simple English in order to popularize medical knowledge.\\

In these two examples, survival data was collected as a binary variable: died or lived. For studies with longer follow-up, a better way of collecting survival data is to record time to the event. The problem with collecting time to event though, is that not all events can be observed. For example, suppose a five-year longitudinal study aims to test an association of blood pressure and time to cardiovascular event. Not all patients will necessarily have the event during these five years. Such patients would be labeled as \emph{censored}.  \cite{fisher1931truncated},  \cite{hald1949maximum}, and \cite{swan1969computing} were probably one of the first to suggest a more complex view of survival data by introducing a concept of \emph{censoring} and suggesting methods of dealing with it. Censoring is \emph{non-informative} if it is not associated with the potential event time. Otherwise it is called \emph{informative}. Assumption of non-informative censoring somewhat simplifies estimation of survival probability, but still requires complex theoretical tools. It was \cite{kaplan1958nonparametric} who suggested an estimate of survival probability for a univariate case and in the presence of censoring. They introduced a well known Product-Limit estimator of the survival curve. \cite{david1972regression} extended their work by introducing a regression model for censored survival outcome. His model utilized a concept of a \emph{hazard rate}, and we define it here in order to understand better the rest of the paper. \emph{Hazard rate} is defined as follows:

$$
\begin{aligned}
	\lambda(t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T\in[t,t+h] | T\geq t)=
	%&=  \frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t)  \right\}= \\&=
	             \frac{f(t)}{\mathcal{S}(t)}
\end{aligned}
$$

Where $T$ is a random variable of time to event; $F(t)$, $f(t)$, $S(t)$ are cumulative distribution function, density function, and \emph{survival function} of random variable $T$; and $\mathcal{S}(t) = 1-F(t)$. $P(T\in[t,t+h] | T\geq t)$ is a \emph{conditional probability}, the probability that the event occurs in interval $[t,t+h]$ given that the subject survived without the event at least until time $t$. In other words, hazard rate can be intuitively thought of as a probability of the event per unit of time.\\
Although the notion of hazard rate is intuitively simple, its estimation required complex theoretical tools even in a univariate case. Adding another dimension not only increases the complexity, but also makes it more difficult to apply intuitive thinking. Pursuing a purely theoretical question in the context of \emph{bivariate} survival function, \cite{basu1971bivariate} %[get this paper from the library ??? this does not add up because Cox wrote his paper later]
defined a \emph{double failure hazard rate} as:
$$
\begin{aligned}
	\lambda(t_1,t_2)&= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[t_1,t_1+h_1], T_2\in[t_2,t_2+h_2] | T_1\geq t_1, T_2 \geq t)=
	%&= \frac{\partial}{\partial t_1}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t_1,t_2)  \right\} - \frac{\partial^2}{\partial t_1\partial t_2}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\\
	 \frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}
\end{aligned}
$$
The intuition of the univariate hazard rate helps us understand the above expression because of the similarity between $\frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}$ and$\frac{f(t)}{\mathcal{S}(t)}$. We included this definition because this and other similar concepts are present in the rest of this paper.\\

Although the work of \cite{basu1971bivariate} was not targeted toward specific applications, there are many practical problems where bivariate survival analysis could find its use, for example, in studies of age of appendicitis in twins, or of time to cardiovascular disease among subjects with familial ties. In this paper, we are concerned with the question of association between paired survival data. Although the literature on this subject is vast, we review only three main approaches. First, this question was addressed by introducing a \emph{bivariate hazard ratio} in the context of specific family of distributions. The second approach was to find a non-parametric estimate of the bivariate survival function with three components: two marginal survival functions and a term that represented an association between two survival variables. The third approach was to develop semi-parametric and non-parametric tests to examine association of residuals. We especially focus on the non-parametric test of \emph{probability scale residuals} introduced and thorouphly researched by \cite{li2012new}, \cite{lui2015covariate}, and \cite{shepherd2016probability}.\\
The main goal of the paper was to show the complexity of theoretical tools required to address the question of bivariate survival in the presence of censoring and to highlight the generalizability of \emph{probability scale residuals} approach and simplicity of its use. In conclusion, we identify the direction of our future research with a goal, to apply \emph{probability scale residuals} for a wide range of different statistical scenarios in the context of bivariate survival with non-informative censoring.

\section{LITERATURE REVIEW}

\subsection{Modeling a Bivariate Hazard Ratio}

Suppose we study an association of time to cardiovascular disease, $T$ and parental history of cardiovascular disease. One way to approach this problem would be to use Cox proportional hazard regression with parental history of cardiovascular disease as the main covariate, $x$, recorded as \emph{1-yes} and \emph{0-No}:

$$
\begin{aligned}
	\lambda(t|x) &= \lambda_0(t) \cdot e^{\beta\cdot x}
\end{aligned}
$$

Note that for this model, the \emph{hazard ratio} or having a cardiovascular event given parental history is simply:

$$
\begin{aligned}
 \theta = \frac{\lambda(t|x=1)}{\lambda(t|x=0)} = e^{\beta}
 \end{aligned}
$$
 
The problem with this approach is that a parent may not live long enough to experience time to cardiovascular disease. This means that not only our outcome, time to cardiovascular disease, but also our covariate, parental history may be censored. This is what motivated \cite{clayton1978model} to think of a different approach for epidemiological studies of chronic disease with familial tendency. Following the intuition of the hazard ratio in the context of Cox regression, he defined a measure similar to hazard ratio, that had the following properties:
\begin{enumerate}
	\item Easy to compute
	\item Expressible as a constant ratio of age-specific rates
  \item Symmetrical in two variables
\end{enumerate}
Let's denote time to event for fathers to be $t$ and time to event for sons to be $s$. Also, let's denote $\lambda_s(s_0|t=t_0) = \frac{f(s_0|t=t_0)}{S(s_0|t=t_0)}$ and $\lambda_s(s_0|t>t_0) = \frac{f(s_0|t>t_0)}{S(s_0|t>t_0)}$. Then Clayton's \emph{cross ratio}, $\theta$, is defined in the following way:
$$
\begin{aligned}
	\theta = \frac{\lambda_s(s_0|t=t_0)}{\lambda_s(s_0|t>t_0)} = \frac{\lambda_t(t_0|s=s_0)}{\lambda_t(t_0|s>s_0)}
\end{aligned}
$$
We clearly see a similarity between the hazard ratio of Cox model and the \emph{cross ratio} defined by Clayton. The requirement of symmetry was motivated by the fact than neither son nor father were causes of the event, rather, there was an unobservable variable that was associated with the event for both of them.\\

This measure can be interpreted for sons (as much as for fathers) in the following way: how much more likely for a son to have an event at time $s_0$ given that his father had an event at time $t_0$ compared to a son whose father survived until time $t_0$ without the event. Independence between fathers and sons would mean $\theta = 1$, positive association $\theta > 1$ and negative association $\theta < 1$.
 %Clayton also explained how his model could be generalized to include covariates, and derived the likelihood to estimate $\theta$ when two marginal distributions are completely unknown using approach similar to one used by Cox.\\
Clayton's assumptions lead to a specific form of survival function (see \cite{oakes1989bivariate}):
$$
\begin{aligned}
S(t_1,t_2) = \left[\left\{ \frac{1}{S_1(t_1)} \right\}^{\theta - 1} + \left\{ \frac{1}{S_2(t_2)} \right\}^{\theta - 1}   - 1 \right]^{-\frac{1}{\theta-1}}
\end{aligned}
$$
where $S_1(t_1)$ and $S_2(t_2)$ are marginal survival functions. Although this bivariate function has a very specific form, Clayton's non-parametric likelihood approach allowed to estimate $\theta$ without knowing specific forms of $S_1(t_1)$ and $S_2(t_2)$.\\
~\\
\cite{oakes1982model} studied Clayton's model extensively and showed that: $\frac{\theta - 1}{\theta + 1} = \tau$, where $\tau$ is a coefficient of concordance introduced by  \cite{kendall1938new}: for two independent pairs of variables with the same bivariate distribution $(U_1, Z_1)$ and $(U_2, Z_2)$, Kendal's $\tau$ is the probability of concordance minus the probability of discordance:
$\tau = P[(U_1 - U_2)(Z_1 - Z_2)>0] - P[(U_1 - U_2)(Z_1 - Z_2)<0] = E[sign((U_1 - U_2)(Z_1 - Z_2))]$. This fact suggested that $\theta$ can be estimated through $\tau$ as well.\\

Later \cite{oakes1989bivariate} introduced a bivariate survival model based on \emph{frailties}, unobservable random variables. If we denote the variable as $W$, this means that the probability of survival is conditional on it: $Pr(T>t|W=w)$. This is a way to model random effect in survival analysis. In the context of bivariate survival analysis, if we assume that both survival times depend on the same unobservable random variable, $W$:
$$
\begin{aligned}
	Pr(T_1>t_1|W=w)=\left\{ B_1(t_1) \right\}^w\\
	Pr(T_2>t_2|W=w)=\left\{ B_2(t_2) \right\}^w\\
\end{aligned}
$$
This induced dependence between the variables, $T_1$ and $T_2$ themselves:
$$
\begin{aligned}
	S(t_1,t_2) = \int \left\{ B_1(t_1) B_2(t_2)\right\}^w dF(w)
\end{aligned}
$$

He showed that bivariate distributions introduced by frailty models were part of a larger class, the of \emph{archimedian} distributions studied by \cite{genest1986copules}:
$$
\begin{aligned}
	S(t) = p\left[ q\left\{ S_1(t_1) \right\}  + q\left\{ S_2(t_2) \right\}  \right]
\end{aligned}
$$

Where $p(u)$ is a non-negative decreasing function with non-negative second derivative, with $p(0)=1$, and $p(u)$ is an inverse function of $q(v)$. It can be shown (see Appendix A) that for Clayton's $\theta$ the following equality holds:
	$$
	\begin{aligned}
		\theta = \frac{\lambda_{t_1}(t_1|T_2=t_2)}{\lambda_{t_1}(t_1|T>t_2)} = \frac{ \frac{\partial^2 S(t_1,t_2)}{\partial t_1 \partial t_2} S(t_1, t_2)}    {\frac{\partial S(t_1, t_2)}{\partial t_2} \frac{\partial S(t_1, t_2)}{\partial t_1}}
	\end{aligned}
	$$
Using this equality, Oakes showed that for \emph{archimedian} class of models, $\theta$ depends on $t$ only through $S(t)$, which allowed him to show the following:
$$
\begin{aligned}
\theta(v) = -v\cdot q''(v)/q'(v)
\end{aligned}
$$
where $v=S(t_1, t_2)$. He also pointed out that because $\theta$ and \emph{Kendall}'s $\tau$ were related through the following equality: $\tau(v) = \frac{\theta(v)-1}{\theta(v)+1}$, both $\theta(v)$ and $\tau(v)$ can be considered as a measure of local dependence because it depends on $v=S(t_1, t_2)$. By showing that Clayton's model is a subset of the \emph{archimedian} class, Oakes extended the set of models for which $\theta(v)$ and $\tau(v)$ can be studied.\\
~\\
As a side note, the \emph{archimedian} class of survival function used by Oakes is a subclass of functions called \emph{copulas}. According to \cite{nelsen2007introduction}, a two-dimensional \emph{copula} is a function with support on $[0, 1] \times [0, 1]$ and values in $[0, 1]$. We omit the formal defintion of a \emph{copula} and only mention its important quality summarized in \emph{Sklar's theorem}: if $H(x,y)$ is a joint distribution function with margins $F(x)$ and $G(y)$, there exists a copula $C$ such that for all $x,y$: $H(x,y) = C(F(x), G(y))$, where, if $F$ and $G$ are continuous then $C$ is unique. Nelson also defines a \emph{survival copula} that holds the same property: $\bar{H}(x,y) = C_S(\bar{F}(x), \bar{G}(y))$, where $\bar{H}(x,y)$, $\bar{F}(x)$, and $\bar{G}(y)$ are joint and marginal survival functions respectively. The implication of this property is that in order to estimate a survival copula, the marginal distributions and their association parameter can be estimated separately for any type of survival copula. 
%Without going into the details, we mention a few papers that used copula approach, . 
In addition to this important property, the theory of copulas for continuous data allows to express two popular association measures, \emph{Kendal's} $\tau$ and \emph{Spearman} correlation, in terms of copulas. Without summarizing the entire literature, we mention only few papers where authors used the theory of copulas for inference in bivariate survival: \cite{shih1996tests}, \cite{shih1995inferences}, and \cite{zhang2008inference}.\\
~\\
Now we go back to the \emph{cross ratio} $\theta$, defined by Clayton and later developed by Oakes. In survival studies, the age of participants is often restricted by inclusion criteria. For example, the age of mothers could be restricted to 50-80, and of daughters to 30-60. This created a need for an association measure that would take this restriction into account and would be well suited for censored time to event. This argument lead \cite{fan2000dependence} to consider a weighted average of Clayton's \textit{cross ratio} $\theta = \frac{ \lambda_2(t_2|T_1=t_1)}{\lambda_2(t_2|T_1 \geq t_1)}$ over $[0,t_1] \times [0, t_2]$. As a reminder, $\theta$ can be interpreted as a risk for daughters of age $t_2$ whose mothers developed a disease at age $t_1$, compared to daughters of age $t_2$ whose mothers were disease free at age $t_1$. The authors chose this measure because of its interpretability, and suggested that in order to increase comparability across studies, the weighting should be independent of censoring mechanism. The following weights were proposed: $\frac{F(dt_1,dt_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)}$. For one dimension, this weight could be interpreted as a probability of an event at point $t$ given that the subject survived until $t$.\\

%They allowed the time to be discrete, continuous, or mixed, but $F(., .)$ was required to possess a density relative to the corresponding counting Lebesque measure.\\
Instead of weighting $\theta(t_1, t_2)$, the authors chose to weight $c(t_1, t_2) = \frac{1}{\theta(t_1, t_2)}$ because weighting $c(t_1, t_2)$ results in a consistent estimator:
	$$
	\begin{aligned}
		C(t_1, t_2) &= \int_0^{t_1}\int_0^{t_2} \frac{c(s_1, s_2)F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)} = ...= \int_0^{t_1}\int_0^{t_2} \frac{F(s_1^-, s_2^-)\Lambda_{10}(ds_1, s^-_2)\Lambda_{01}(s^-_1, ds_2)}{1 - F(t_1, 0) - F(0, t_2)+ F(t_1, t_2)} \\
	\end{aligned}
	$$
	The above measure can be estimated non-parametrically using $\hat{F}$ of \cite{dabrowska1988kaplan} or \cite{prentice1992covariance}, and $\hat{\Lambda}$ can be estimated using a Nelson-Aalen type estimator.
	%??? Nelson-Aalen
Note that $C(t_1, t_2)$ may be interpreted as an average relative risk over $[0,t_1] \times [0, t_2]$. When $C(t_1, t_2)=1$, there is no association. When $C(t_1, t_2)<1$, there is positive association. It can also be interpreted as a weighted average hazard ratio with weights proportional to the failure time density at $(t_1,t_2)$.\\
Because \cite{oakes1989bivariate} noted that \emph{Kendal's} $\tau(t_1, t_2)$ can be written as:
	$$
	\begin{aligned}
		\tau(t_1, t_2) &= \frac{1 - c(t_1, t_2)}{1+c(t_1, t_2)}
	\end{aligned}
	$$
	The authors also proposed to weight conditional version of \emph{Kendal's} $\tau=\tau(t_1, t_2)$ proposed by \cite{oakes1989bivariate}:
	$$
	\begin{aligned}
		&\tau(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} = t_1,~T_{21}\wedge T_{22} = t_2  \}
		&~~~~where~\wedge~is~minimum
	\end{aligned}
	$$
The above measure can be viewed as an association between paired subjects who reached certain age. After weighting $\tau(t_1, t_2)$ proportionally to the density, they obtained a weighted conditional \emph{Kendal's} $\tau(t_1, t_2)$:
	$$
	\begin{aligned}
		&\mathcal{T}(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} \leq t_1,~T_{21}\wedge T_{22} \leq t_2  \}
	\end{aligned}
	$$
This measure can be especially useful in studies with long follow-up. For example, the authors estimated that the association of time to appendectomy between monozygotic female twins measured using $C(t_1, t_2)$ and $\mathcal{T}(t_1, t_2)$ was stronger at younger ages and weaker at older ages.\\

Measures $C(t_1, t_2)$ and $\mathcal{T}(t_1, t_2)$ were further considered by
\cite{fan2000class} who suggested a class of estimators similar to ones described by \cite{fan2000dependence} . They considered special cases of $\theta(t_1, t_2)$ and weights, where instead of using the bivariate survival function estimators of \cite{dabrowska1988kaplan} and \cite{prentice1992covariance} the hazard function estimators of Nelson-Aalen type could be used. And instead of bootstrap variance estimator, an explicit variance formulae can be obtained.\\
% The authors utilize the fact that in some specific cases, both $C(\cdot, \cdot)$ and $\mathcal{T}(\cdot, \cdot)$ depend on $F(\cdot, \cdot)$ only through the hazard function and therefore can be estimated using Nelson-Aalan-type estimator of the hazard:
% 	$$
% 	\begin{aligned}
%     C(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2)}{\int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2)}\\
%     \mathcal{T}(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) - \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) + \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }\\
% 	\end{aligned}
%  $$

This completes our overview of papers that studied association of bivariate survival through modeling of a \emph{cross ratio}. We proceed to a different but perhaps more complex way of evaluating association of bivariate survival data by estimating a \emph{bivariate survival surface}.

\subsection{Estimating Bivariate Survival Surface}

In univariate survival analysis, Kaplan-Meier curve is a popular visual summary and a reliable analytic tool. For bivariate survival, several attempts were made to find something similar. The main difficulty however according to \cite{kalbfleisch2011statistical} is that in the bivariate case, there is a problem of identifiability of bivariate survival surface. For example, a nonparametric maximum likelihood for a survival function, $S(t_1, t_2) = P(T_1>t_1, T_2>t_2)$ in the presence censoring has a uniqueness problem and also, cumulative hazard function $\Lambda(t_1, t_2) = \int_0^{t_1}\int_0^{t_2}\Lambda(du_1,du_2)$ is not uniquely defined by $\Lambda(du_1,du_2)$.  \cite{dabrowska1988kaplan} found a way around this problem. By introducing marginal cumulative hazards in addition to $\Lambda(du_1,du_2)$, she was able to derive a consistent non-parametric estimator of the bivariate survival surface. Let's introduce the following notations. We let the density of time to event be $f(s,t)$ and let $S(s,t) = P(T_1>s, T_2>t)$ be the corresponding joint survival function. Bivariate cumulative hazard functions are defined by Dabrowska in the following way:
% 	$$
% 	\begin{aligned}
% 		\lambda_{11}(t_1,t_2) &= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[t_1,t_1+h_1], T_2\in[t_2,t_2+h_2] | T_1\geq t_1, T_2 \geq t_2) & = \frac{f(t_1,t_2)}{S(t_1-,t_2-)}\\
% 		\lambda_{10}(t_1,t_2) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_1\in[t_1,t_1+h] | T_1\geq t_1, T_2 >t_2) & = \int_{t_2}^{\infty} \frac{f(t_1,v)dv}{S(t_1-,t_2)}\\
% 		\lambda_{01}(t_1,t_2) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_2\in[t_2,t_2+h] | T_1 > t_1, T_2 \geq t_2) & = \int_{t_1}^{\infty} \frac{f(u,t_2)du}{S(t_1,t_2-)}\\
% 	\end{aligned}
% 	$$
% Where $\lambda_{11}(s,t)$ is the instantaneous rate of \emph{double failure} at point $(s,t)$, given that the individuals were alive at times $T_1=s-$ and $T_2 = t-$; $\lambda_{10}(s, t)$ is the rate of a \emph{single failure} at time $s$ given that the first individual was alive at time $T_1=s$ and the second survived beyond time $T_2 = t_1$.\\
	$$
	\begin{aligned}
		%\Lambda(t_1,t_2) &= (\Lambda_{10}(t_1,t_2), \Lambda_{01}(t_1,t_2), \Lambda_{11}(t_1,t_2))\\
		%&where\\
		\Lambda_{11}(dt_1,dt_2) &= \frac{P(T_1 \in dt_1, T_2\in dt_2)}{P(T_1 \geq t_1, T_2 \geq t_2)} = \frac{S(dt_1, dt_2)}{S(t_1^-, t_2^-)}\\
		\Lambda_{10}(dt_1,t_2) &= \frac{P(T_1 \in dt_1, T_2 > t_2)}{P(T_1 \geq dt_1, T_2 > t_2)} = \frac{-S(dt_1, t_2)}{S(t_1^-, t_2)}\\ %~~~t_1ee ~t_2he~paper~where ~it_2~it_1~S(t_1-, t_2-),~probably~a~t_2ypo\\
		\Lambda_{01}(t_1,dt_2) &= \frac{P(T_1 > t_1, T_2\in dt_2)}{P(T_1 > t_1, T_2 \geq t_2)} = \frac{-S(t_1, dt_2)}{S(t_1, t_2^-)}\\
		% &and\\
		% \Lambda_{10}(0,t_2) &= \Lambda_{01}(t_1,0) = \Lambda_{11}(0,0) = 0\\
	\end{aligned}
	$$
% If , we have $\Lambda_{11}(ds,dt) = \lambda_{11}(s,t)ds~dt$, $\Lambda_{10}(ds,t) = \lambda_{10}(s,t)dt$, $\Lambda_{01}(s,dt) = \lambda_{01}(s,t)dt$, so


Under condition of independence of event times and censoring times, the author derived a bivariate survival function estimator that could be computed as a product of two univariate survival function estimators and a third term:
	$$
	\begin{aligned}
		{S}(t_1,t_2) &= {S}(t_1,0){S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}\\
	\end{aligned}
	$$
Where $S(s,0)$ and $S(0,t)$ are marginal survival functions and $L$ is the following:
	$$
	\begin{aligned}
		L(du, dv) &= \frac{{\Lambda}_{10}(d u,v^-){\Lambda}_{01}(u^-,d v) - {\Lambda}_{11}(du, dv)}{\left(1-{\Lambda}_{10}(d u,v^-)\right)\left(1-{\Lambda}_{01}(u^-,\Delta v)\right)}
	\end{aligned}
	$$
From the estimator's definition, ${S}(t_1,t_2) = {S}(t_1,0){S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}$ it is clear that term $\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\}$ can be written as:
	$$
	\begin{aligned}
		\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - {L}(d u, d v)\} &=
		\frac{S(0,0)  S(t_1,t_2)}{ S(t_1,0)S(0,t_2) }
	\end{aligned}
	$$
The above expression looks very much like:
	$$
	\begin{aligned}
		\theta &= \frac{ \frac{\partial^2 S(t_1,t_2)}{\partial t_1 \partial t_2} S(t_1, t_2)}    {\frac{\partial S(t_1, t_2)}{\partial t_2} \frac{\partial S(t_1, t_2)}{\partial t_1}},
	\end{aligned}
	$$
which is one of the ways to define the \emph{cross ratio} of \cite{clayton1978model}. This helps us interpret Dabrowska's association term the same way as Clayton's \emph{cross ratio}, but instead of being constant, it varies over $[0, t_1]\times [0, t_2]$.\\

Dabrowska's estimator of the bivariate survival surface is computed in the following way:
	$$
	\begin{aligned}
		\hat{S}(t_1,t_2) &= \hat{S}(t_1,0)\hat{S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
	\end{aligned}
	$$
Where $\hat{S}(s,0)$ and $\hat{S}(0,t)$ are usual Kaplan-Meier estimates, for example, $\hat{S}(s,0) = \prod_{u\leq t_1}[1-\hat{\Lambda}_{10}(\Delta u, 0)]$, and the third term is estimating in the following way:
	$$
	\begin{aligned}
    \hat{L}(\Delta u, \Delta v) &= \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}(\Delta t_1, \Delta t_2) = \#(T_1=t_1, T_2=t_2, \delta_1=\delta_2=1)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{10}(\Delta t_1, t_2^-) = \#(T_1=t_1, \delta_1=1, T_2\geq t_2)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) = \#(T_2=t_2, \delta_2=1, T_1\geq t_1)/\#(T_1\geq t_1, T_2\geq t_2)
	\end{aligned}
	$$

This estimator is easy to compute, but as we see later, it assigns negative mass to certain points (see \ref{appendixB}). The authors proved an example when negative mass occurs and she also proved that in the absence of censoring, this problem does not arise.\\

\cite{pruitt1991negative} investigated conditions under which Dabrowska's estimator assigns negative mass and showed that although the value of negative mass decreased, the number of such points did not disappear with growing sample size, resulting in non-disappearing negative mass. This problem was related to the problem of identifiability of the bivariate survival functions in the presence of censored data. The proof was rather complicated, but we can intuitively see why this can be the case. Let's rewrite again Dabrowska's estimator:

	$$
	\begin{aligned}
		\hat{S}(s,t) &= \hat{S}(s,0)\hat{S}(0,t)\cdot \prod_{{0<u\leq s~0<v\leq t}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
    1 - \hat{L}(\Delta u, \Delta v) &= 1 - \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}_{11}(\Delta t_1, \Delta t_2) = \#(T_1=t_1, T_2=t_2, \delta_1=\delta_2=1)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{10}(\Delta t_1, t_2^-) = \#(T_1=t_1, \delta_1=1, T_2\geq t_2)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) = \#(T_2=t_2, \delta_2=1, T_1\geq t_1)/\#(T_1\geq t_1, T_2\geq t_2)
	\end{aligned}
	$$
It is possible that $\hat{S}(s,t\neq 0) > \hat{S}(s,0)$ when expression $1 - \hat{L}(\Delta u, \Delta v)$ is greater than one, which in turn may happen when  $\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) \cdot \hat{\Lambda}_{10}(\Delta t_1, t_2^-)< \hat{\Lambda}_{11}(\Delta t_1, \Delta t_2)$. In the presence of censoring, $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, $\hat{\Lambda}_{11}$ are somewhat \emph{unsynchronized}, in the sense that the number at risk for single failures is the same as the number at risk for double failures in spite of the fact that marginally the number at risk can be larger. Also, because of the censoring, each term $1 - \hat{L}(\Delta u, \Delta v)$ is slightly contributing to the overall larger value that eventually can be bigger than one, which results in the fact that the bivariate survival function is not monotone.\\
Simple simulations showed (not presented here) that term $1 - \hat{L}(\Delta u, \Delta v)$ is greater than $2$ when $\hat{\Lambda}_{01}$ is very large and $\hat{\Lambda}_{10}$ is very small (or the other way around), and when $\hat{\Lambda}_{11}$ is very large. Even for $\hat{\Lambda}_{01} = \hat{\Lambda}_{10} = \hat{\Lambda}_{11} = 0.5$, term $1 - \hat{L}(\Delta u, \Delta v) = 2$.  Dabrowska's example presented in \ref{appendixB} can also illustrates that when all observations are not censored, the values of $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, and $\hat{\Lambda}_{11}$ balance out and the negative mass disappears.\\

\cite{prentice1992covariance} introduced an estimator similar to Dabrowska's. It had similar performance, and the same negative mass problem. In fact, \cite{van1997nonparametric} sited the work of \cite{gill1990survey} who proved that the estimators of Dabrowska and Prentice and Cai were equivalent and performed similarly well under the assumption of complete independence of events and censoring. Van Der Laan further discussed difficulties of evaluating bivariate survival using non-parametric likelihood (NPMLE). He cited the work of \cite{tsai1986nonparametric}, who give an example showing that NPMLE for continuous bivariate survival case, the NPMLE is not consistent. Without formally proving this, Van Der Laan gave an intuitive explanation of why this is true using the mechanics of expectation-maximization (EM) algorithm applied to discrete case. When solving solving NPMLE for discrete time to events, the EM algorithm assigned equal mass to uncensored observations. The mass of doubly censored observations is redistributed to the uncensored observations that are located in their quadrant. Singly censored observations redistribute their mass onto to uncensored observations in their \emph{path}. When time to event is continuous, the probability that an uncensored observation lies in their path is zero. This means that the EM algorithm would not know how to redistribute the mass.\\

Van Der Laan suggested a repaired NPMLE, where instead of the half-lines there were strips that were wide enough to contain the uncensored observations, and suggested a way of redistributing the mass for singly censored observations based on the uncensored observations that fell into the strips or quadrants.
After comparing several estimator, the author came to conclusion that with no or small dependence, the Dabrowska's and Prentice and Cai's estimators are better. With increased dependence however, the author's estimator performed better.\\

\cite{moodie2005adjustment} pointed out that the method of Van Der Laan could be inefficient when the strips were large because the uncensored points could borrow the mass from singly censored on one coordinate observations that have very different event time for the other coordinate. They suggested an improvement. They first applied Van Der Laan's method (including dividing the plane into strips containing uncensored and censored observations), and then redistributed the mass from uncensored observations on to points that were located on the half lines of the singly censored observations from the same strip. They reported that this method improves efficiency and reduced computational complexity when computing the variance.

% \begin{figure}[!h]
% \includegraphics{MoodiePrentice2005.png}
% \caption{Moodie and Prentice suggestion.}
% \label{fig:bubbles}
% \end{figure}
% \clearpage

\subsection{Linear Rank Tests}
Before summarizing the literature for linear rank tests, we give a quick introduction of Probability Scale Residuals.

\subsubsection{Probability Scale Residuals for Censored Data}

\cite{li2012new} proposed a new method of evaluating residuals for ordinal data. They defined Probability Scale Residuals (PSR) as:\\
$$
\begin{aligned}
	r(t,F^*) &= E\{sign(y,Y^*)\} = pr(Y^* < y) - pr(Y^* > y) = F^*(y-) - (1-F^*(y)) =\\
	 &=F^*(y-) - 1 + F^*(y)
	\end{aligned}
$$

The authors also extended this definition for two cases: censored time to event and current status data. In order to understand how the PSR were extended for censored data, let's assume that $T$ is time to event, but we only observe $Y = min(T, C)$, where $C$ is time to censoring. When we do not observe $T$, the PSR are defined in terms of $Y$ (time to event or censoring) and $\Delta$, where $\Delta = 1$ when $min(T, C)=T$ (we observe the event) and $\Delta =0$ when $min(T, C)=C$ (the event is not observed because of censoring). \textbf{By definition}, if $\Delta = 1$, then $Y=T$ and $r(y,F^*, \delta=1) = F^*(y-) - 1 + F^*(y)$. If $\Delta = 0$, then $T$ is unknown, except that it occurs some time after the censoring time $C$, so \textbf{by definition} $r(t,F^*, \delta=0) = E\{r(t,F^*)|T^*>y\}$. The authors prove that:

	$$
	\begin{aligned}
		r(y, F^*) &= F^*(y) + F^*(y-) - 1,~~~&\delta = 1 \\
		r(y, F^*) &= E\{r(T,F^*)|T^*>y\} = F^*(y) ,~~~&\delta = 0 \\
		&or\\
    r(y, F^*, \delta) &= F^*(y) - \delta(1 - F^*(y-)),~~~&where~\delta \in \{0,1\}
	\end{aligned}
	$$

In a similar manner, PSR are defined for current status data. Current status data is a pair $(C, \Delta)$, where variable $C$ is the time when we observe the subject, and variable $\Delta$ is whether the subject has already experienced the event or not.
	$$
	\begin{aligned}
		r(c, F^*) &= E\{r(T,F^*)|T^*\leq c\} = F^*(c) - 1,~~~&\delta = 1 \\
		r(c, F^*) &= E\{r(T,F^*)|T^*>c\} = F^*(c) ,~~~&\delta = 0 \\
		&or\\
    r(c, F^*, \delta) &= F^*(c) - \delta,~~~&where~\delta \in \{0,1\}\\
	\end{aligned}
	$$

In both cases the PSR have expectation of zero when $F^*$ is properly defined and $T \perp C$.\\

The authors showed how PSR were related to martingale, Cox-Snell, and deviance residuals and that PSR also can be used to examine the functional adequacy of the model, with positive PSR indicating that the event time was longer than expected. In addition to examining the functional adequacy of the model, PSR can be used to measure and test association of two censored times to event. One of the intuitive ways to find the association between two time-to-event outcomes would be to find their correlation. We now show that correlation of PSR is equivalent to the censored version of \emph{Spearman} correlation.

\subsubsection{Spearman Correlation and Correlation of PSR for Continuous Censored Outcome.}
In this following section we use $S$, $C$, $\delta=I(S\leq C)$, $X = min(S, C)$ for the first outcome and $T$, $D$, $\epsilon=I(T\leq D)$, $Y = min(T, D)$ for the second.\\
Spearman correlation for continuous outcome can be defined as $cor(F_S(S), F_T(T))$. In case of censored outcomes however, we don't always observe $S$ and $T$. This means that we have to modify the definition of Spearman correlation in the following way: When $\delta = 1$ or $S\leq C$, we have that $X = min(S, C)=S$ so $F_S(s)=F_S(x)$ and we use $F_S(x)$ in order to compute correlation. When $\delta = 0$ or $S > C$, we have that $X = min(S, C)=C$ and instead of $F_S(s)$, in order to compute correlation, we can take $E_S[F_S(s)|S>x]$. Let's first compute $E_S[F_S(s)|S>x]$:
% keeping in mind that in continuous case $F_S(s-) = F_S(s)$
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)}\int_{(x, \infty)} F_S(s)dF_S(s) = \frac{1}{1-F_S(x)} \left.\frac{ [F_S(s)]^2}{2}\right|_{(x, \infty)} = \frac{1-[F_S(x-)]^2}{2(1-F_S(x))} \\
	\end{aligned}
	$$
Since we are focusing now on the continuous case, we can assume that $F_S(x)=F_S(x-)$ therefore we have
	$$
	\begin{aligned}
		E_S[F_S(x)|S>X] &=\frac{1-[F_S(x)]^2}{2(1-F_S(x))}=\frac{\cancel{(1-F_S(x))}(1+F_S(x))}{2\cancel{(1-F_S(x))}}  = \frac{1+F_S(x)}{2} \\
	\end{aligned}
	$$
Now, we redefine the Spearman correlation for the continuous censored data as:
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta F_S(x) + (1-\delta) \frac{1+F_S(x)}{2},~\epsilon F_T(y) + (1-\epsilon) \frac{1+F_T(y)}{2}  \right)\\
		&=~ cor\left(  \frac{1+\delta}{2}F_S(x) +  \frac{1-\delta}{2},~\frac{1+\epsilon}{2}F_T(y) +  \frac{1-\epsilon}{2}  \right)\\
		&=~ cor\left(  (1+\delta)F_S(x) +  1-\delta,~(1+\epsilon)F_T(y) +  1-\epsilon  \right)\\
	\end{aligned}
	$$
It is easy to check that in the continuous case, the above expression is the same as the correlation of PSR in the continuous case ($F_S(s-) = F_S(s)$):
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  (1+\delta)F_S(x) -\delta,~(1+\epsilon)F_T(y) -\epsilon  \right)\\
	\end{aligned}
	$$

\subsubsection{Spearman Correlation and Correlation of PSR for Discrete Censored Outcome.}
For the discrete case, Spearman correlation can be defined as:
	$$
	\begin{aligned}
		cor\left( \frac{F_S(s) + F_S(s-)}{2},~\frac{F_T(t) + F_T(t-)}{2} \right)\\
	\end{aligned}
	$$
In addition, if we do not observe time to event, instead of $F_S(x)$ or $F_S(x-)$ we take the expectation:  $E_S[F_S(x)|S>x]$ or $E_S[F_S(x-)|S>x]$ respectively. Let's first find $E_S[F_S(x)]$. For brevity, we denote $Pr\left\{S=s\right\}$ as $P_s$:
	$$
	\begin{aligned}
		E_S[F_S(x)] &= \sum_{i=0}^{\infty}F_S(i)P_i = \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		&= (P_0 P_0) + (P_0 P_1 + P_1 P_1) + (P_0 P_2 + P_1 P_2 + P_2 P_2) + ...= \\
		&= P_0^2 + P_1^2 + P_2^2 + ... + P_0 P_1 + P_0 P_2 + P_2 P_1 + ... =\frac{1}{2}(P_0 + P_1 + P_2 + ...)^2 + \frac{1}{2}(P_0^2 + P_1^2 + P_2^2 + ...) =\\
		&= \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2
	\end{aligned}
	$$
Now it is easier to derive $E_S[F_S(x)|S>x]$:
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 + \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$

In a similar manner, we derive $E_S[F_S(x-)|S>x]$, assuming that $F_S(0-)=0$:
	$$
	\begin{aligned}
		E_S[F_S(x-)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i-1)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i-1}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} - \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 - \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$
Now, let's recall that in the discrete case each Spearman correlation component is $\frac{F_S(s) + F_S(s-)}{2}$, so let's compute it for the case when $\delta=0$:
	$$
	\begin{aligned}
	  \frac{E_S[F_S(x)|S>x] + E_S[F_S(x-)|S>x]}{2} &= \frac{1}{2}\left[\frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}   +   \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\right]=\\
		&= \frac{1 - F_S^2(x)}{2(1-F_S(x))}= \frac{1 + F_S(x)}{2}\\
	\end{aligned}
	$$
As a result, we have:
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta\frac{F_S(x) + F_S(x-)}{2} + (1-\delta) \frac{1 + F_S(x)}{2}, ~\epsilon\frac{F_T(y) + F_T(y-)}{2} + (1-\epsilon) \frac{1 + F_T(y)}{2} \right)=\\
		&=~ cor\left(  \delta( F_S(x) + F_S(x-)) + (1-\delta) (1 + F_S(x)), ~\epsilon (F_T(y) + F_T(y-)) + (1-\epsilon) (1 + F_T(y)) \right)=\\
		&=~ cor\left( \cancel{\delta F_S(x)} + \delta F_S(x-) +  1 + F_S(x) -\delta - \cancel{\delta F_S(x)}, ~\cancel{\epsilon F_T(y)} + \epsilon F_T(y-) + 1 + F_T(y)-\epsilon -\cancel{\epsilon F_T(y)}   \right)=\\
		&=~ cor\left( F_S(x) + \delta F_S(x-) +  1 -\delta , ~ F_T(y) + \epsilon F_T(y-) + 1 -\epsilon   \right)\\
	\end{aligned}
	$$
It can be easily shown that the above expression is equivalent to correlation of \emph{PSR}:
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  F_S(x) + \delta F_S(x-) -\delta,~F_T(y) + \epsilon F_T(y-) -\epsilon  \right)\\
	\end{aligned}
	$$

We have proved the equivalence of \emph{Spearman} correlation and the correlation of the PSR based on the following definition of \emph{Spearman} correlation: $corr(F_S(s), F_T(t))$. It is also possible to show the equivalence of \emph{Spearman} correlation and the correlation of PSR using another definition: $E(sign(S-S_0)sign(T-T_0))$, where $(S,T)$ is a paired event time, and $S_0 \perp T_0$.\\
~\\
We now summarized papers that in the search of a linear rank test for association of bivariate survival data, arrived to a censored version of \emph{Spearman} correlation.


\subsection{Censored Version of Spearman Correlation in Earlier Research}

\cite{prentice1978linear}  considered a model of linear regression with time to event as an outcome, while allowing the time to be censored.
	$$
	\begin{aligned}
		y = \alpha + \beta z + \sigma e
	\end{aligned}
	$$
The author's goal was to develop a linear rank statistic in order to check the following hypothesis: $\beta = 0$. Although this model described univariate survival, we are interested in it because of the author's approach to censored observations.
%In order to do this he had to find an approach to rank survival outcome in the presence of censored data. He chose a simplified approach to the ranking of censored values [Kalbfleisch \& Prentice, 1973].
According to this approach, uncensored observations were ranked among themselves, while censored observations were ranked according to their location relatively to uncensored observations. This approach allowed to develop a linear rank statistic and its approximation that assigned the same score to all censored residuals between adjacent uncensored values.\\
%Even after using ranks and approximations, Prentice's approach required assuming $F_i$ - distribution of residuals (check this please).

Using the results of Prentice's work, \cite{cuzick1982rank} considered a general model of association:
	$$
	\begin{aligned}
		Y_1 = aZ + e_1 ~~~~~~~~ Y_2 = bZ + e_2,
	\end{aligned}
	$$
where $Y_1$, and $Y_2$ were event times that could be censored, and random variable $Z$ and $e_1$ and $e_2$ were indenpendent, and finally $b=a\lambda,~~0<|\lambda|<\infty$. The model suggests that two randome variables $(Y_1, Y_2)$ are related to a third unobserved covariate.\\
Cuzick assumed the same generalized ranking of censored data as \cite{prentice1978linear}: the uncensored observations were ranked among themselves and the censored observations were given the same rank as its closest uncensored observation on the left. Although his approach was rather general, it did require knowing $f_i(x)$ the density of $e_i$, and as an illustration, he assumed a logistic density of $e_1$ and $e_2$:  $f(x) = 2\pi_{-1} e^{-x}/(1+e^{-x})^2$. It turned out that for this particular case the rank of each observation with no censoring was $F(x)$, and in the presence of censoring, the rank was $\frac{1+F(x)}{2}$, which resulted in the association test statistic equivalent to \emph{Spearman} correlation.\\

\cite{dabrowska1986rank} developed a semi-parametric statistic for testing an association of two survival outcomes, possibly censored, with the following null hypothesis: $H_0:~F=F_1 F_2$. In order to test this hypothesis, the author further developed the results of \cite{prentice1978linear} and \cite{cuzick1982rank} , and suggested a \emph{linear rank} statistic and its simplification in the following form:
	$$
	\begin{aligned}
		 S_n = \sum_{n=1}^N \mathcal{J}_1( \hat{F}_1, \delta_{1n}) \cdot \mathcal{J}_2( \hat{F}_2, \delta_{2n}),
	\end{aligned}
	$$
where $\hat{F}_i$ were estimators similar to the usual \emph{Kaplan-Meier} estimators of the marginal \emph{CDF}'s. She showed that under certain assumptions and with a particular choice of $\mathcal{J}_i(u,d)$, $\mathcal{J}_i(u,d) =d-(1+d)u$, the test statistic corresponds to the censored data version of the Spearman test. Substituting this choice of $\mathcal{J}_i(u=F_i,~d=\delta_i)$ into the above expression, we get:
	$$
	\begin{aligned}
		 S_n &= \sum_{n=1}^N (\hat{F}_1 - \delta_{1n}(1-\hat{F}_1))\cdot (\hat{F}_2 - \delta_{2n}(1-\hat{F}_2))\\
	\end{aligned}
	$$
The above expression reminds us of $E[PSR_1 \cdot PSR_2]$, covariance of probability scale residuals. As a reminder, $PSR(x, \delta) = F(x)-\delta(1-F(x))$ for continuous and censored failure time.\\

\cite{ding2004testing} suggested the following statistic for testing independence for bivariate \emph{current status data}:
	$$
	\begin{aligned}
		 E[cov(\delta_1, \delta_2)|C_1,C_2] = E\left\{ [\delta_1 - F_1(C_1)][\delta_2 - F_2(C_2)]  \right\}\\
	\end{aligned}
	$$
Which is exactly covariance of PSR for \emph{current status data}.\\
~\\
Although censored version of \emph{Spearman} correlation was suggested before PSR were introduced, the framework for \emph{Spearman} correlation statistics was based on specific distribution assumptions were not generalized for discrete data. The using the tests statistic computed as correlation of PSR does not have require distribution assumptions and can be applied to discrete time to event.

\section{DISCUSSION AND FUTURE RESEARCH}

In this paper we summarized several different approaches to address association of bivariate survival data. Our focus was on the three main approaches. We first summarized literature on evaluating a \emph{cross ratio} in the context of a parametric model. This approach was later developed to include larger family of models, and to generalize the estimate of this association measure to a non-parametric approach. The second approach was to evaluate a bivariate survival surface as a product of two marginal survival functions and a function representing the association component between two time to event variables. The third approach was developing association tests for bivariate time to event variables.\\

Although all approaches were based on a complex statistical and mathematical theory, their main disadvantages were parametric assumptions, computational complexity, and lack of generalizability. For example, although \cite{cuzick1982rank}, \cite{dabrowska1988kaplan}, and \cite{ding2004testing} derived statistics identical to correlation of PSR for continuous case, their conclusions were rooted in a semi-parametric context, with very specific assumptions about the distribution of time to event, asymptotic properties of the statistics, and ranking of censored events. Also, their results also were not generalized to discrete outcomes.\\

The use of correlation of PSR, on the other hand, does not require parametric assumptions and can be generalized to discrete censored data and to discrete currents status data. The test of association can be performed using the following statistic:
$$
\begin{aligned}
	T = \frac{\sum_{i=1}^{n} (r_{si} - \bar{r}_s)(r_{ti} - \bar{r}_t)}{\sqrt{\sum_{i=1}^{n} ((r_{si} - \bar{r}_s)^2\sum_{i=1}^{n} (r_{ti} - \bar{r}_t)^2}}
\end{aligned}
$$
where $r_{si}$ and $r_{ti}$ are PSR and $\bar{r}_s = \frac{1}{n}\sum_{i=1}^{n} r_{si}$ and $\bar{r}_t = \frac{1}{n}\sum_{i=1}^{n} r_{ti}$.

For ordinal uncensored data, \cite{li2010test} suggested two ways of performing the inference: 1) a bootstrap procedure and 2) a large sample approximation. In our future work we will focus on proving the validity of applying bootstrap procedure to finding the confidence intervals for $T$, and on finding the asymptotic distribution for $T$ for continuous and discrete censored data and current status data.\\

We also plan to focus on conditional and partial correlation of PSR for censored and current status data in order to test adjusted association of time to event and adjusted association of status. Partial correlation can be obtain by fitting a parametric or semi-parametric survival model using covariates of interest, and then computing correlation of the resulting PSRs. Conditional correlation can also be computed by first fitting a parametric or semi-parametric survival model, and then evaluating conditional moments that approximate conditional correlation of PSR.

Another challenge of our future work is to compare the performance of test based on covariance of PSR with the performance of test described in \cite{shih1996tests}. The authors suggested two test statistics based on the covariance process of the martingale residuals: a supremum of covariance and a weighted covariance. \cite{shepherd2016probability} have already established the connection between PSR and martingale residuals and have compared the performance of these two residuals when checking functional adequacy of model fit. In the future we are planning to compare the performance of the statistics suggested by \cite{shih1996tests} using optimal and suboptimal weights and with varying fraction of censored data.




% \subsection{Zhang, 2008: Inference on the association measure for bivariate survival data with hybrind censoring and applications to an HIV study, ref ?}
% The author uses archimedian copulas and Kendall's $\tau$ to assess association of bivariate survival data with hybrid censoring.
%
% \section{Summary of: An Adjustment to Improve the Bivariate Survival Function Repaired NPMLE. Moodie, Prentice, 2005 \cite{moodie2005adjustment}}
%
% \subsection{Alvo and Cabilio, 1995 citation ?}
% UNDER CONSTRUCTION.\\
% The authors use a concept of distance applied to \emph{Spearman} and \emph{Kendal} correlation measure in order to build a test that can deal with missing data.


%
% \section{Oral Exam Stuff}
% \begin{itemize}
% 	\item Probability scale residuals, PSR (Li\&Shepard)
% 	\item Challanges and limitations (discrete data covariates)
% 	\item Qi - covariates (look at prentice and Cuzick)
% 	\item show that this is Spearman
% 	\item apply to dataset
% 	\item simmulations
% 	\item explore performance of partial and conditional Spearman.
% 	\item Understand Qi's conditional, partial, and conditional-partial correlation
% 	\item Develop Spearman correlation for different $s$ and $t$: instead of having one number $\rho$ develop a new measure $\rho(s, t)$.
% 	\item \textbf{READ: Hanley, J. A., and Parnes, M. N. (1983), Nonparametric Estimation of a Multivariate Distribution in the Presence of Censoring, Biometrics, 39, 129-139.}
% \end{itemize}
~\\
% \subsubsection{Shih, Louis, 1996, \cite{shih1996tests}}
% \textbf{Motivation}: Leukemia patients that undergo bone marrow transplantation are at risk of \emph{acute graft versus host disease (AGVHD)} and \emph{cytomegalovirus (CMV)}, and the question is if these events are correlated:
%
% The authors consider martingale residuals, $M_{ij}$, which is the difference between observed and expected deaths. They base their new statistic on the result of Clayton [\cite{clayton1978model}] and Cuzick [\cite{shih1996tests}] who developed a model for bivariate time to event with frailties, and suggested the following test statistic that measured the sample covariance of the martingale residuals:
% $$
% \begin{aligned}
%   T_n=\sum_i \left\{ \delta_{i1} - \hat{\Lambda}(X_{i1}) \right\} \left\{ \delta_{i2} - \hat{\Lambda}(X_{i2}) \right\} =\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2).
% \end{aligned}
% $$
%
% Shih and Louis suggested to improve its power by introducing the following two statistics:
%
% \begin{enumerate}
% 	\item take supremum: $U_n= \sup_{0\leq t \leq t_0} \left|\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)\right|$
% 	\item weigh it: $V_n= \sum_i \int_0^{t_0}  \int_0^{t_0} W_n(u_1,u_2) d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)$ (which is analogous to the weighted logrank test)
% \end{enumerate}
% They choose \emph{optimal} weights by somehow using the \emph{cross ratio} defined by Oakes $\theta = \frac{ \frac{\partial^2 S(s,t)}{\partial s \partial t} S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}$:
%
% Although statistic $U_n$ is a supremum over $t_1=t_2$, the simulations showed promising results.
% For $V_n$, if the weights are not specified correctly, the test based on $V_n$ looses power.


% Lucy's way:
%\printbibliography
% Jonathan's way:
%\bibliographystyle{plainnat}

\bibliography{summaryOfRef}
\clearpage

\addcontentsline{toc}{section}{Appendix A}
\section*{Appendix A} \label{appendixA}
Here we show that the \emph{cross ratio} of \cite{clayton1978model} can be written in the following ways:
	$$
	\begin{aligned}
	  \theta = \frac{ \lambda_s(s|T=t)}{\lambda_s(s|T>t)} = \frac{ \lambda_t(t|S=s)}{\lambda_t(t|S>s)} = \frac{ \frac{\partial^2 S(s, t)}{\partial s\partial t} \cdot S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}
	\end{aligned}
	$$

Let $f(s,t)$ and $S(s,t)$ be joint density and survival function respectively of age at which fathers ($T$) and sons ($S$) succumb to disease, then we can write:
	$$
	\begin{aligned}
		\frac{\partial^2 S(s, t)}{\partial s\partial t} &= f(s,t) = f(s|t)\cdot f(t)\\
		\frac{\partial S(s,t)}{\partial t} &= \int_s^{\infty}f(u, t)du = \int_s^{\infty}f(u|t)f_t(t)du = S(s|t)\cdot f_t(t)\\
		\frac{\partial S(s,t)}{\partial s} &=...= S(t|s)\cdot f(s) = Pr(T>t|s)\cdot f(s) = Pr(T>t, s) = Pr(s,T>t) = Pr(s|T>t)S_t(t) = \\
		&=f(s|T>t)\cdot S_t(t)\\
		S(s,t) &= Pr(S>s|T>t)\cdot Pr(T>t) = S(s|T>t)\cdot S_t(t)
	\end{aligned}
	$$
By substituting the right-hand sides of the above equalities into the expression for $\theta$, we get:
	$$
	\begin{aligned}
		\theta &= \frac{\frac{\partial^2 S(s, t)}{\partial s\partial t}/\frac{\partial S(s, t)}{\partial t}}  {\frac{\partial S(s, t)}{\partial s}/ S(s, t) } = \frac{  \frac{f(s|t)\cancel{f_t(t)}}{S(s|t)\cancel{f_t(t)}} } { \frac{f(s|T>t)\cdot \cancel{S_t(t)}}{S(s|T>t)\cdot \cancel{S_t(t)}}  }
		  = \frac{f(s|t)/S(s|t)}     {f(s|T>t) / S(s|T>t) }\\
	\end{aligned}
	$$
By defintion, $\lambda_s(s|t) = \frac{f(s|t)}{S(s|t)}$ and $\lambda_s(s|T>t) = \frac{f(s|T>t)}{S(s|T>t)}$, therefore:
	$$
	\begin{aligned}
		\theta &=\frac{f(s|t)/S(s|t)}     {f(s|T>t) / S(s|T>t) }
		= \frac{\lambda_s(s|t) } {\lambda_s(s|T>t) }\\
	\end{aligned}
	$$
Because of the symmetry, we can write:
	$$
	\begin{aligned}
	  \theta = \frac{ \frac{\partial^2 S(s, t)}{\partial s\partial t} \cdot S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}} = \frac{ \lambda_s(s|T=t)}{\lambda_s(s|T>t)} = \frac{ \lambda_t(t|S=s)}{\lambda_t(t|S>s)}~~\blacksquare
	\end{aligned}
	$$
\clearpage

\addcontentsline{toc}{section}{Appendix B}
\section*{Appendix B} \label{appendixB}

In the same paper where \cite{dabrowska1988kaplan} derived an estimator of bivariate survival surface, she gave an example data for which her estimator assigned a negative mass. Figure \ref{fig:bubbles} shows this data graphically. The red points represent uncensored observations. The orange point is a singly censored observation (it is censored on the $T_2$ axis). The white point is an observation censored in both components, $T_1$ and $T_2$. The values for each element of the Dabrowska's estimator can be found in Table \ref{dabrExTable}. The computations following this table show that the bivariate survival function estimator in point $(0.51,~0.02)$ (see Figure \ref{fig:bubbles}) was greater than its marginal estimator in point $(0.51,~0)$. 

\begin{figure}[!h]
\caption{Example that demonstrates assignment of negative point mass. The red points represent uncensored observations. The orange point is a singly censored observation (it is censored on the $T_2$ axis). The white point is an observation censored in both components. The number at risk is plotted near each point.}
\includegraphics{figure1.pdf}
\label{fig:bubbles}
\end{figure}

We use Table \ref{dabrExTable} and the formula below to order to show that $\hat{S}(t_1=0.51,~t_2=0) < \hat{S}(t_1=0.51,~t_2=0.02)$:

	$$
	\begin{aligned}
		\hat{S}(t_1,t_2) &= \hat{S}(t_1,0)\hat{S}(0,t_2)\cdot \prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
	\end{aligned}
	$$

\input{dabrowskaExampTable}
From Table \ref{dabrExTable} we have:
	$$
	\begin{aligned}
		&\hat{S}(0.51,~0)  = 0.375\\
		&\hat{S}(0,~0.02)  = 0.75\\
		&\hat{S}(0.51,~0.02) = \hat{S}(0.51,~0)\hat{S}(0,~0.02) \times 
		\prod_{{0<u\leq t_1~0<v\leq t_2}}\{1 - \hat{L}(\Delta u, \Delta v)\}=\\
		&~~~~~~~~~~~~~= 0.375 \cdot 0.75 \cdot (1-0.111)\cdot (1-0)\cdot (1-(-1)) = \\
		&~~~~~~~~~~~~~= 0.5
	\end{aligned}
	$$

We observe that $\hat{S}(0.51,~0) = 0.375 < 0.5 = \hat{S}(0.51,~0.02)$.

\end{document}          

