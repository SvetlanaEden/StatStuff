\documentclass[]{article}

%\usepackage[final]{pdfpages}
\usepackage{anysize}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\DeclareGraphicsExtensions{.png,.pdf}

% %%% for citations. See Lucy's tutorial: https://github.com/LucyMcGowan/Tutorials/blob/master/BiblatexTutorial.md
% % \usepackage[backend=biber, maxnames=10, citestyle=science]{biblatex}
% \usepackage[style=authoryear, backend= biber]{biblatex}
% \addbibresource{summaryOfRef.bib}

% %%% for citations. Jonathan's way:
\usepackage{natbib}
\bibliographystyle{unsrtnat}



%%% when using biber as a "backend" compile like this:
% pdflatex myFile
% biber myFile
% pdflatex myFile


\let\epsilon\varepsilon
\newcommand\SLASH{\char`\\}
\newcommand{\csch}{\text{csch}}
\marginsize{0.5in}{1in}{0.5in}{1in}
\setlength\parindent{0pt}

% Title Page: Modify as needed
\title{Association of bivariate survival data}
\author{Svetlana Eden}
\date{Month Day, 2015}

\hypersetup{hidelinks}

% \usepackage[pdftex,bookmarks,pagebackref,pdfpagemode=UseOutlines,
%      colorlinks,linkcolor=\linkcol,
%      pdfauthor={Svetlana K Eden},
%      pdftitle={\titl}]{hyperref}


\begin{document}
\maketitle
\tableofcontents
\listoffigures
\listoftables
\clearpage

\section{INTRODUCTION}
The interest in survival data can be traced back to at least 1846, when a Hungarian doctor of the Vienna General Hospital maternity clinic, Ignaz Semmelweis, recorded the rate of death from childbed fever. After excluding several other hypotheses, he suggested that a child birth is helped by a doctor or a medical student, the women's rate of death from childbed fever was higher because of a disease that was caused by cadaverous particles that medical students and doctors had on their hands after dissecting cadavers. As you might know, his theory was dismissed, and he died alienated from the medical community. The rational for his hypothesis was discovered decades later, when the germ theory of the disease was developed. 


Another example was Florence Nightingale, who was born into a rich upper-class British family, and in spite of her status she pursued a career of a nurse. She was credited for significantly reducing hospital mortality of British soldiers during the Crimean War, in 1854 (with hand-washing being one of the most important practices implemented under her supervision). She also published some of her works in simple English in order to popularize medical knowledge.\\
In these two examples, the survival data could be collected as a binary variable: died or lived.  Another way of collecting survival data is to record time to death or any other event of interest. \\

\cite{fisher1931truncated},  \cite{hald1949maximum}, and \cite{swan1969computing} were probably one of the first to suggest a more complex view of survival data and introduced a concept of \emph{censoring}. An subject is censored at time $t$ if the event did not happen yet, and after time $t$ it can not be observed. For example, suppose we have a five-year longitudinal study that aims to measure an association of blood pressure and time to cardiovascular event. Patients who do not have a cardiovascular event during this period are labeled as \emph{censored}. Censoring is \emph{non-informative} if it is not associated with the potential event time. Otherwise it is called \emph{informative}. Assumption of non-informative censoring simplifies estimation of survival, but nevertheless is a non-trivial problem. It was solved for a univariate case by \cite{kaplan1958nonparametric} who introduced the Product-Limit estimator of the survival curve. \cite{david1972regression} extended their work by introducing a regression model for censored survival outcome. His model utilized a concept of a \emph{hazard rate}:

$$
\begin{aligned}
	\lambda(t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T\in[t,t+h] | T\geq t)=
	%&=  \frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t)  \right\}= \\&=
	             \frac{f(t)}{\mathcal{S}(t)}
\end{aligned}
$$

Where $T$ is time to event, $\mathcal{S}(t) = 1-F(t)$, and $F(t)$ and $f(t)$ are cumulative distribution function and density function of random variable $T$ respectively. $P(T\in[t,t+h] | T\geq t)$ is the probability that the event occurs in the time period of $[t,t+h]$ given that the subject survived without the event at least until time $t$. In other words, hazard rate can be intuitively thought of as a probability of the event per unit of time.\\
Although the notion of hazard rate is intuitively simple, its estimation required complex theoretical tools even in a univariate case. Adding another dimension not only increases the complexity, but also makes it more difficult to apply intuitive thinking. Pursuing a purely theoretical question in the context of \emph{bivariate} survival function, \cite{basu1971bivariate} %[get this paper from the library ??? this does not add up because Cox wrote his paper later]
defined a double failure hazard rate as:
$$
\begin{aligned}
	\lambda(t_1,t_2)&= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[t_1,t_1+h_1], T_2\in[t_2,t_2+h_2] | T_1\geq t_1, T_2 \geq t)=
	%&= \frac{\partial}{\partial t_1}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\frac{\partial}{\partial t}\left\{  -log \mathcal{S}(t_1,t_2)  \right\} - \frac{\partial^2}{\partial t_1\partial t_2}\left\{  -log \mathcal{S}(t_1,t_2)  \right\}\\
	 \frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}
\end{aligned}
$$
The intuition of the univariate hazard rate helps us understand the above expression because of the similarity between $\frac{f(t_1,t_2)}{\mathcal{S}(t_1,t_2)}$ and$\frac{f(t)}{\mathcal{S}(t)}$. We included this definition because the rest of the summarized literature relies on it.\\

Although the work of \cite{basu1971bivariate} was not targeted toward specific applications, there are many practical problems where bivariate survival analysis could find its use, for example, in studies of time to appendicitis in twins, or of time to cardiovascular disease among subjects with familial ties. In this paper, we are concerned with the question of association between paired survival data. Although the literature on this subject is vast, we review only three main approaches. First, this question was addressed by introducing a \emph{bivariate hazard ratio} in the context of specific family of distributions. The second approach was to find a non-parametric estimate of the bivariate survival function with three components: two marginal survival functions and a term that represented an association between two survival variables. The third approach was to develop semi-parametric and non-parametric tests to examine association of residuals. We especially focus on the non-parametric test of \emph{probability scale residuals} introduced by \cite{li2012new}.\\
The main goal of the paper was to show the complexity of theoretical tools required to address the question of bivariate survival in the presence of censoring and to highlight the generalizability of \emph{probability scale residuals} approach and simplicity of its use. In conclusion, we identify the direction of our future research with a goal, to apply \emph{probability scale residuals} for a wide range of different statistical scenarios in the context of bivariate survival with non-informative censoring.

\section{LITERATURE REVIEW}

\subsection{Modeling a Bivariate Hazard Ratio}

The work of \cite{clayton1978model} was motivated by epidemiological studies of chronic disease in studies of familial tendency. In such studies, the assumption of independence between individual events is no longer valid. It would be possible to consider parental history as a covariate and to use Cox proportional hazard model, but the problem is that time to event for the parent can be censored, for example, if time to cardiovascular disease is studied and the parent dies of cancer. 
%  Cox assumed that all subjects have the same baseline hazard rate:
%
% $$
% \begin{aligned}
% 	\lambda(s, x) &= \lambda_0(t) \cdot e^{\pmb{x\beta}}
% \end{aligned}
% $$

Clayton addressed the question of association between paired survival times of fathers and sons and defined a measure similar to hazard ratio, that had the following properties:
\begin{enumerate}
	\item Easy to compute
	\item Expressible as a constant ratio of age-specific rates
  \item Symmetrical in two variables
\end{enumerate}
If we denote time to event for fathers to be $s$ and time to event for sons to be $t$, this measure can be written in the following way:
$$
\begin{aligned}
	\theta = \frac{\lambda_s(s_0|t=t_0)}{\lambda_s(s_0|t>t_0)} = \frac{\lambda_t(t_0|s=s_0)}{\lambda_t(t_0|s>s_0)}
\end{aligned}
$$
The requirement of symmetry was motivated by the fact than neither son nor father were causes of the event, rather, there was an unobservable variable that was associated with the event for both of them.

This measure can be interpreted for sons (as much as for fathers) in the following way: how much more likely for a son to have an event at time $s_0$ given that his father had an event at time $t_0$ compared to a son whose father survived until time $t_0$ without the event. Independence between fathers and sons would mean $\theta = 1$, positive association $\theta > 1$ and negative association $\theta < 1$.
 %Clayton also explained how his model could be generalized to include covariates, and derived the likelihood to estimate $\theta$ when two marginal distributions are completely unknown using approach similar to one used by Cox.\\
Clayton's assumptions lead to a specific form of survival function:
$$
\begin{aligned}
f(s,t) = \frac{\theta a'(s)b'(t)}{[1+(\theta - 1)(a(s) + b(t))]^{2+\frac{1}{\theta-1}}}
\end{aligned}
$$
where $a(s)$ and $b(t)$ are non-decreasing functions and $a(0)=0$ and $b(0)=0$. He suggested a combined approach of evaluating probabilities of having and even non-parametrically and then using likelihood to estimate $\theta$.\\
~\\
\cite{oakes1982model} studied Clayton's model extensively and corrected his likelihood approach for estimating the variance of $\theta$. He also showed that : $\frac{\theta - 1}{\theta + 1} = \tau$, where $\tau$ is a coefficient of concordance introduced by  \cite{kendall1938new}: for two independent pairs of variables (with the same bivariate distribution) $(U_1, Z_1)$ and $(U_2, Z_2)$, Kendal's $\tau$ is a probability of concordance minus probability of discordance:
$\tau = P[(U_1 - U_2)(Z_1 - Z_2)>0] - P[(U_1 - U_2)(Z_1 - Z_2)<0] = E[sign((U_1 - U_2)(Z_1 - Z_2))]$. This fact suggested that $\theta$ that there might be alternative non-parametric ways of estimating $\theta$.\\

Later \cite{oakes1989bivariate} introduced a bivariate survival model based on \emph{frailties}, unobservable random variables. If we denote the variable as $W$, this means that the probability of survival is conditional on it: $Pr(T>t|W=w)$. This is a way to model random effect in survival analysis. In the context of bivariate survival analysis, if we assume that both survival times depend on the same unobservable random variable, $W$:
$$
\begin{aligned}
	Pr(T_1>t_1|W=w)=\left\{ B_1(t_1) \right\}^w\\
	Pr(T_2>t_2|W=w)=\left\{ B_2(t_2) \right\}^w\\
\end{aligned}
$$
This induced dependence between the variables, $T_1$ and $T_2$ themselves:
$$
\begin{aligned}
	S(t_1,t_2) = \int \left\{ B_1(t_1) B_2(t_2)\right\}^w dF(w)
\end{aligned}
$$

He showed that bivariate distributions introduced by frailty models were part of a larger class of \emph{archimedian} distributions studied by \cite{genest1986copules}:
$$
\begin{aligned}
	S(t) = p\left[ q\left\{ S_1(t_1) \right\}  + q\left\{ S_2(t_2) \right\}  \right]
\end{aligned}
$$

Where $p(u)$ is a non-negative decreasing function with non-negative second derivative, with $p(0)=1$, and $p(u)$ is an inverse function of $q(v)$.\\
It can be shown that Clayton's $\theta$ can be expressed as following:
	$$
	\begin{aligned}
		\theta &= \frac{ \frac{\partial^2 S(s,t)}{\partial s \partial t} S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}
	\end{aligned}
	$$
Using this expression, Oakes showed that for \emph{archimedian} class of models, $\theta$ depends on $t$ only through $S(t)$, which allowed him to derive an algebraic expression for $\theta$ for this class of models:
$$
\begin{aligned}
\theta(v) = -v\cdot q''(v)/q'(v)
\end{aligned}
$$
where $v=S(t_1, t_2)$. He also pointed out that because $\theta$ and \emph{Kendall}'s $\tau$ were related through the following equality: $\tau(v) = \frac{\theta(v)-1}{\theta(v)+1}$, $\tau(v)$ can be considered as a measure of local dependence because it depends on $v=S(t_1, t_2)$.\\
~\\

\cite{fan2000dependence} argued that in many studies the age of the participants was restricted by design, for example, the age of mothers could be restricted to 50-80, and of daughters to 30-60. Because of this, there was a need to have an association measure that would take this restriction into account and would be well suited for censored time to event. \\
They considered a weighted average of Clayton's \textit{cross ratio} $\theta$ over $[0,t_1] \times [0, t_2]$, $\theta = \frac{ \lambda_2(s_2|T_1=s_1)}{\lambda_2(s_2|T_1 \geq s_1)}$. As a reminder, $\theta$ can be interpreted as a risk for daughters of age $s_2$ whose mothers developed a disease at age $s_1$, compared to daughters of age $s_2$ whose mothers were disease free at age $s_1$. They chose this measure because of its interpretability, and suggested that in order to increase comparability across studies, the weighting should be independent of censoring mechanism. The following weights were proposed: $\frac{F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)}$. For one dimension, this weight could be interpreted as a probability of an event at point $t$ given that the subject survived until $t$.\\

%They allowed the time to be discrete, continuous, or mixed, but $F(., .)$ was required to possess a density relative to the corresponding counting Lebesque measure.\\
Instead of weighting $\theta(s_1, s_2)$, the authors chose to weight $c(s_1, s_2) = \frac{1}{\theta(s_1, s_2)}$ because weighting $c(s_1, s_2)$ results in a consistent estimator:
	$$
	\begin{aligned}
		C(t_1, t_2) &= \int_0^{t_1}\int_0^{t_2} \frac{c(s_1, s_2)F(ds_1,ds_2)}{\int_0^{t_1}\int_0^{t_2} F(du_1, du_2)} = ...= \int_0^{t_1}\int_0^{t_2} \frac{F(s_1^-, s_2^-)\Lambda_{10}(ds_1, s^-_2)\Lambda_{01}(s^-_1, ds_2)}{1 - F(t_1, 0) - F(0, t_2)+ F(t_1, t_2)} \\
	\end{aligned}
	$$
	The above measure can be estimated non-parametrically using $\hat{F}$ of \cite{dabrowska1988kaplan} or \cite{prentice1992covariance}, and $\hat{\Lambda}$ can be estimated using a Nelson-Aalen type estimator.
	%??? Nelson-Aalen
Note that $C(t_1, t_2)$ may be interpreted as an average relative risk over $[0,t_1] \times [0, t_2]$. When $C(t_1, t_2)=1$, there is no association. When $C(t_1, t_2)<1$, there is positive association. It can also be interpreted as a weighted average hazard ratio with weights proportional to the failure time density at $(s_1,s_2)$.\\
Because \cite{oakes1989bivariate} noted that \emph{Kendal's} $\tau(s_1, s_2)$ can be written as:
	$$
	\begin{aligned}
		\tau(s_1, s_2) &= \frac{1 - c(s_1, s_2)}{1+c(s_1, s_2)}
	\end{aligned}
	$$
	the authors also proposed another summary measure of association, a conditional \emph{Kendal's} $\tau(s_1, s_2)$:
	$$
	\begin{aligned}
		&\tau(s_1, s_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} = s_1,~T_{21}\wedge T_{22} = s_2  \}
		&~~~~where~\wedge~is~minimum
	\end{aligned}
	$$

After weighting $\tau(s_1, s_2)$ proportionally to the density, they obtained a weighted conditional \emph{Kendal's} $\tau(s_1, s_2)$:
	$$
	\begin{aligned}
		&\mathcal{T}(t_1, t_2) = E\{sign(T_{11} - T_{12}) (T_{21} - T_{22})~|~ T_{11}\wedge T_{12} \leq t_1,~T_{21}\wedge T_{22} \leq t_2  \}
	\end{aligned}
	$$
	so that $\mathcal{T}(t_1, t_2) \in [-1,1]$. Both, $C(t_1, t_2)$ and $\mathcal{T}(t_1, t_2)$ were proven to be consistent and converged in distribution to a mean zero Gaussian process.\\
% and had a property of asymptotic validity (definition ?) of the bootstrap.
~\\

\cite{fan2000class} suggested a class of estimators similar to ones described by \cite{fan2000dependence} . They considered special cases of $\theta(t_1, t_2)$ and weights, where instead of using the bivariate survivor function estimators of \cite{dabrowska1988kaplan} and \cite{prentice1992covariance} the hazard function estimators of Nelson-Aalen type could be used. And instead of bootstrap variance estimator, an explicit variance formulae can be obtained.
% The authors utilize the fact that in some specific cases, both $C(\cdot, \cdot)$ and $\mathcal{T}(\cdot, \cdot)$ depend on $F(\cdot, \cdot)$ only through the hazard function and therefore can be estimated using Nelson-Aalan-type estimator of the hazard:
% 	$$
% 	\begin{aligned}
%     C(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2)}{\int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2)}\\
%     \mathcal{T}(t_1, t_2) &= \frac{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) - \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }{\int_0^{t_1}\int_0^{t_1} H_{11}(ds_1, ds_2) + \int_0^{t_1}\int_0^{t_1} H_{10}(ds_1, s_2^-)H_{01}(s_1^-, ds_2) }\\
% 	\end{aligned}
%  $$




\subsection{Estimating Bivariate Survival Surface}

\cite{dabrowska1988kaplan} introduced a bivariate non-parametric estimator and proved its consistency. Let's introduce the following notations.
Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $F(s,t) = P(T_1>s, T_2>t)$ be the corresponding joint survival function. Bivariate cumulative hazard functions are defined by Dabrowska in the following way:
	$$
	\begin{aligned}
		%\Lambda(s,t) &= (\Lambda_{10}(s,t), \Lambda_{01}(s,t), \Lambda_{11}(s,t))\\
		%&where\\
		\Lambda_{11}(ds,dt) &= \frac{P(T_1 \in ds, T_2\in dt)}{P(T_1 \geq s, T_2 \geq t)} = \frac{F(ds, dt)}{F(s-, t-)}\\
		\Lambda_{10}(ds,t) &= \frac{P(T_1 \in ds, T_2 > dt)}{P(T_1 \geq ds, T_2 > t)} = \frac{-F(ds, t)}{F(s-, t)}\\ %~~~see ~the~paper~where ~it~is~F(s-, t-),~probably~a~typo\\
		\Lambda_{01}(s,dt) &= \frac{P(T_1 > s, T_2\in dt)}{P(T_1 > s, T_2 \geq t)} = \frac{-F(s, dt)}{F(s, t-)}\\
		&and\\
		\Lambda_{10}(0,t) &= \Lambda_{01}(s,0) = \Lambda_{11}(0,0) = 0\\
	\end{aligned}
	$$
If $F$ has a density $f(s,t)$, we have $\Lambda_{11}(ds,dt) = \lambda_{11}(s,t)ds~dt$, $\Lambda_{10}(ds,t) = \lambda_{10}(s,t)dt$, $\Lambda_{01}(s,dt) = \lambda_{01}(s,t)dt$, so

	$$
	\begin{aligned}
		\lambda_{11}(s,t) &= \lim_{(h_1,h_2)\rightarrow 0} \frac{1}{h_1 h_2} P(T_1\in[s,s+h_1], T_2\in[t,t+h_2] | T_1\geq s, T_2 \geq t) & = \frac{f(s,t)}{F(s-, t-)}\\
		\lambda_{10}(s,t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_1\in[s,s+h] | T_1\geq s, T_2 > t) & = \int_t^{\infty} \frac{f(s,v)dv}{F(s-, t)}\\
		\lambda_{01}(s,t) &= \lim_{h\rightarrow 0} \frac{1}{h} P(T_2\in[t,t+h] | T_1 > s, T_2 \geq t) & = \int_s^{\infty} \frac{f(u,t)du}{F(s, t-)}\\
	\end{aligned}
	$$
Where $\lambda_{11}(s,t)$ is the instantaneous rate of \emph{double failure} at point $(s,t)$, given that the individuals were alive at times $T_1=s-$ and $T_2 = t-$; $\lambda_{10}(s, t)$ is the rate of a \emph{single failure} at time $s$ given that the first individual was alive at time $T_1=s$ and the second survived beyond time $T_2 = 1$.\\

Under condition of independence of independence of event times and censoring times, the author derived a bivariate survival function estimator that could be computed as a product of two univariate survival function estimators and a third term:
	$$
	\begin{aligned}
		\hat{F}(s,t) &= \hat{F}(s,0)\hat{F}(0,t)\cdot \prod_{{0<u\leq s~0<v\leq t}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
	\end{aligned}
	$$
Where $\hat{F}(s,0)$ and $\hat{F}(0,t)$ are usual Kaplan-Meier estimates, for example, $\hat{F}(s,0) = \prod_{u\leq s}[1-\hat{\Lambda}_{10}(\Delta u, 0)]$, and the third term is a function of bivariate hazards:
	$$
	\begin{aligned}
    \hat{L}(\Delta u, \Delta v) &= \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}(\Delta t_1, \Delta t_2) = \#(T_1=t_1, T_2=t_2, \delta_1=\delta_2=1)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{10}(\Delta t_1, t_2^-) = \#(T_1=t_1, \delta_1=1, T_2\geq t_2)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) = \#(T_2=t_2, \delta_2=1, T_1\geq t_1)/\#(T_1\geq t_1, T_2\geq t_2)
	\end{aligned}
	$$
It is possible to show that expression $1-L(du, dv)$ can be written as:
	$$
	\begin{aligned}
		1 - L(du, dv) &= \frac{P(T_1>u,~T_2>v|T_1\geq u,~T_2\geq v)}  {P(T_1>u|T_1\geq u,~T_2\geq v)P(T_2>v|T_1\geq u,~T_2\geq v)} =
		\frac{ F(u,v)F(u-,v-) }{ F(u,v-)F(u-,v) }
	\end{aligned}
	$$

The above expression looks very much like:
	$$
	\begin{aligned}
		\theta &= \frac{ \frac{\partial^2 S(s,t)}{\partial s \partial t} S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}},
	\end{aligned}
	$$
which is the coefficient of association derived by Clayton[\cite{clayton1978model}] for a specific model. In the context of  Clayton's model, this coefficient was a constant. Dabrowska's  $1-L(du, dv)$ is not a constant and is not a ratio of changes in survival function, but it represents a measure of dependency between two survival times.
This estimator is very easy to compute, but as we see later, it assigns negative mass to some points. The example below was provided by the author:

% \subsubsection{Dabrowska's example}
% {{\tiny
% \begin{verbatim}
% ### Example from Dabrowska, 1988, Kaplan-Meier estimate of the plane
% library(survival)
% kmdata = data.frame(Y1 = c(.51, .68, .11, .24), d1 = c(1, 1, 1, 0), Y2 = c(.02, .68, .62, .24), d2 = c(1, 1, 0, 0))
% KM1 <- survfit( Surv(kmdata$Y1, kmdata$d1)~ 1)$surv
% KM2 <- survfit( Surv(kmdata$Y2, kmdata$d2)~ 1)$surv
% KM1
% KM2
%
% X = c(rep(.11, 4), rep(.24, 4), rep(.51, 4), rep(.68, 4))
% Y = c(rep(c(.02, .24, .62, .68), 4))
% lam11 = c(rep(0, 8),   .5,   rep(0, 6),   1)
% lam10 = c(c(.25, 1/3, 1/2, 1),    rep(0, 4),   0.5,   rep(1,7))
% lam01 = c(.25, c(0, 0, 1),   1/3,   c(0, 0, 1),   .5,   c(0, 0, 1),   1,   c(0, 0, 1) )
% numAtRisk = c(4,3,2,1,   3,2,1,1,   2,1,1,1,   1,1,1,1)
% L = (lam10*lam01 - lam11)/((1 - lam10)*(1 - lam01))
% bivarSurvEstData = data.frame(X=X, Y=Y, lam11=lam11, lam10=lam10, lam01=lam01, KM1=rep(KM1, each=4), KM2 = rep(KM2, 4), L=L, numAtRisk = numAtRisk)
%
% ############## estimator at point (X,Y) = (.51, .02):
% L_11_02 = bivarSurvEstData$L[bivarSurvEstData$X == .11 & bivarSurvEstData$Y == .02]
% L_24_02 = bivarSurvEstData$L[bivarSurvEstData$X == .24 & bivarSurvEstData$Y == .02]
% L_51_02 = bivarSurvEstData$L[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
% KM1_51_02 = bivarSurvEstData$KM1[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
% KM2_51_02 = bivarSurvEstData$KM2[bivarSurvEstData$X == .51 & bivarSurvEstData$Y == .02]
%
% BivarSurvEstimate_51_02 = KM1_51_02 * KM2_51_02 * (1-L_11_02) * (1-L_24_02) * (1-L_51_02)
% BivarSurvEstimate_51_02
%
% BivarSurvEstimate_51_02 > KM1_51_02
%
% ############# plot:
% ylim = range(c(X, Y))
% cex = 0.7
% setwd("/Users/svetlanaeden/stuff/StatStuff/BRYAN")
% pdf("figure1.pdf", height=5, width = 5)
% 	par(mar=c(4, 4, 2, 2))
% 	plot(ylim, ylim, type="n", axes=FALSE, xlab="T1", ylab="T2")
% 	segments(x0=c(0, unique(X)), y0=0, y1 = range(Y)[2], lty=3, col="gray")
% 	segments(x0=0, y0=c(0, unique(Y)), x1 = range(X)[2], lty=3, col="gray")
% 	axis(side=1, at = unique(X), col = "gray", cex.axis=cex)
% 	axis(side=2, at = unique(Y), col = "gray", cex.axis=cex)
% 	ray = 0.08
% 	koef = ray/sqrt(2)
% 	segments(x0=c(.11, .24), y0=c(.62, .24), x1=c(.11, .24+koef), y1=c(.62+ray, .24+koef))
% 	points(unique(X), c(.62, .24, .02, .68), cex=1.2, pch=21, bg=c("orange", "white", "red", "red"), col="black")
% 	offset = 0.015
% 	text(bivarSurvEstData$X-offset, bivarSurvEstData$Y-offset, bivarSurvEstData$numAtRisk, cex=cex)
% dev.off()
%
% if(FALSE){
%   nSim = 100
% 	set.seed(1)
% 	l01 = runif(nSim); l10 = runif(nSim); l11 = runif(nSim)
% 	xData = data.frame(x = (l01*l10 - l11)/((1-l01)*(1-l10)), l01 = l01, l10 = l10, l11 = l11)
% 	xData = xData[order(xData$x),]
% 	plot(xData$x, type="l")
% 	xData[xData$x < -1,]
% 	xData[xData$x > 1,]
% }
%
%
% \end{verbatim}
% }}


\begin{figure}[!h]
\includegraphics{figure1.pdf}
\caption{Example that demonstrates assignment of negative point mass. The red points represent uncensored observations. The orange point is a singly censored observation (it is censored on the $T_2$ axis). The white point is an observation censored in both components.}
\label{fig:bubbles}
\end{figure}

Dabrowska demonstrated that the bivariate survival function estimator in point $(0.51,~0.02)$ (see Figure \ref{fig:bubbles}) was greater than its marginal estimator, point $(0.51,~0)$. She noted that this could be explained by the fact that $\Lambda_{11}$, $\Lambda_{10}$, and $\Lambda_{01}$ summarize three different portions of the data. She proved that in the absence of censoring, this problem does not arise.\\
%\clearpage
~\\
\cite{pruitt1991negative} showed that for Dabrowska's estimator, although the value of negative mass decreased, the number of such points did not disappear with growing sample size, resulting in non-disappearing negative mass. This problem was related to the problem of identifiability of the bivariate survival functions in the presence of censored data. The proof of the authors was rather complicated, but we can intuitively see why this can be the case. Let's rewrite again Dabrowska's estimator:

	$$
	\begin{aligned}
		\hat{F}(s,t) &= \hat{F}(s,0)\hat{F}(0,t)\cdot \prod_{{0<u\leq s~0<v\leq t}}\{1 - \hat{L}(\Delta u, \Delta v)\}\\
    1 - \hat{L}(\Delta u, \Delta v) &= 1 - \frac{\hat{\Lambda}_{10}(\Delta u,v^-)\hat{\Lambda}_{01}(u^-,\Delta v) - \hat{\Lambda}_{11}(\Delta u,\Delta v)}{\left(1-\hat{\Lambda}_{10}(\Delta u,v^-)\right)\left(1-\hat{\Lambda}_{01}(u^-,\Delta v)\right)}\\
  &where\\
	&\hat{\Lambda}_{11}(\Delta t_1, \Delta t_2) = \#(T_1=t_1, T_2=t_2, \delta_1=\delta_2=1)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{10}(\Delta t_1, t_2^-) = \#(T_1=t_1, \delta_1=1, T_2\geq t_2)/\#(T_1\geq t_1, T_2\geq t_2)\\
	&\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) = \#(T_2=t_2, \delta_2=1, T_1\geq t_1)/\#(T_1\geq t_1, T_2\geq t_2)
	\end{aligned}
	$$
It is possible that $\hat{F}(s,t\neq 0) > \hat{F}(s,0)$ when expression $1 - \hat{L}(\Delta u, \Delta v)$ is greater than one, which in term may happen when  $\hat{\Lambda}_{01}(t_1^-,\Delta  t_2) \cdot \hat{\Lambda}_{10}(\Delta t_1, t_2^-)< \hat{\Lambda}_{11}(\Delta t_1, \Delta t_2)$. In the presence of censoring, $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, $\hat{\Lambda}_{11}$ are somewhat \emph{unsynchronized}, in the sense that the number at risk for single failures is the same as the number at risk for double failures in spite of the fact that marginally the number at risk can be larger. Also, because of the censoring, each term $1 - \hat{L}(\Delta u, \Delta v)$ is slightly contributing to the overall larger value that eventually can be bigger than one, which results in the fact that the bivariate survival function is not monotone.\\
Simple simulations showed (not presented here) that term $1 - \hat{L}(\Delta u, \Delta v)$ is greater than $2$ when $\hat{\Lambda}_{01}$ is very large and $\hat{\Lambda}_{10}$ is very small (or the other way around), and when $\hat{\Lambda}_{11}$ is very large. Even for $\hat{\Lambda}_{01} = \hat{\Lambda}_{10} = \hat{\Lambda}_{11} = 0.5$, term $1 - \hat{L}(\Delta u, \Delta v) = 2$. For this  example, when all observations are not censored, the values of $\hat{\Lambda}_{01}$, $\hat{\Lambda}_{10}$, and $\hat{\Lambda}_{11}$ balance out and the negative mass disappears.
~\\

\cite{prentice1992covariance} introduced an estimator similar to Dabrowska's. It had similar performance, and the same negative mass problem. \cite{van1997nonparametric} (see the next section) gave an overview for both estimators and suggested another way of estimating bivariate survival.

\cite{van1997nonparametric} summarized literature for several estimators, and shows that Dabrowska's and Prentice \& Cai's estimators were essentially the following:
	$$
	\begin{aligned}
    S(t_1, t_2) &= S_1(t_1, 0)S_2(0, t_2)R(t_1, t_2);\\
		R(t_1, t_2) &= \frac{S(t_1, t_2)S(0, 0)}{S(t_1,0)S(0,t_2)}
 	\end{aligned}
	$$
where $R(t_1, t_2)$ is called \emph{a cross-ratio}. We see that this is a dependency measure because this is a ratio of columns or rows of a two-by-two table.\\
The author sited the work of \cite{gill1990survey} who proved that these two estimators were equivalent, and performed similarly well, under assumption of complete independence of events and censoring events.
Van Der Laan further discussed difficulties of evaluating bivariate survival using non-parametric likelihood (NPMLE).
%In the presence of singly censored observations it has a problem of uniqueness (see page 321 of Prentice).
He noted that when NPMLE was solved using expectation-maximization (EM) algorithm, the algorithm assigned equal mass to uncensored observations, and singly and double censored observations were assigned mass based on the nearby uncensored observations. In case of singly censored observations, in order for them to get a mass assignment the half line had to contain at least one uncensored observation, which might not happen in a case of continuous time to event.\\
Van Der Laan suggested a repaired NPMLE, where instead of the half-lines there were strips that were wide enough to contain the uncensored observations, and suggested a way of redistributing the mass for singly censored observations based on the uncensored observations that fell into the strips or quadrants.
After comparing several estimator, the author came to conclusion that with no or small dependence, the Dabrowska's and Prentice and Cai's estimators are better. With increased dependence however, the author's estimator performed better.\\
~\\

\cite{moodie2005adjustment} pointed out that the method of Van Der Laan could be inefficient when the strips were large because the uncensored points could borrow the mass from singly censored on one coordinate observations that have very different event time for the other coordinate. They suggested an improvement. They first applied Van Der Laan's method (including dividing the plane into strips containing uncensored and censored observations), and then redistributed the mass from uncensored observations on to points that were located on the half lines of the singly censored observations from the same strip. They reported that this method improves efficiency and reduced computational complexity when computing the variance.\\

% \begin{figure}[!h]
% \includegraphics{MoodiePrentice2005.png}
% \caption{Moodie and Prentice suggestion.}
% \label{fig:bubbles}
% \end{figure}
% \clearpage

\subsection{Linear Rank Tests}
Before summarizing the literature for linear rank tests, we give a quick introduction of Probability Scale Residuals.

\subsubsection{Probability Scale Residuals for Censored Data}

\cite{li2012new} proposed a new method of evaluating residuals for ordinal data. They defined Probability Scale Residuals (PSR) as:\\
$$
\begin{aligned}
	r(t,F^*) &= E\{sign(y,Y^*)\} = pr(Y^* < y) - pr(Y^* > y) = F^*(y-) - (1-F^*(y)) =\\
	 &=F^*(y-) - 1 + F^*(y)
	\end{aligned}
$$

The authors also extended this definition for two cases: censored time to event and current status data. In order to understand how the PSR were extended for censored data, let's assume that $T$ is time to event, but we only observe $Y = min(T, C)$, where $C$ is time to censoring. When we do not observe $T$, the PSR are defined in terms of $Y$ (time to event or censoring) and $\Delta$, where $\Delta = 1$ when $min(T, C)=T$ (we observe the event) and $\Delta =0$ when $min(T, C)=C$ (the event is not observed because of censoring). \textbf{By definition}, if $\Delta = 1$, then $Y=T$ and $r(y,F^*, \delta=1) = F^*(y-) - 1 + F^*(y)$. If $\Delta = 0$, then $T$ is unknown, except that it occurs some time after the censoring time $C$, so \textbf{by definition} $r(t,F^*, \delta=0) = E\{r(t,F^*)|T^*>y\}$. The authors prove that:

	$$
	\begin{aligned}
		r(y, F^*) &= F^*(y) + F^*(y-) - 1,~~~&\delta = 1 \\
		r(y, F^*) &= E\{r(T,F^*)|T^*>y\} = F^*(y) ,~~~&\delta = 0 \\
		&or\\
    r(y, F^*, \delta) &= F^*(y) - \delta(1 - F^*(y-)),~~~&where~\delta \in \{0,1\}
	\end{aligned}
	$$

In a similar manner, PSR are defined for current status data. Current status data is a pair $(C, \Delta)$, where variable $C$ is the time when we observe the subject, and variable $\Delta$ is whether the subject has already experienced the event or not.
	$$
	\begin{aligned}
		r(c, F^*) &= E\{r(T,F^*)|T^*\leq c\} = F^*(c) - 1,~~~&\delta = 1 \\
		r(c, F^*) &= E\{r(T,F^*)|T^*>c\} = F^*(c) ,~~~&\delta = 0 \\
		&or\\
    r(c, F^*, \delta) &= F^*(c) - \delta,~~~&where~\delta \in \{0,1\}\\
	\end{aligned}
	$$

In both cases the PSR have expectation of zero when $F^*$ is properly defined and $T \perp C$.\\

The authors showed how PSR were related to martingale, Cox-Snell, and deviance residuals and that PSR also can be used to examine the functional adequacy of the model, with positive PSR indicating that the event time was longer than expected. In addition to examining the functional adequacy of the model, PSR can be used to measure and test association of two censored times to event. One of the intuitive ways to find the association between two time-to-event outcomes would be to find their correlation. We now show that correlation of PSR is equivalent to the censored version of \emph{Spearman} correlation.

\subsubsection{Spearman Correlation and Correlation of PSR for Continuous Censored Outcome.}
In this following section we use $S$, $C$, $\delta=I(S\leq C)$, $X = min(S, C)$ for the first outcome and $T$, $D$, $\epsilon=I(T\leq D)$, $Y = min(T, D)$ for the second.\\
Spearman correlation for continuous outcome can be defined as $cor(F_S(S), F_T(T))$. In case of censored outcomes however, we don't always observe $S$ and $T$. This means that we have to modify the definition of Spearman correlation in the following way: When $\delta = 1$ or $S\leq C$, we have that $X = min(S, C)=S$ so $F_S(s)=F_S(x)$ and we use $F_S(x)$ in order to compute correlation. When $\delta = 0$ or $S > C$, we have that $X = min(S, C)=C$ and instead of $F_S(s)$, in order to compute correlation, we can take $E_S[F_S(s)|S>x]$. Let's first compute $E_S[F_S(s)|S>x]$:
% keeping in mind that in continuous case $F_S(s-) = F_S(s)$
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)}\int_{(x, \infty)} F_S(s)dF_S(s) = \frac{1}{1-F_S(x)} \left.\frac{ [F_S(s)]^2}{2}\right|_{(x, \infty)} = \frac{1-[F_S(x-)]^2}{2(1-F_S(x))} \\
	\end{aligned}
	$$
Since we are focusing now on the continuous case, we can assume that $F_S(x)=F_S(x-)$ therefore we have
	$$
	\begin{aligned}
		E_S[F_S(x)|S>X] &=\frac{1-[F_S(x)]^2}{2(1-F_S(x))}=\frac{\cancel{(1-F_S(x))}(1+F_S(x))}{2\cancel{(1-F_S(x))}}  = \frac{1+F_S(x)}{2} \\
	\end{aligned}
	$$
Now, we redefine the Spearman correlation for the continuous censored data as:
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta F_S(x) + (1-\delta) \frac{1+F_S(x)}{2},~\epsilon F_T(y) + (1-\epsilon) \frac{1+F_T(y)}{2}  \right)\\
		&=~ cor\left(  \frac{1+\delta}{2}F_S(x) +  \frac{1-\delta}{2},~\frac{1+\epsilon}{2}F_T(y) +  \frac{1-\epsilon}{2}  \right)\\
		&=~ cor\left(  (1+\delta)F_S(x) +  1-\delta,~(1+\epsilon)F_T(y) +  1-\epsilon  \right)\\
	\end{aligned}
	$$
It is easy to check that in the continuous case, the above expression is the same as the correlation of PSR in the continuous case ($F_S(s-) = F_S(s)$):
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  (1+\delta)F_S(x) -\delta,~(1+\epsilon)F_T(y) -\epsilon  \right)\\
	\end{aligned}
	$$

\subsubsection{Spearman Correlation and Correlation of PSR for Discrete Censored Outcome.}
For the discrete case, Spearman correlation can be defined as:
	$$
	\begin{aligned}
		cor\left( \frac{F_S(s) + F_S(s-)}{2},~\frac{F_T(t) + F_T(t-)}{2} \right)\\
	\end{aligned}
	$$
In addition, if we do not observe time to event, instead of $F_S(x)$ or $F_S(x-)$ we take the expectation:  $E_S[F_S(x)|S>x]$ or $E_S[F_S(x-)|S>x]$ respectively. Let's first find $E_S[F_S(x)]$. For brevity, we denote $Pr\left\{S=s\right\}$ as $P_s$:
	$$
	\begin{aligned}
		E_S[F_S(x)] &= \sum_{i=0}^{\infty}F_S(i)P_i = \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		&= (P_0 P_0) + (P_0 P_1 + P_1 P_1) + (P_0 P_2 + P_1 P_2 + P_2 P_2) + ...= \\
		&= P_0^2 + P_1^2 + P_2^2 + ... + P_0 P_1 + P_0 P_2 + P_2 P_1 + ... =\frac{1}{2}(P_0 + P_1 + P_2 + ...)^2 + \frac{1}{2}(P_0^2 + P_1^2 + P_2^2 + ...) =\\
		&= \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2
	\end{aligned}
	$$
Now it is easier to derive $E_S[F_S(x)|S>x]$:
	$$
	\begin{aligned}
		E_S[F_S(x)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} + \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 + \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$

In a similar manner, we derive $E_S[F_S(x-)|S>x]$, assuming that $F_S(0-)=0$:
	$$
	\begin{aligned}
		E_S[F_S(x-)|S>x] &= \frac{1}{1-F_S(x)} \sum_{i=x+1}^{\infty}F_S(i-1)P_i = \frac{1}{1-F_S(x)}\sum_{i=x+1}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i =\\
		 &= \frac{1}{1-F_S(x)}\left[ \sum_{i=0}^{\infty}\left( \sum_{k=0}^{i-1}P_k \right)P_i - \sum_{i=0}^{x}\left( \sum_{k=0}^{i-1}P_k \right)P_i   \right] = \\
		 &= \frac{1}{1-F_S(x)}\left[ \frac{1}{2} - \frac{1}{2}\sum_{i=0}^{\infty}P_i^2 -  \frac{1}{2} \left\{ \left(\sum_{i=0}^{x}P_i\right)^2 - \sum_{i=0}^{x}P_i^2  \right\}    \right] = \\
		 &= \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\\
	\end{aligned}
	$$
Now, let's recall that in the discrete case each Spearman correlation component is $\frac{F_S(s) + F_S(s-)}{2}$, so let's compute it for the case when $\delta=0$:
	$$
	\begin{aligned}
	  \frac{E_S[F_S(x)|S>x] + E_S[F_S(x-)|S>x]}{2} &= \frac{1}{2}\left[\frac{ 1 - F_S^2(x) + \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}   +   \frac{ 1 - F_S^2(x) - \sum_{i=x+1}^{\infty}P_i^2 }{2(1-F_S(x))}\right]=\\
		&= \frac{1 - F_S^2(x)}{2(1-F_S(x))}= \frac{1 + F_S(x)}{2}\\
	\end{aligned}
	$$
As a result, we have:
	$$
	\begin{aligned}
		Spearman~correlation~&=~ cor\left(  \delta\frac{F_S(x) + F_S(x-)}{2} + (1-\delta) \frac{1 + F_S(x)}{2}, ~\epsilon\frac{F_T(y) + F_T(y-)}{2} + (1-\epsilon) \frac{1 + F_T(y)}{2} \right)=\\
		&=~ cor\left(  \delta( F_S(x) + F_S(x-)) + (1-\delta) (1 + F_S(x)), ~\epsilon (F_T(y) + F_T(y-)) + (1-\epsilon) (1 + F_T(y)) \right)=\\
		&=~ cor\left( \cancel{\delta F_S(x)} + \delta F_S(x-) +  1 + F_S(x) -\delta - \cancel{\delta F_S(x)}, ~\cancel{\epsilon F_T(y)} + \epsilon F_T(y-) + 1 + F_T(y)-\epsilon -\cancel{\epsilon F_T(y)}   \right)=\\
		&=~ cor\left( F_S(x) + \delta F_S(x-) +  1 -\delta , ~ F_T(y) + \epsilon F_T(y-) + 1 -\epsilon   \right)\\
	\end{aligned}
	$$
It can be easily shown that the above expression is equivalent to correlation of \emph{PSR}:
	$$
	\begin{aligned}
		PSR~correlation~&=~ cor\left(  F_S(x) + \delta F_S(x-) -\delta,~F_T(y) + \epsilon F_T(y-) -\epsilon  \right)\\
	\end{aligned}
	$$

We have proved the equivalence of \emph{Spearman} correlation and the correlation of the PSR based on the following definition of \emph{Spearman} correlation: $corr(F_S(s), F_T(t))$. It is also possible to show the equivalence of \emph{Spearman} correlation and the correlation of PSR using another definition: $E(sign(S-S_0)sign(T-T_0))$, where $(S,T)$ is a paired event time, and $S_0 \perp T_0$.\\
~\\
We now summarized papers that in the search of a linear rank test for association of bivariate survival data, arrived to a censored version of \emph{Spearman} correlation.


\subsection{Censored Version of Spearman Correlation in Earlier Research}

\cite{prentice1978linear} considered a model of linear regression with time to event as an outcome, while allowing the time to be censored.
	$$
	\begin{aligned}
		y = \alpha + \beta z + \sigma e
	\end{aligned}
	$$
The author's goal was to develop a \emph{linear rank statistic} in order to check the following hypothesis: $\beta = 0$. Although this model described univariate survival, we are interested in it because of the author's approach to censored observations.
%In order to do this he had to find an approach to rank survival outcome in the presence of censored data. He chose a simplified approach to the ranking of censored values [Kalbfleisch \& Prentice, 1973].
According to this approach, uncensored observations were ranked among themselves, while censored observations were ranked according to their location relatively to uncensored observations. This approach allowed to develop a linear rank statistic and its approximation that assigned the same score to all censored residuals between adjacent uncensored values.\\
%Even after using ranks and approximations, Prentice's approach required assuming $F_i$ - distribution of residuals (check this please).
~\\

\cite{cuzick1982rank} considered a general model of association:
	$$
	\begin{aligned}
		Y_1 = aZ + e_1 ~~~~~~~~ Y_2 = bZ + e_2,
	\end{aligned}
	$$
where $Z$, $e_1$, and $e_2$ were independent and  $b=a\lambda,~~0<|\lambda|<\infty$. In this case two variables $(Y_1, Y_2)$ were thought to be related to a third unobserved covariate.\\
The author assumed the same generalized ranking of censored data as \cite{prentice1978linear}: the uncensored observations were ranked among themselves and the censored observations were given the same rank as its closest uncensored observation on the left.\\
Cuzick's approach required knowing $f_i(x)$ the density of $e_i$, and as an illustration, he assumed a logistic density of $e_1$ and $e_2$:  $f(x) = 2\pi_{-1} e^{-x}/(1+e^{-x})^2$. It turned out that for this particular case the rank of each observation with no censoring was $F(x)$, and in the presence of censoring, the rank was $\frac{1+F(x)}{2}$, which resulted in the association test statistic equivalent to \emph{Spearman} correlation.\\
~\\

\cite{dabrowska1986rank} developed a semi-parametric statistic for testing an association of two survival outcomes, possibly censored, with the following null hypothesis: $H_0:~F=F_1 F_2$. In order to test this hypothesis, the author further developed the results of \cite{prentice1978linear} , \cite{kalbfleisch2011statistical} , and \cite{cuzick1982rank} , and suggested a \emph{linear rank} statistic and its simplification in the following form:


	$$
	\begin{aligned}
		 S_n = \sum_{n=1}^N \mathcal{J}_1( \hat{F}_1, \delta_{1n}) \cdot \mathcal{J}_2( \hat{F}_2, \delta_{2n}),
	\end{aligned}
	$$
where $\hat{F}_i$ were estimators similar to the usual \emph{Kaplan-Meier} estimators of the marginal \emph{CDF}'s. She showed that under certain assumptions and with a particular choice of $\mathcal{J}_i(u,d)$, $\mathcal{J}_i(u,d) =d-(1+d)u$, the test statistic corresponds to the censored data version of the Spearman test. Substituting this choice of $\mathcal{J}_i(u=F_i,~d=\delta_i)$ into the above expression, we get:
	$$
	\begin{aligned}
		 S_n &= \sum_{n=1}^N (\hat{F}_1 - \delta_{1n}(1-\hat{F}_1))\cdot (\hat{F}_2 - \delta_{2n}(1-\hat{F}_2))\\
	\end{aligned}
	$$
The above expression reminds us of $E[PSR_1 \cdot PSR_2]$, where $PSR(x, \delta) = F(x)-\delta(1-F(x))$ is PSR for continuous and censored failure time.\\
~\\

\cite{ding2004testing} suggested the following statistic for testing independence for bivariate \emph{current status data}:
	$$
	\begin{aligned}
		 E[cov(\delta_1, \delta_2)|C_1,C_2] = E\left\{ [\delta_1 - F_1(C_1)][\delta_2 - F_2(C_2)]  \right\}\\
	\end{aligned}
	$$
Which is exactly covariance of PSR for \emph{current status data}.\\
~\\
Although censored version of \emph{Spearman} correlation was suggested before PSR were introduced, the framework for \emph{Spearman} correlation statistics was based on specific distribution assumptions were not generalized for discrete data. The tests statistic computed as correlation of PSR does not have these problem.


\section{DISCUSSION AND FUTURE RESEARCH}

The interest in association of bivariate survival data goes back to 1978. Many different approaches have beed taken to address this question althogh not all them were covered here. Our focus was on the three main approaches. We first summarized literature on evaluating a bivariate hazard ratio or cross ratio in the context of a parametric model. This approach was later developed to include larger family of models, and to find cases when a non-parametric approach could be taken. Another approach was to evaluate a bivariate survival surface as a product of two marginal survival functions and a function representing the association component between two time to event variables. A third approach was developing association tests for bivariate time to event variables.\\

Although all approaches were based on a complex statistical and mathematical theory, their main disadvantages were parametric assumptions, computational complexity, and lack of generalizability. For example, although \cite{cuzick1982rank}, \cite{dabrowska1988kaplan}, and \cite{ding2004testing} derived statistics identical to correlation of PSR for continuous case, their conclusions were rooted in a semi-parametric context, with very specific assumptions about the distribution of time to event, asymptotic properties of the statistics, and ranking of censored events. Their results also were not generalized to discrete outcomes.\\

The use of correlation of PSR, on the other hand, does not require parametric assumptions and can be generalized to discrete censored data and to discrete currents status data. The test of association can be performed using the following statistic:
$$
\begin{aligned}
	T = \frac{\sum_{i=1}^{n} (r_{si} - \bar{r}_s)(r_{ti} - \bar{r}_t)}{\sqrt{\sum_{i=1}^{n} ((r_{si} - \bar{r}_s)^2\sum_{i=1}^{n} (r_{ti} - \bar{r}_t)^2}}
\end{aligned}
$$
where $r_{si}$ and $r_{ti}$ are PSR and $\bar{r}_s = \frac{1}{n}\sum_{i=1}^{n} r_{si}$ and $\bar{r}_t = \frac{1}{n}\sum_{i=1}^{n} r_{ti}$.

For ordinal uncensored data, \cite{li2010test} suggested two ways of performing the inference: 1) a bootstrap procedure and 2) a large sample approximation. In our future work we will focus on proving the validity of applying bootstrap procedure to finding the confidence intervals for $T$, and on finding the asymptotic distribution for $T$ for continuous and discrete censored data and currents status data.\\

We also plan to focus on conditional and partial correlation of PSR for censored and current status data in order to test adjusted association of time to event and adjusted association of status. Partial correlation can be obtain by fitting a parametric or semi-parametric survival model using covariates of interest, and then computing correlation of the resulting PSRs. Conditional correlation can also be computed by first fitting a parametric or semi-parametric survival model, and then evaluating conditional moments that approximate conditional correlation of PSR.

In the future, we plan to compare the performance of PSR correlation for censored data to the performance of martingale, Cox-Snell, and deviance residuals when checking functional adequacy of model fit. The performance can be compared using data simulated under different conditions and assumptions, such as different distribution of the survival data, different functional dependence between time to event and covariates, in the context of parametric and non-parametric models, with proportionality of the hazard and without, and finally with varying fraction of censored data.




% \subsection{Zhang, 2008: Inference on the association measure for bivariate survival data with hybrind censoring and applications to an HIV study, ref ?}
% The author uses archimedian copulas and Kendall's $\tau$ to assess association of bivariate survival data with hybrid censoring.
%
% \section{Summary of: An Adjustment to Improve the Bivariate Survivor Function Repaired NPMLE. Moodie, Prentice, 2005 \cite{moodie2005adjustment}}
%
% \subsection{Alvo and Cabilio, 1995 citation ?}
% UNDER CONSTRUCTION.\\
% The authors use a concept of distance applied to \emph{Spearman} and \emph{Kendal} correlation measure in order to build a test that can deal with missing data.


%
% \section{Oral Exam Stuff}
% \begin{itemize}
% 	\item Probability scale residuals, PSR (Li\&Shepard)
% 	\item Challanges and limitations (discrete data covariates)
% 	\item Qi - covariates (look at prentice and Cuzick)
% 	\item show that this is Spearman
% 	\item apply to dataset
% 	\item simmulations
% 	\item explore performance of partial and conditional Spearman.
% 	\item Understand Qi's conditional, partial, and conditional-partial correlation
% 	\item Develop Spearman correlation for different $s$ and $t$: instead of having one number $\rho$ develop a new measure $\rho(s, t)$.
% 	\item \textbf{READ: Hanley, J. A., and Parnes, M. N. (1983), Nonparametric Estimation of a Multivariate Distribution in the Presence of Censoring, Biometrics, 39, 129-139.}
% \end{itemize}
~\\
% \subsubsection{Shih, Louis, 1996, \cite{shih1996tests}}
% \textbf{Motivation}: Leukemia patients that undergo bone marrow transplantation are at risk of \emph{acute graft versus host disease (AGVHD)} and \emph{cytomegalovirus (CMV)}, and the question is if these events are correlated:
%
% The authors consider martingale residuals, $M_{ij}$, which is the difference between observed and expected deaths. They base their new statistic on the result of Clayton [\cite{clayton1978model}] and Cuzick [\cite{shih1996tests}] who developed a model for bivariate time to event with frailties, and suggested the following test statistic that measured the sample covariance of the martingale residuals:
% $$
% \begin{aligned}
%   T_n=\sum_i \left\{ \delta_{i1} - \hat{\Lambda}(X_{i1}) \right\} \left\{ \delta_{i2} - \hat{\Lambda}(X_{i2}) \right\} =\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2).
% \end{aligned}
% $$
%
% Shih and Louis suggested to improve its power by introducing the following two statistics:
%
% \begin{enumerate}
% 	\item take supremum: $U_n= \sup_{0\leq t \leq t_0} \left|\sum_i \int_0^{t_0}  \int_0^{t_0} d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)\right|$
% 	\item weigh it: $V_n= \sum_i \int_0^{t_0}  \int_0^{t_0} W_n(u_1,u_2) d\hat{M}_{i1}(u_1) d\hat{M}_{i2}(u_2)$ (which is analogous to the weighted logrank test)
% \end{enumerate}
% They choose \emph{optimal} weights by somehow using the \emph{cross ratio} defined by Oakes $\theta = \frac{ \frac{\partial^2 S(s,t)}{\partial s \partial t} S(s, t)}    {\frac{\partial S(s, t)}{\partial t} \frac{\partial S(s, t)}{\partial s}}$:
%
% Although statistic $U_n$ is a supremum over $t_1=t_2$, the simulations showed promising results.
% For $V_n$, if the weights are not specified correctly, the test based on $V_n$ looses power.

% % Lucy's way:
% %\printbibliography

% Jonathan's way:
%\bibliographystyle{plainnat}

\bibliography{summaryOfRef}

\end{document}          

